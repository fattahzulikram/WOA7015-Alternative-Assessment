{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e0002a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "de6515cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import pickle\n",
    "import nltk\n",
    "import json\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa9adc",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28550e8a",
   "metadata": {},
   "source": [
    "### Dataset Contstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1f6e438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "MAX_NODES_PER_QUESTION = 10\n",
    "\n",
    "# Directory Information\n",
    "DATA_DIR = \"data/\"\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\n",
    "KG_PATH = os.path.join(DATA_DIR, 'KG/')\n",
    "VOCABS_PATH = os.path.join(DATA_DIR, 'vocabs/')\n",
    "\n",
    "# Huggingface Repository Information\n",
    "repo_id = \"BoKelvin/SLAKE\"\n",
    "repo_type = \"dataset\"\n",
    "kg_file = \"KG.zip\"\n",
    "img_file = \"imgs.zip\"\n",
    "\n",
    "# Entity Extraction\n",
    "MAX_PHRASE_LENGTH = 5 # Look up to 5-grams for entity matching\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1a291",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befca6a3",
   "metadata": {},
   "source": [
    "### Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55169c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for downloading and extracting ZIP file\n",
    "def download_and_store_ZIP(filename, save_dir):\n",
    "    print(f\"Fetching file {filename} from {repo_id} repo\")\n",
    "\n",
    "    try:\n",
    "        # Caches the file locally and returns the path to the cached file\n",
    "        cached_zip_path = hf_hub_download(\n",
    "          repo_id=repo_id,\n",
    "          filename=filename,\n",
    "          repo_type=repo_type\n",
    "        )\n",
    "        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Extract the contents\n",
    "        print(f\"Extracting to {save_dir}...\")\n",
    "        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "\n",
    "        print(\"Extraction complete.\")\n",
    "        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or extract {filename}: {e}\")\n",
    "\n",
    "# Scoping to English only\n",
    "def filter_language(original):\n",
    "    return original.filter(lambda data: data['q_lang'] == 'en')\n",
    "\n",
    "# Download and store the dataset\n",
    "def download_and_store_english_dataset():\n",
    "    print(f\"Downloading dataset from {repo_id} repo\")\n",
    "\n",
    "    # Load from Hugging Face\n",
    "    original = load_dataset(repo_id)\n",
    "\n",
    "    # Scope to English Only\n",
    "    original = filter_language(original)\n",
    "\n",
    "    # Show the dataset formatting\n",
    "    pprint(original)\n",
    "\n",
    "    # Save the original dataset\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        os.makedirs(DATASET_PATH)\n",
    "\n",
    "    original.save_to_disk(DATASET_PATH)\n",
    "    return original\n",
    "\n",
    "# Download and store the Knowledge Graph files\n",
    "def download_and_store_KG():\n",
    "    download_and_store_ZIP(kg_file, DATA_DIR)\n",
    "\n",
    "# Download and store the image files\n",
    "def download_and_store_image():\n",
    "    download_and_store_ZIP(img_file, DATA_DIR)\n",
    "\n",
    "# Download necessary files\n",
    "def download_and_store_slake():\n",
    "    dataset = download_and_store_english_dataset()\n",
    "    download_and_store_image()\n",
    "    download_and_store_KG()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8490de7",
   "metadata": {},
   "source": [
    "### Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14506466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NetworkX graph from the knowledge graph\n",
    "def build_slake_knowledge_graph(kg_directory):\n",
    "    # Initialize a Directed Multi-Graph (allows parallel edges)\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    search_path = os.path.join(kg_directory, \"en_*.csv\")\n",
    "    csv_files = glob.glob(search_path)\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No files found matching {search_path}\")\n",
    "        return None\n",
    "    print(f\"Found {len(csv_files)} English KG files. Building graph...\")\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        print(f\"Processing {file_path}\")\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, dtype=str, delimiter='#')\n",
    "            print(f\"Total relations in {filename}: {df.size}\")\n",
    "\n",
    "            if len(df.columns) < 3:\n",
    "                print(f\"Error with {filename}: Expected 3 columns, found {len(df.columns)}\")\n",
    "                continue\n",
    "\n",
    "            df.columns = ['head', 'relation', 'tail'] + list(df.columns[3:])\n",
    "\n",
    "            # Add edges for the graph\n",
    "            for _, row in df.iterrows():\n",
    "                u = row['head'].strip().lower()\n",
    "                rel = row['relation'].strip().lower()\n",
    "                v = row['tail'].strip().lower()\n",
    "\n",
    "                # Add to edge list\n",
    "                G.add_edge(u, v, relation=rel)\n",
    "\n",
    "            # G.add_edges_from(edges)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "    print(f\"\\nGraph Built Successfully!\")\n",
    "    print(f\"Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Edges: {G.number_of_edges()}\")\n",
    "\n",
    "    # Save as pickle for fast load\n",
    "    if not os.path.exists(KG_PATH):\n",
    "        os.makedirs(KG_PATH)\n",
    "\n",
    "    pickle.dump(G, open(os.path.join(KG_PATH, 'slake_kg.pickle'), 'wb'))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b67dc9",
   "metadata": {},
   "source": [
    "### Vocabs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cf5248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def build_word_vocabs(self, sentences):\n",
    "        counter = Counter()\n",
    "        start_index = len(self.stoi)\n",
    "\n",
    "        # 1. Count frequencies of all tokens in the tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        # 2. Add words that meet the frequency threshold\n",
    "        for word, count in counter.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = start_index\n",
    "                self.itos[start_index] = word\n",
    "                start_index += 1\n",
    "\n",
    "        print(f\"Vocabulary Built. Vocabulary Size: {len(self.stoi)}\")\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "            for token in tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389c456",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1de11da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies for questions and answers\n",
    "def build_vocabs(dataset):\n",
    "    questions = [item['question'] for item in dataset]\n",
    "    answers = [item['answer'] for item in dataset]\n",
    "\n",
    "    # Question Vocabulary\n",
    "    questvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "    questvocab_builder.build_word_vocabs(questions)\n",
    "    \n",
    "    # Answer Vocabulary\n",
    "    ansvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "    ansvocab_builder.build_word_vocabs(answers)\n",
    "\n",
    "    return questvocab_builder, ansvocab_builder\n",
    "\n",
    "# Save vocabularies to JSON files\n",
    "def save_vocabs(quest_vocab, ans_vocab):\n",
    "    if not os.path.exists(VOCABS_PATH):\n",
    "        os.makedirs(VOCABS_PATH)\n",
    "\n",
    "    # Save Question Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'question_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': quest_vocab.stoi, 'itos': quest_vocab.itos}, f)\n",
    "\n",
    "    # Save Answer Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'answer_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': ans_vocab.stoi, 'itos': ans_vocab.itos}, f)\n",
    "\n",
    "    print(\"Vocabularies saved successfully.\")\n",
    "\n",
    "# Mapping from node name to index\n",
    "def create_node_mapping(graph):\n",
    "    nodes = list(graph.nodes)\n",
    "    node_to_index = {node: idx + 1 for idx, node in enumerate(nodes)} # Index 0 reserved for padding\n",
    "    index_to_node = {idx + 1: node for idx, node in enumerate(nodes)}\n",
    "\n",
    "    return node_to_index, index_to_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82457fbe",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c9065769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlakeDataset(Dataset):\n",
    "    def __init__(self, dataset, question_vocab, answer_vocab, graph, node_to_index, transform=None):\n",
    "        self.data = dataset\n",
    "        self.question_vocab = question_vocab\n",
    "        self.answer_vocab = answer_vocab\n",
    "        self.graph = graph\n",
    "        self.node_to_index = node_to_index\n",
    "        self.transform = transform\n",
    "\n",
    "        # Pre compute graph nodes for faster access\n",
    "        self.graph_nodes = set(graph.nodes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        # Initialize Lemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        extracted_entities = []\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            match_found = False\n",
    "            # Check for n-grams from MAX_PHRASE_LENGTH down to 1\n",
    "            for length in range(MAX_PHRASE_LENGTH, 0, -1):\n",
    "                if i + length <= len(tokens):\n",
    "                    phrase_tokens = tokens[i:i+length]\n",
    "\n",
    "                    # Strategy 1: Exact Match\n",
    "                    phrase = ' '.join(phrase_tokens)\n",
    "                    if phrase in self.graph_nodes:\n",
    "                        index = self.node_to_index.get(phrase, None)\n",
    "                        extracted_entities.append(index)\n",
    "                        i += length\n",
    "                        match_found = True\n",
    "                        break\n",
    "\n",
    "                    # Strategy 2: Lemmatized Match (For singular/plural issues)\n",
    "                    phrase_lemma_tokens = [lemmatizer.lemmatize(token, pos='n') for token in phrase_tokens]\n",
    "                    phrase_lemma_str = \" \".join(phrase_lemma_tokens)\n",
    "                    if phrase_lemma_str in self.graph_nodes:\n",
    "                        index = self.node_to_index.get(phrase_lemma_str, None)\n",
    "                        extracted_entities.append(index)\n",
    "                        i += length\n",
    "                        match_found = True\n",
    "                        break\n",
    "\n",
    "            if not match_found:\n",
    "                i += 1\n",
    "\n",
    "        return extracted_entities if extracted_entities else [0] # Return placeholder if no entities found\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Image Processing\n",
    "        image_path = item['img_name']\n",
    "        image_path = os.path.join(IMAGE_PATH, image_path)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 2. Question Processing\n",
    "        question = item['question']\n",
    "        question_indices = self.question_vocab.numericalize(question)\n",
    "\n",
    "        # 3. Answer Processing\n",
    "        answer = str(item.get('answer', '')) # Answer may be missing in test set\n",
    "        answer_index = self.answer_vocab.numericalize(answer)\n",
    "\n",
    "        # 4. Inject Knowledge from KG\n",
    "        kg_index = self.extract_entities(question)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'question' : torch.tensor(question_indices),\n",
    "            'answer' : torch.tensor(answer_index),\n",
    "            'kg_index' : torch.tensor(kg_index),\n",
    "            # Add original items for reference\n",
    "            'original_question': question,\n",
    "            'original_answer': answer,\n",
    "            # Add ID for tracking\n",
    "            'id': item['qid']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d20724",
   "metadata": {},
   "source": [
    "### Collate function\n",
    "\n",
    "Questions have different lengths, need to pad properly to make sure the length is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a187526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlakeCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Stack images\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "\n",
    "        # Pad Questions\n",
    "        questions = [item['question'] for item in batch]\n",
    "        padded_questions = pad_sequence(questions, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        # Pad Answers\n",
    "        answers = [item['answer'] for item in batch]\n",
    "        answers = pad_sequence(answers, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        # Pad KG Indices\n",
    "        kg_indices = [item['kg_index'] for item in batch]\n",
    "        kg_indices = pad_sequence(kg_indices, batch_first=True, padding_value=0)\n",
    "\n",
    "        return {\n",
    "            'image': images,\n",
    "            'question': padded_questions,\n",
    "            'answer': answers,\n",
    "            'kg_index': kg_indices,\n",
    "            'original_questions': [item['original_question'] for item in batch],\n",
    "            'original_answers': [item['original_answer'] for item in batch],\n",
    "            'ids': [item['id'] for item in batch]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642aab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Purpose\n",
    "def test_pipelines(need_download=True):\n",
    "    if need_download:\n",
    "        # Download and store the dataset\n",
    "        dataset = download_and_store_slake()\n",
    "    else:\n",
    "        dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "    # Build the knowledge graph\n",
    "    kg = build_slake_knowledge_graph(KG_PATH)\n",
    "\n",
    "    # Build vocabularies\n",
    "    train_data = dataset['train']\n",
    "    question_vocab, answer_vocab = build_vocabs(train_data)\n",
    "\n",
    "    # Save vocabularies\n",
    "    save_vocabs(question_vocab, answer_vocab)\n",
    "\n",
    "    # Create node mappings\n",
    "    node_to_index, index_to_node = create_node_mapping(kg)\n",
    "\n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = SlakeDataset(train_data, question_vocab, answer_vocab, kg, node_to_index, transform=transform)\n",
    "    collate_fn = SlakeCollate(pad_idx=0)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Test the loader\n",
    "    for batch in train_loader:\n",
    "        print(\"Batch Size:\", train_loader.batch_size)\n",
    "        print(\"Images shape:\", batch['image'].shape)     # (32, 3, 224, 224)\n",
    "        print(\"Questions shape:\", batch['question'].shape) # (32, max_q_len)\n",
    "        print(\"Answer shape:\", batch['answer'].shape) # (32, max_q_len)\n",
    "        print(\"KG Nodes shape:\", batch['kg_index'].shape)   # (32, max_nodes_len)\n",
    "\n",
    "        # Specific data\n",
    "        print(\"Question: \", batch['original_questions'][0])\n",
    "        print(\"Tokenized Question: \", batch['question'][0])\n",
    "        print(\"Answer: \", batch['original_answers'][0])\n",
    "        print(\"Tokenized Answer: \", batch['answer'][0])\n",
    "        print(\"KG Nodes: \", batch['kg_index'][0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4ee7c928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 English KG files. Building graph...\n",
      "Processing data/KG\\en_disease.csv\n",
      "Total relations in en_disease.csv: 6648\n",
      "Processing data/KG\\en_organ.csv\n",
      "Total relations in en_organ.csv: 843\n",
      "Processing data/KG\\en_organ_rel.csv\n",
      "Total relations in en_organ_rel.csv: 309\n",
      "\n",
      "Graph Built Successfully!\n",
      "Nodes: 1892\n",
      "Edges: 2600\n",
      "Vocabulary Built. Vocabulary Size: 281\n",
      "Vocabulary Built. Vocabulary Size: 249\n",
      "Vocabularies saved successfully.\n",
      "Batch Size: 32\n",
      "Images shape: torch.Size([32, 3, 224, 224])\n",
      "Questions shape: torch.Size([32, 17])\n",
      "Answer shape: torch.Size([32, 5])\n",
      "KG Nodes shape: torch.Size([32, 2])\n",
      "Question:  Is this a study of the chest?\n",
      "Tokenized Question:  tensor([  6,  10, 117, 118,  15,  16, 128,  12,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0])\n",
      "Answer:  Yes\n",
      "Tokenized Answer:  tensor([7, 0, 0, 0, 0])\n",
      "KG Nodes:  tensor([1722,    0])\n"
     ]
    }
   ],
   "source": [
    "loader, kg, n2i, i2n = test_pipelines(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3965675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
