{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e0002a",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6515cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\WOA7015 - Advanced ML\\Assignments\\AA\\aml_aa\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import nltk\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa9adc",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28550e8a",
   "metadata": {},
   "source": [
    "### Dataset Contstants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f6e438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "MAX_NODES_PER_QUESTION = 10\n",
    "\n",
    "# Directory Information\n",
    "DATA_DIR = \"data/\"\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\n",
    "VOCABS_PATH = os.path.join(DATA_DIR, 'vocabs/')\n",
    "\n",
    "# Huggingface Repository Information\n",
    "repo_id = \"BoKelvin/SLAKE\"\n",
    "repo_type = \"dataset\"\n",
    "img_file = \"imgs.zip\"\n",
    "\n",
    "# Entity Extraction\n",
    "MAX_PHRASE_LENGTH = 5 # Look up to 5-grams for entity matching\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1a291",
   "metadata": {},
   "source": [
    "# Dataset Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befca6a3",
   "metadata": {},
   "source": [
    "### Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55169c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for downloading and extracting ZIP file\n",
    "def download_and_store_ZIP(filename, save_dir):\n",
    "    print(f\"Fetching file {filename} from {repo_id} repo\")\n",
    "\n",
    "    try:\n",
    "        # Caches the file locally and returns the path to the cached file\n",
    "        cached_zip_path = hf_hub_download(\n",
    "          repo_id=repo_id,\n",
    "          filename=filename,\n",
    "          repo_type=repo_type\n",
    "        )\n",
    "        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Extract the contents\n",
    "        print(f\"Extracting to {save_dir}...\")\n",
    "        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "\n",
    "        print(\"Extraction complete.\")\n",
    "        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or extract {filename}: {e}\")\n",
    "\n",
    "# Scoping to English only\n",
    "def filter_language(original):\n",
    "    return original.filter(lambda data: data['q_lang'] == 'en')\n",
    "\n",
    "# Download and store the dataset\n",
    "def download_and_store_english_dataset():\n",
    "    print(f\"Downloading dataset from {repo_id} repo\")\n",
    "\n",
    "    # Load from Hugging Face\n",
    "    original = load_dataset(repo_id)\n",
    "\n",
    "    # Scope to English Only\n",
    "    original = filter_language(original)\n",
    "\n",
    "    # Show the dataset formatting\n",
    "    pprint(original)\n",
    "\n",
    "    # Save the original dataset\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        os.makedirs(DATASET_PATH)\n",
    "\n",
    "    original.save_to_disk(DATASET_PATH)\n",
    "    return original\n",
    "\n",
    "# Download and store the image files\n",
    "def download_and_store_image():\n",
    "    download_and_store_ZIP(img_file, DATA_DIR)\n",
    "\n",
    "# Download necessary files\n",
    "def download_and_store_slake():\n",
    "    dataset = download_and_store_english_dataset()\n",
    "    download_and_store_image()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b67dc9",
   "metadata": {},
   "source": [
    "### Vocabs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0cf5248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def build_word_vocabs(self, sentences):\n",
    "        counter = Counter()\n",
    "        start_index = len(self.stoi)\n",
    "\n",
    "        # 1. Count frequencies of all tokens in the tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        # 2. Add words that meet the frequency threshold\n",
    "        for word, count in counter.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = start_index\n",
    "                self.itos[start_index] = word\n",
    "                start_index += 1\n",
    "\n",
    "        print(f\"Vocabulary Built. Vocabulary Size: {len(self.stoi)}\")\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "            for token in tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389c456",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1de11da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies for questions and answers\n",
    "def build_vocabs(dataset):\n",
    "    questions = [item['question'] for item in dataset]\n",
    "    answers = [item['answer'] for item in dataset]\n",
    "\n",
    "    # Question Vocabulary\n",
    "    questvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "    questvocab_builder.build_word_vocabs(questions)\n",
    "    \n",
    "    # Answer Vocabulary\n",
    "    ansvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "\n",
    "    # Use a dummy tokenizer that just returns the whole lowercased string as one token\n",
    "    identity_tokenizer = lambda x: [x.lower().strip()]\n",
    "    ansvocab_builder.tokenize = identity_tokenizer\n",
    "\n",
    "    ansvocab_builder.build_word_vocabs(answers)\n",
    "\n",
    "    return questvocab_builder, ansvocab_builder\n",
    "\n",
    "# Save vocabularies to JSON files\n",
    "def save_vocabs(quest_vocab, ans_vocab):\n",
    "    if not os.path.exists(VOCABS_PATH):\n",
    "        os.makedirs(VOCABS_PATH)\n",
    "\n",
    "    # Save Question Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'question_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': quest_vocab.stoi, 'itos': quest_vocab.itos}, f)\n",
    "\n",
    "    # Save Answer Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'answer_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': ans_vocab.stoi, 'itos': ans_vocab.itos}, f)\n",
    "\n",
    "    print(\"Vocabularies saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82457fbe",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9065769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlakeDataset(Dataset):\n",
    "    def __init__(self, dataset, question_vocab, answer_vocab, transform=None):\n",
    "        self.data = dataset\n",
    "        self.question_vocab = question_vocab\n",
    "        self.answer_vocab = answer_vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Image Processing\n",
    "        image_path = item['img_name']\n",
    "        image_path = os.path.join(IMAGE_PATH, image_path)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 2. Question Processing\n",
    "        question = item['question']\n",
    "        question_indices = self.question_vocab.numericalize(question)\n",
    "\n",
    "        # 3. Answer Processing\n",
    "        answer = str(item.get('answer', '')) # Answer may be missing in test set\n",
    "        answer_index = self.answer_vocab.numericalize(answer)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'question' : torch.tensor(question_indices),\n",
    "            'answer' : torch.tensor(answer_index, dtype=torch.long),\n",
    "            # Add original items for reference\n",
    "            'original_question': question,\n",
    "            'original_answer': answer,\n",
    "            # Add ID for tracking\n",
    "            'id': item['qid']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d20724",
   "metadata": {},
   "source": [
    "### Collate function\n",
    "\n",
    "Questions have different lengths, need to pad properly to make sure the length is constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "206a9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slake_collate_fn(batch, pad_index=0):\n",
    "    # Separate different components\n",
    "    images = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    original_questions = []\n",
    "    original_answers = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        questions.append(item['question'])\n",
    "        answers.append(item['answer'])\n",
    "        original_questions.append(item['original_question'])\n",
    "        original_answers.append(item['original_answer'])\n",
    "        ids.append(item['id'])\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images)  # [batch_size, 3, H, W]\n",
    "    \n",
    "    # Get question lengths BEFORE padding\n",
    "    question_lengths = torch.tensor([len(q) for q in questions])\n",
    "    \n",
    "    # Pad questions to the longest sequence in THIS batch\n",
    "    # pad_sequence expects list of tensors, pads with 0 by default\n",
    "    questions_padded = pad_sequence(questions, batch_first=True, padding_value=pad_index)\n",
    "    # questions_padded: [batch_size, max_len_in_batch]\n",
    "    \n",
    "    # Handling answers\n",
    "    # Handling each answer as a single class\n",
    "    # answers = torch.stack(answers)\n",
    "    answers = torch.tensor([item['answer'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'question': questions_padded,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer': answers,\n",
    "        'original_question': original_questions,\n",
    "        'original_answer': original_answers,\n",
    "        'id': ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625161eb",
   "metadata": {},
   "source": [
    "## Getting everything ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0572f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Built. Vocabulary Size: 281\n",
      "Vocabulary Built. Vocabulary Size: 225\n"
     ]
    }
   ],
   "source": [
    "# Comment out if dataset is already downloaded\n",
    "# dataset = download_and_store_slake()\n",
    "\n",
    "# Uncomment if dataset is already downloaded\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Build vocabularies for training\n",
    "train_data = dataset['train']\n",
    "question_vocab, answer_vocab = build_vocabs(train_data)\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create train dataset and dataloader\n",
    "train_dataset = SlakeDataset(train_data, question_vocab, answer_vocab, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=slake_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3965675",
   "metadata": {},
   "source": [
    "## Modeling (Baseline)\n",
    "\n",
    "1. Basic CNN-LSTM </br>\n",
    "2. CNN-Bidirectional LSTM with self attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23bf0203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalVQABaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, embed_dim=256, hidden_dim=512):\n",
    "        super(MedicalVQABaseline, self).__init__()\n",
    "\n",
    "        # 1. CNN - ResNet\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        num_features = 512\n",
    "        # Remove the last classification layer\n",
    "        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.img_projector = nn.Linear(num_features, hidden_dim)\n",
    "        self.bn_img = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # 2. LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # 4. Classifier\n",
    "        fusion_dim = hidden_dim * 2\n",
    "\n",
    "        self.attention = nn.Linear(embed_dim, 1)\n",
    "        self.kg_gate = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, questions, question_lengths=None):\n",
    "        # CNN\n",
    "        # Extract features\n",
    "        img_feats = self.resnet_features(images).view(images.size(0), -1)\n",
    "        img_feats = self.img_projector(img_feats)\n",
    "        img_feats = self.bn_img(img_feats) # Normalize\n",
    "        img_feats = torch.relu(img_feats)\n",
    "\n",
    "        # LSTM\n",
    "        embeds = self.embedding(questions) # (Batch, Seq, Embed_Dim)\n",
    "        # LSTM output: (Batch, Seq, Hidden), (h_n, c_n)\n",
    "        # Take and modify the final hidden state h_n: (1, Batch, Hidden)\n",
    "        _, (h_n, _) = self.lstm(embeds)\n",
    "        text_feats = h_n.squeeze(0) # (Batch, Hidden)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat((img_feats, text_feats), dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(combined)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8de953de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM with Self-Attention for question encoding\n",
    "class BiLSTMWithSelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=1, dropout=0.5, pooling_strategy='mean'):\n",
    "        super(BiLSTMWithSelfAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        # BiLSTM outputs hidden_dim * 2 (forward + backward)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "    def forward(self, questions, question_lengths=None):\n",
    "        # Embed questions\n",
    "        embeds = self.embedding(questions)  # [B, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequence if lengths provided (for efficiency)\n",
    "        if question_lengths is not None:\n",
    "            embeds = nn.utils.rnn.pack_padded_sequence(\n",
    "                embeds, question_lengths.cpu(), \n",
    "                batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        \n",
    "        # BiLSTM encoding\n",
    "        lstm_out, (hidden, cell) = self.bilstm(embeds)\n",
    "        \n",
    "        # Unpack if needed\n",
    "        if question_lengths is not None:\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "                lstm_out, batch_first=True\n",
    "            )\n",
    "        \n",
    "        # lstm_out: [B, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Self-attention: query = key = value = lstm_out\n",
    "        attn_out, attn_weights = self.attention(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            need_weights=True\n",
    "        )\n",
    "        \n",
    "        # Residual connection + Layer Norm\n",
    "        attn_out = self.layer_norm(lstm_out + attn_out)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        \n",
    "        # Pooling strategy - you can experiment with these:\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            question_feature = attn_out.mean(dim=1)  # [B, hidden_dim * 2]\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            question_feature = attn_out.max(dim=1)[0]\n",
    "        else:\n",
    "            # Last hidden state (concatenate forward and backward)\n",
    "            question_feature = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        return question_feature, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1273544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete VQA model: ResNet34 + BiLSTM with Self-Attention\n",
    "class VQA_ResNet_BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, embed_dim=300, \n",
    "                 lstm_hidden=512, fusion_dim=1024, dropout=0.5):\n",
    "        super(VQA_ResNet_BiLSTM_Attention, self).__init__()\n",
    "        \n",
    "        # Image encoder: ResNet34\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # Remove the final FC layer\n",
    "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.image_feature_dim = 512  # ResNet34 final layer\n",
    "        \n",
    "        # Question encoder: BiLSTM + Self-Attention\n",
    "        self.question_encoder = BiLSTMWithSelfAttention(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=lstm_hidden,\n",
    "            num_layers=2,  # 2-layer BiLSTM\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.question_feature_dim = lstm_hidden * 2  # Bidirectional\n",
    "        \n",
    "        # Multimodal fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim + self.question_feature_dim, fusion_dim),\n",
    "            nn.BatchNorm1d(fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.BatchNorm1d(fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n",
    "        \n",
    "    def forward(self, images, questions, question_lengths=None):\n",
    "        # Extract image features\n",
    "        img_features = self.image_encoder(images)  # [B, 512, 1, 1]\n",
    "        img_features = img_features.squeeze(-1).squeeze(-1)  # [B, 512]\n",
    "        \n",
    "        # Extract question features with attention\n",
    "        q_features, attn_weights = self.question_encoder(questions, question_lengths) # [B, lstm_hidden * 2]\n",
    "        \n",
    "        # Concatenate image and question features\n",
    "        combined = torch.cat([img_features, q_features], dim=1)\n",
    "        # combined: [B, 512 + lstm_hidden*2]\n",
    "        \n",
    "        # Fusion\n",
    "        fused = self.fusion(combined)  # [B, fusion_dim // 2]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)  # [B, num_classes]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93797c87",
   "metadata": {},
   "source": [
    "## Training-Validation-Testing and Hyperparameter Tuning\n",
    "\n",
    "Currently for testing purpose I have not added any validation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dde76e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, targets):\n",
    "    # Get the index of the max log-probability\n",
    "    _, preds = torch.max(predictions, dim=1)\n",
    "    correct = (preds == targets).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # tqdm creates a nice progress bar\n",
    "    loop = tqdm(loader, total=len(loader), leave=True)\n",
    "    \n",
    "    for batch in loop:\n",
    "        batch_idx = loop.n\n",
    "\n",
    "        # 1. Move data to Device\n",
    "        images = batch['image'].to(device)\n",
    "        questions = batch['question'].to(device)\n",
    "        question_lengths = batch['question_lengths'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "\n",
    "        # 2. Forward Pass\n",
    "        # Forward pass - now with question_lengths!\n",
    "        logits = model(images, questions, question_lengths)\n",
    "        \n",
    "        # 3. Calculate Loss\n",
    "        loss = criterion(logits, answers)\n",
    "        \n",
    "        # 4. Backward Pass\n",
    "        optimizer.zero_grad() # Clear previous gradients\n",
    "        loss.backward()       # Compute gradients\n",
    "\n",
    "        # Gradient clipping (important for LSTMs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()      # Update weights\n",
    "        \n",
    "        # 5. Metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "        \n",
    "        # 6. Update progress bar\n",
    "        acc = calculate_accuracy(logits, answers)\n",
    "        loop.set_description(f\"Train\")\n",
    "        loop.set_postfix(loss=loss.item(), acc=acc.item())\n",
    "        \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fde345d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing purpose: Train one epoch\n",
    "model_basic = MedicalVQABaseline(\n",
    "    vocab_size=len(question_vocab),\n",
    "    num_classes=len(answer_vocab)\n",
    ").to(device)\n",
    "\n",
    "# 2. FREEZE the ResNet\n",
    "for param in model_basic.resnet_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Fine-tune only later layers instead of freezing it fully\n",
    "# for param in list(self.resnet_features.parameters())[:-20]:\n",
    "#     param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_basic.parameters()), lr=0.0003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0fe03b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 154/154 [00:33<00:00,  4.56it/s, acc=0.304, loss=2.67]\n",
      "Train: 100%|██████████| 154/154 [00:33<00:00,  4.59it/s, acc=0.522, loss=1.79]\n",
      "Train: 100%|██████████| 154/154 [00:34<00:00,  4.42it/s, acc=0.478, loss=1.53] \n",
      "Train: 100%|██████████| 154/154 [00:36<00:00,  4.28it/s, acc=0.609, loss=1.55] \n",
      "Train: 100%|██████████| 154/154 [00:36<00:00,  4.27it/s, acc=0.652, loss=0.839]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train_loss, train_acc = train_one_epoch(model_basic, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de451fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = MedicalVQABaseline(\n",
    "    vocab_size=len(question_vocab),\n",
    "    num_classes=len(answer_vocab)\n",
    ").to(device)\n",
    "\n",
    "# 2. FREEZE the ResNet (Crucial for Baseline stability)\n",
    "for param in new_model.resnet_features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer_new = torch.optim.Adam(filter(lambda p: p.requires_grad, new_model.parameters()), lr=0.0003)\n",
    "criterion_new = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7671a090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 154/154 [00:34<00:00,  4.42it/s, acc=0.391, loss=2.01] \n",
      "Train: 100%|██████████| 154/154 [00:38<00:00,  4.04it/s, acc=0.435, loss=1.68]\n",
      "Train: 100%|██████████| 154/154 [00:37<00:00,  4.08it/s, acc=0.652, loss=1.17] \n",
      "Train: 100%|██████████| 154/154 [00:37<00:00,  4.11it/s, acc=0.522, loss=1.37] \n",
      "Train: 100%|██████████| 154/154 [00:38<00:00,  4.05it/s, acc=0.478, loss=1.23] \n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    train_loss, train_acc = train_one_epoch(new_model, train_loader, criterion_new, optimizer_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02f873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
