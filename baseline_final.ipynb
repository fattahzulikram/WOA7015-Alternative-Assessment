{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc2e801",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32082585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import nltk\n",
    "import json\n",
    "import optuna\n",
    "import random\n",
    "import joblib\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Metrics libraries\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "from utils import plot_training_curves, plot_type_specific_comparison, plot_ngram_analysis, print_all_metrics\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf434d8",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "MAX_NODES_PER_QUESTION = 10\n",
    "\n",
    "# Directory Information\n",
    "DATA_DIR = \"data/\"\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\n",
    "VOCABS_PATH = os.path.join(DATA_DIR, 'vocabs/')\n",
    "HYPERPARAMETERS_RESULT_PATH = os.path.join(DATA_DIR, 'tuning/')\n",
    "FINAL_MODEL_PATH = os.path.join(DATA_DIR, 'final_model/')\n",
    "\n",
    "# Huggingface Repository Information\n",
    "repo_id = \"BoKelvin/SLAKE\"\n",
    "repo_type = \"dataset\"\n",
    "img_file = \"imgs.zip\"\n",
    "\n",
    "# Seeding\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ba9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed():\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    torch.manual_seed(GLOBAL_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(GLOBAL_SEED)\n",
    "        torch.cuda.manual_seed_all(GLOBAL_SEED)\n",
    "        # For deterministic CuDNN operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0dd1b",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for downloading and extracting ZIP file\n",
    "def download_and_store_ZIP(filename, save_dir):\n",
    "    print(f\"Fetching file {filename} from {repo_id} repo\")\n",
    "\n",
    "    try:\n",
    "        # Caches the file locally and returns the path to the cached file\n",
    "        cached_zip_path = hf_hub_download(\n",
    "          repo_id=repo_id,\n",
    "          filename=filename,\n",
    "          repo_type=repo_type\n",
    "        )\n",
    "        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Extract the contents\n",
    "        print(f\"Extracting to {save_dir}...\")\n",
    "        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "\n",
    "        print(\"Extraction complete.\")\n",
    "        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or extract {filename}: {e}\")\n",
    "\n",
    "# Scoping to English only\n",
    "def filter_language(original):\n",
    "    return original.filter(lambda data: data['q_lang'] == 'en')\n",
    "\n",
    "# Download and store the dataset\n",
    "def download_and_store_english_dataset():\n",
    "    print(f\"Downloading dataset from {repo_id} repo\")\n",
    "\n",
    "    # Load from Hugging Face\n",
    "    original = load_dataset(repo_id)\n",
    "\n",
    "    # Scope to English Only\n",
    "    original = filter_language(original)\n",
    "\n",
    "    # Show the dataset formatting\n",
    "    pprint(original)\n",
    "\n",
    "    # Save the original dataset\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        os.makedirs(DATASET_PATH)\n",
    "\n",
    "    original.save_to_disk(DATASET_PATH)\n",
    "    return original\n",
    "\n",
    "# Download and store the image files\n",
    "def download_and_store_image():\n",
    "    download_and_store_ZIP(img_file, DATA_DIR)\n",
    "\n",
    "# Download necessary files\n",
    "def download_and_store_slake():\n",
    "    dataset = download_and_store_english_dataset()\n",
    "    download_and_store_image()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00591e",
   "metadata": {},
   "source": [
    "### Vocabulary Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2e6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def build_word_vocabs(self, sentences):\n",
    "        counter = Counter()\n",
    "        start_index = len(self.stoi)\n",
    "\n",
    "        # 1. Count frequencies of all tokens in the tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        # 2. Add words that meet the frequency threshold\n",
    "        for word, count in counter.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = start_index\n",
    "                self.itos[start_index] = word\n",
    "                start_index += 1\n",
    "\n",
    "        print(f\"Vocabulary Built. Vocabulary Size: {len(self.stoi)}\")\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "            for token in tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies for questions and answers\n",
    "def build_vocabs(dataset):\n",
    "    questions = [item['question'] for item in dataset]\n",
    "    answers = [item['answer'] for item in dataset]\n",
    "\n",
    "    # Question Vocabulary\n",
    "    questvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "    questvocab_builder.build_word_vocabs(questions)\n",
    "    \n",
    "    # Answer Vocabulary\n",
    "    ansvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "\n",
    "    # Use a dummy tokenizer that just returns the whole lowercased string as one token\n",
    "    identity_tokenizer = lambda x: [x.lower().strip()]\n",
    "    ansvocab_builder.tokenize = identity_tokenizer\n",
    "\n",
    "    ansvocab_builder.build_word_vocabs(answers)\n",
    "\n",
    "    return questvocab_builder, ansvocab_builder\n",
    "\n",
    "# Save vocabularies to JSON files\n",
    "def save_vocabs(quest_vocab, ans_vocab):\n",
    "    if not os.path.exists(VOCABS_PATH):\n",
    "        os.makedirs(VOCABS_PATH)\n",
    "\n",
    "    # Save Question Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'question_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': quest_vocab.stoi, 'itos': quest_vocab.itos}, f)\n",
    "\n",
    "    # Save Answer Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'answer_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': ans_vocab.stoi, 'itos': ans_vocab.itos}, f)\n",
    "\n",
    "    print(\"Vocabularies saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416468b",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00620cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlakeDataset(Dataset):\n",
    "    def __init__(self, dataset, question_vocab, answer_vocab, transform=None, cache_images=True):\n",
    "        self.data = dataset\n",
    "        self.question_vocab = question_vocab\n",
    "        self.answer_vocab = answer_vocab\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "\n",
    "        # Caching\n",
    "        self.image_cache = {}\n",
    "        if self.cache_images:\n",
    "            print(f\"Caching images for into RAM...\")\n",
    "            # Get unique image names to avoid duplicate loading\n",
    "            unique_imgs = set(item['img_name'] for item in self.data)\n",
    "            \n",
    "            for img_name in unique_imgs:\n",
    "                path = os.path.join(IMAGE_PATH, img_name)\n",
    "                # Load and convert to RGB\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                \n",
    "                # Resize immediately to save RAM and CPU later\n",
    "                img = img.resize((224, 224)) \n",
    "                \n",
    "                self.image_cache[img_name] = img\n",
    "            print(f\"Cached {len(self.image_cache)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Image Processing\n",
    "        image_path = item['img_name']\n",
    "\n",
    "        if self.cache_images:\n",
    "            # Get from RAM\n",
    "            image = self.image_cache[image_path]\n",
    "        else:\n",
    "            # Load from Disk and Resize\n",
    "            img_path = os.path.join(IMAGE_PATH, image_path)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.resize((224, 224))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 2. Question Processing\n",
    "        question = item['question']\n",
    "        question_indices = self.question_vocab.numericalize(question)\n",
    "\n",
    "        # 3. Answer Processing\n",
    "        answer = str(item.get('answer', '')) # Answer may be missing in test set\n",
    "        answer_index = self.answer_vocab.numericalize(answer)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'question' : torch.tensor(question_indices),\n",
    "            'answer' : torch.tensor(answer_index, dtype=torch.long),\n",
    "            # Add original items for reference\n",
    "            'original_question': question,\n",
    "            'original_answer': answer,\n",
    "            # Add ID for tracking\n",
    "            'id': item['qid']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fff29b",
   "metadata": {},
   "source": [
    "### Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slake_collate_fn(batch, pad_index=0):\n",
    "    # Separate different components\n",
    "    images = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    original_questions = []\n",
    "    original_answers = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        questions.append(item['question'])\n",
    "        answers.append(item['answer'])\n",
    "        original_questions.append(item['original_question'])\n",
    "        original_answers.append(item['original_answer'])\n",
    "        ids.append(item['id'])\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images)  # [batch_size, 3, H, W]\n",
    "    \n",
    "    # Get question lengths BEFORE padding\n",
    "    question_lengths = torch.tensor([len(q) for q in questions])\n",
    "    \n",
    "    # Pad questions to the longest sequence in THIS batch\n",
    "    # pad_sequence expects list of tensors, pads with 0 by default\n",
    "    questions_padded = pad_sequence(questions, batch_first=True, padding_value=pad_index)\n",
    "    # questions_padded: [batch_size, max_len_in_batch]\n",
    "    \n",
    "    # Handling answers\n",
    "    # Handling each answer as a single class\n",
    "    # answers = torch.stack(answers)\n",
    "    answers = torch.tensor([item['answer'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'question': questions_padded,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer': answers,\n",
    "        'original_question': original_questions,\n",
    "        'original_answer': original_answers,\n",
    "        'id': ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8d268",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a6559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out if dataset is already downloaded\n",
    "# dataset = download_and_store_slake()\n",
    "\n",
    "# Uncomment if dataset is already downloaded\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Build vocabularies for training\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "question_vocab, answer_vocab = build_vocabs(train_data)\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create train dataset and dataloader\n",
    "train_dataset = SlakeDataset(train_data, question_vocab, answer_vocab, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=slake_collate_fn\n",
    ")\n",
    "\n",
    "validation_dataset = SlakeDataset(validation_data, question_vocab, answer_vocab, transform=transform)\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=slake_collate_fn\n",
    ")\n",
    "\n",
    "test_dataset = SlakeDataset(test_data, question_vocab, answer_vocab, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999865a2",
   "metadata": {},
   "source": [
    "## Modeling Baseline\n",
    "\n",
    "CNN with Bidirectional LSTM with Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c65f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM with Self-Attention for question encoding\n",
    "class BiLSTMWithSelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=1, \n",
    "                 dropout=0.5, pooling_strategy='mean', attention_heads=8):\n",
    "        super(BiLSTMWithSelfAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        # BiLSTM outputs hidden_dim * 2 (forward + backward)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=attention_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "    def forward(self, questions, question_lengths=None):\n",
    "        # Embed questions\n",
    "        embeds = self.embedding(questions)  # [B, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequence if lengths provided (for efficiency)\n",
    "        if question_lengths is not None:\n",
    "            embeds = nn.utils.rnn.pack_padded_sequence(\n",
    "                embeds, question_lengths.cpu(), \n",
    "                batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        \n",
    "        # BiLSTM encoding\n",
    "        lstm_out, (hidden, cell) = self.bilstm(embeds)\n",
    "        \n",
    "        # Unpack if needed\n",
    "        if question_lengths is not None:\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "                lstm_out, batch_first=True\n",
    "            )\n",
    "        \n",
    "        # lstm_out: [B, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Self-attention: query = key = value = lstm_out\n",
    "        attn_out, attn_weights = self.attention(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            need_weights=True\n",
    "        )\n",
    "        \n",
    "        # Residual connection + Layer Norm\n",
    "        attn_out = self.layer_norm(lstm_out + attn_out)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        \n",
    "        # Pooling strategy - experiment with these:\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            question_feature = attn_out.mean(dim=1)  # [B, hidden_dim * 2]\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            question_feature = attn_out.max(dim=1)[0]\n",
    "        else:\n",
    "            # Last hidden state (concatenate forward and backward)\n",
    "            question_feature = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        return question_feature, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a86567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete VQA model: ResNet34 + BiLSTM with Self-Attention\n",
    "class VQA_ResNet_BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, embed_dim=300, \n",
    "                 lstm_hidden=512, fusion_dim=1024, lstm_dropout=0.5, \n",
    "                 lstm_num_layers=1, attention_heads=8, fusion_dropout=0.5,\n",
    "                 pooling_strategy='mean'):\n",
    "        super(VQA_ResNet_BiLSTM_Attention, self).__init__()\n",
    "        \n",
    "        # Image encoder: ResNet34\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # Remove the final FC layer\n",
    "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.image_feature_dim = 512  # ResNet34 final layer\n",
    "        \n",
    "        # Question encoder: BiLSTM + Self-Attention\n",
    "        self.question_encoder = BiLSTMWithSelfAttention(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=lstm_hidden,\n",
    "            num_layers=lstm_num_layers,\n",
    "            dropout=lstm_dropout,\n",
    "            attention_heads=attention_heads,\n",
    "            pooling_strategy=pooling_strategy\n",
    "        )\n",
    "        self.question_feature_dim = lstm_hidden * 2  # Bidirectional\n",
    "        \n",
    "        # Multimodal fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim + self.question_feature_dim, fusion_dim),\n",
    "            nn.BatchNorm1d(fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fusion_dropout),\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.BatchNorm1d(fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fusion_dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n",
    "        \n",
    "    def forward(self, images, questions, question_lengths=None):\n",
    "        # Extract image features\n",
    "        img_features = self.image_encoder(images)  # [B, 512, 1, 1]\n",
    "        img_features = img_features.squeeze(-1).squeeze(-1)  # [B, 512]\n",
    "        \n",
    "        # Extract question features with attention\n",
    "        q_features, attn_weights = self.question_encoder(questions, question_lengths) # [B, lstm_hidden * 2]\n",
    "        \n",
    "        # Concatenate image and question features\n",
    "        combined = torch.cat([img_features, q_features], dim=1)\n",
    "        # combined: [B, 512 + lstm_hidden*2]\n",
    "        \n",
    "        # Fusion\n",
    "        fused = self.fusion(combined)  # [B, fusion_dim // 2]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)  # [B, num_classes]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc61c29",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        questions = batch['question'].to(device)\n",
    "        question_lengths = batch['question_lengths'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(images, questions, question_lengths)\n",
    "        loss = criterion(logits, answers)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validating'):\n",
    "            images = batch['image'].to(device)\n",
    "            questions = batch['question'].to(device)\n",
    "            question_lengths = batch['question_lengths'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "            \n",
    "            logits = model(images, questions, question_lengths)\n",
    "            loss = criterion(logits, answers)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    def __init__(self, train_dataset, validation_dataset, vocab_size, num_classes, \n",
    "                 n_trials=50):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_trials = n_trials\n",
    "        self.results_dir = Path(HYPERPARAMETERS_RESULT_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Track all trial results\n",
    "        self.trial_results = []\n",
    "\n",
    "    def config_BLSTM(self, trial):\n",
    "        return {\n",
    "            # Embedding parameters\n",
    "            'embed_dim': trial.suggest_categorical('embed_dim', [200, 300, 512]),\n",
    "\n",
    "            # LSTM parameters\n",
    "            'lstm_hidden': trial.suggest_categorical('lstm_hidden', [256, 512, 768, 1024]),\n",
    "            'lstm_num_layers': trial.suggest_int('lstm_num_layers', 1, 3),\n",
    "            'lstm_dropout': trial.suggest_float('lstm_dropout', 0.1, 0.6),\n",
    "            'pooling_strategy': trial.suggest_categorical('pooling_strategy', ['mean', 'max', 'last']),\n",
    "\n",
    "            # Attention parameters\n",
    "            'attention_heads': trial.suggest_categorical('attention_heads', [4, 8, 16]),\n",
    "\n",
    "            # Fusion parameters\n",
    "            'fusion_dim': trial.suggest_categorical('fusion_dim', [512, 1024, 2048]),\n",
    "            'fusion_dropout': trial.suggest_float('fusion_dropout', 0.2, 0.6),\n",
    "\n",
    "            # Training parameters\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
    "            'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n",
    "            'scheduler_step_size': trial.suggest_int('scheduler_step_size', 5, 15),\n",
    "            'scheduler_gamma': trial.suggest_float('scheduler_gamma', 0.3, 0.7),\n",
    "        }\n",
    "\n",
    "    def objective(self, trial):\n",
    "        print(f\"Trial {trial.number + 1}/{self.n_trials}\")\n",
    "\n",
    "        config = self.config_BLSTM(trial)\n",
    "        model = VQA_ResNet_BiLSTM_Attention(\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=config['embed_dim'],\n",
    "            lstm_hidden=config['lstm_hidden'],\n",
    "            lstm_num_layers=config['lstm_num_layers'],\n",
    "            attention_heads=config['attention_heads'],\n",
    "            fusion_dim=config['fusion_dim'],\n",
    "            lstm_dropout=config['lstm_dropout'],\n",
    "            fusion_dropout=config['fusion_dropout'],\n",
    "            pooling_strategy=config['pooling_strategy']\n",
    "        ).to(device)\n",
    "        \n",
    "        for param in model.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "        print(f\"Config: {json.dumps(config, indent=2)}\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.validation_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config['scheduler_step_size'],\n",
    "            gamma=config['scheduler_gamma']\n",
    "        )\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        threshold = 5\n",
    "        threshold_count = 0\n",
    "        max_epochs = 30\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer\n",
    "            )\n",
    "\n",
    "            val_loss, val_acc = validate(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                threshold_count = 0\n",
    "            else:\n",
    "                threshold_count += 1\n",
    "            \n",
    "            if threshold_count >= threshold:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        trial_result = {\n",
    "            'trial_number': trial.number,\n",
    "            'config': config,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'final_epoch': epoch + 1\n",
    "        }\n",
    "        self.trial_results.append(trial_result)\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def save_results(self, study):\n",
    "        # Save best parameters\n",
    "        best_params_path = self.results_dir / f'best_params_BLSTM.json'\n",
    "        with open(best_params_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'best_params': study.best_params,\n",
    "                'best_value': study.best_value,\n",
    "                'best_trial': study.best_trial.number\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        # Save all trial results\n",
    "        all_results_path = self.results_dir / f'all_trials_BLSTM.json'\n",
    "        with open(all_results_path, 'w') as f:\n",
    "            json.dump(self.trial_results, f, indent=2)\n",
    "        \n",
    "        # Save study\n",
    "        study_path = self.results_dir / f'study_BLSTM.pkl'\n",
    "        joblib.dump(study, study_path)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {self.results_dir}\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"STARTING HYPERPARAMETER TUNING FOR BLSTM MODEL\\n\")\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "            sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED)\n",
    "        )\n",
    "\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        self.save_results(study)\n",
    "\n",
    "        # Print best results\n",
    "        print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "        print(f\"Best Trial: {study.best_trial.number}\")\n",
    "        print(f\"Best Validation Accuracy: {study.best_value:.2f}%\\n\")\n",
    "        print(f\"Best Hyperparameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Hyperparameters for a BLSTM model\n",
    "tuner = HyperparameterTuner(\n",
    "    vocab_size=len(question_vocab),\n",
    "    num_classes=len(answer_vocab),\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    n_trials=50\n",
    ")\n",
    "\n",
    "# Run tuning\n",
    "study = tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018251b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameter for the model\n",
    "best_params_path = os.path.join(HYPERPARAMETERS_RESULT_PATH, 'best_params_BLSTM.json')\n",
    "with open(best_params_path, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3362a",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98886ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModelTrainer:\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, \n",
    "                 best_params, vocab_size, num_classes):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.best_params = best_params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        # Create results directory\n",
    "        self.results_dir = Path(FINAL_MODEL_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def final_evaluation(self, model):\n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Store all predictions and results\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_ids = []\n",
    "        \n",
    "        # Type-specific tracking\n",
    "        type_stats = {\n",
    "            'CLOSED': {'correct': 0, 'total': 0, 'predictions': [], 'targets': []},\n",
    "            'OPEN': {'correct': 0, 'total': 0, 'predictions': [], 'targets': []}\n",
    "        }\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Testing'):\n",
    "                images = batch['image'].to(device)\n",
    "                questions = batch['question'].to(device)\n",
    "                question_lengths = batch['question_lengths'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "                \n",
    "                logits = model(images, questions, question_lengths)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().tolist())\n",
    "                all_targets.extend(answers.cpu().tolist())\n",
    "                all_ids.extend(batch['id'])\n",
    "        \n",
    "        # Categorize by answer type\n",
    "        for pred, target, qid in zip(all_predictions, all_targets, all_ids):\n",
    "            # Find the question in the dataset\n",
    "            item = next((x for x in self.test_dataset.data if x['qid'] == qid), None)\n",
    "            \n",
    "            if item is not None:\n",
    "                answer_type = item.get('answer_type', 'OPEN').upper()\n",
    "                \n",
    "                # Ensure answer_type is in our tracking dict\n",
    "                if answer_type not in type_stats:\n",
    "                    type_stats[answer_type] = {\n",
    "                        'correct': 0, 'total': 0, \n",
    "                        'predictions': [], 'targets': []\n",
    "                    }\n",
    "                \n",
    "                type_stats[answer_type]['total'] += 1\n",
    "                type_stats[answer_type]['predictions'].append(pred)\n",
    "                type_stats[answer_type]['targets'].append(target)\n",
    "                \n",
    "                if pred == target:\n",
    "                    type_stats[answer_type]['correct'] += 1\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        overall_correct = sum(p == t for p, t in zip(all_predictions, all_targets))\n",
    "        overall_total = len(all_predictions)\n",
    "        overall_acc = 100 * overall_correct / overall_total if overall_total > 0 else 0\n",
    "        \n",
    "        type_accuracies = {}\n",
    "        for answer_type, stats in type_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                acc = 100 * stats['correct'] / stats['total']\n",
    "                type_accuracies[answer_type] = acc\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"DETAILED EVALUATION RESULTS\")\n",
    "        print(f\"Overall Accuracy: {overall_acc:.2f}% ({overall_correct}/{overall_total})\")\n",
    "        print(f\"\\nPerrformance on Answer Types:\")\n",
    "        \n",
    "        for answer_type in sorted(type_stats.keys()):\n",
    "            stats = type_stats[answer_type]\n",
    "            if stats['total'] > 0:\n",
    "                acc = type_accuracies[answer_type]\n",
    "                print(f\"  {answer_type:12s}: {acc:6.2f}% ({stats['correct']:4d}/{stats['total']:4d})\")\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        results = {\n",
    "            'overall_accuracy': overall_acc,\n",
    "            'overall_correct': overall_correct,\n",
    "            'overall_total': overall_total,\n",
    "            'type_accuracies': type_accuracies,\n",
    "            'type_stats': {\n",
    "                answer_type: {\n",
    "                    'accuracy': type_accuracies.get(answer_type, 0),\n",
    "                    'correct': stats['correct'],\n",
    "                    'total': stats['total']\n",
    "                }\n",
    "                for answer_type, stats in type_stats.items()\n",
    "            },\n",
    "            'predictions': all_predictions,\n",
    "            'targets': all_targets,\n",
    "            'ids': all_ids\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train(self, num_epochs=100, threshold=15, save_every=10):\n",
    "        print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
    "        print(f\"Training for up to {num_epochs} epochs\")\n",
    "        print(f\"Early stopping threshold: {threshold} epochs\")\n",
    "\n",
    "        print(f\"\\nBest hyperparameters:\")\n",
    "        print(json.dumps(self.best_params, indent=2))\n",
    "        \n",
    "        # 1. Create model with best hyperparameters\n",
    "        model = VQA_ResNet_BiLSTM_Attention(\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=self.best_params['best_params']['embed_dim'],\n",
    "            lstm_hidden=self.best_params['best_params']['lstm_hidden'],\n",
    "            lstm_num_layers=self.best_params['best_params']['lstm_num_layers'],\n",
    "            lstm_dropout=self.best_params['best_params']['lstm_dropout'],\n",
    "            pooling_strategy=self.best_params['best_params']['pooling_strategy'],\n",
    "            attention_heads=self.best_params['best_params']['attention_heads'],\n",
    "            fusion_dim=self.best_params['best_params']['fusion_dim'],\n",
    "            fusion_dropout=self.best_params['best_params']['fusion_dropout'],\n",
    "        ).to(device)\n",
    "        \n",
    "        # 2. Create dataloaders with best batch size\n",
    "        batch_size = self.best_params['best_params']['batch_size']\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 3. Setup optimizer and scheduler with best parameters\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.best_params['best_params']['learning_rate'],\n",
    "            weight_decay=self.best_params['best_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.best_params['best_params']['scheduler_step_size'],\n",
    "            gamma=self.best_params['best_params']['scheduler_gamma']\n",
    "        )\n",
    "        \n",
    "        # 4. Training loop\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "        threshold_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = validate(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "            \n",
    "            # Get current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                threshold_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                self.save_checkpoint(\n",
    "                    model, optimizer, epoch, val_acc, \n",
    "                    filename='best_model.pth'\n",
    "                )\n",
    "                print(f\"New best model found with Val Acc: {val_acc:.2f}%\")\n",
    "            else:\n",
    "                threshold_counter += 1\n",
    "                print(f\"No improvement ({threshold_counter}/{threshold})\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if (epoch + 1) % save_every == 0:\n",
    "                self.save_checkpoint(\n",
    "                    model, optimizer, epoch, val_acc,\n",
    "                    filename=f'checkpoint_epoch_{epoch+1}.pth'\n",
    "                )\n",
    "            \n",
    "            # Early stopping check\n",
    "            if threshold_counter >= threshold:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "                break\n",
    "        \n",
    "        # 5. Load best model and evaluate on test set\n",
    "        print(\"FINAL EVALUATION ON TEST SET\")\n",
    "        \n",
    "        self.load_checkpoint(model, 'best_model.pth')\n",
    "        test_results = self.final_evaluation(model)\n",
    "        \n",
    "        # 6. Save training history and results\n",
    "        self.save_results(test_results, best_epoch, best_val_acc)\n",
    "        \n",
    "        # 7. Plot training curves\n",
    "        plot_training_curves(\n",
    "            self.history,\n",
    "            FINAL_MODEL_PATH,\n",
    "            'baseline'\n",
    "        )\n",
    "        \n",
    "        print(\"TRAINING COMPLETE!\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "        print(f\"\\nTest Set Results:\")\n",
    "        print(f\"  Overall Accuracy: {test_results['overall_accuracy']:.2f}%\")\n",
    "        if 'type_accuracies' in test_results:\n",
    "            print(f\"\\n  By Answer Type:\")\n",
    "            for answer_type in sorted(test_results['type_accuracies'].keys()):\n",
    "                acc = test_results['type_accuracies'][answer_type]\n",
    "                total = test_results['type_stats'][answer_type]['total']\n",
    "                print(f\"    {answer_type:12s}: {acc:6.2f}% ({total:4d} samples)\")\n",
    "        print(f\"\\nResults saved to: {self.results_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return model, test_results\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, epoch, val_acc, filename):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'best_params': self.best_params,\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, self.results_dir / filename)\n",
    "    \n",
    "    def load_checkpoint(self, model, filename):\n",
    "        checkpoint = torch.load(self.results_dir / filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from {filename} (Epoch {checkpoint['epoch']}, Val Acc: {checkpoint['val_acc']:.2f}%)\")\n",
    "    \n",
    "    def save_results(self, test_results, best_epoch, best_val_acc):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        results = {\n",
    "            'timestamp': timestamp,\n",
    "            'best_hyperparameters': self.best_params,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'test_results': {\n",
    "                'overall_accuracy': test_results['overall_accuracy'],\n",
    "                'overall_correct': test_results['overall_correct'],\n",
    "                'overall_total': test_results['overall_total'],\n",
    "                'type_accuracies': test_results['type_accuracies'],\n",
    "                'type_stats': test_results['type_stats']\n",
    "            },\n",
    "            'training_history': self.history\n",
    "        }\n",
    "        \n",
    "        with open(self.results_dir / f'final_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14827f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_trainer = FinalModelTrainer(\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    test_dataset,\n",
    "    best_params,\n",
    "    len(question_vocab),\n",
    "    len(answer_vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, test_results = final_model_trainer.train(\n",
    "    num_epochs=100,\n",
    "    threshold=15,\n",
    "    save_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a5659",
   "metadata": {},
   "source": [
    "## Comprehensive Metrics Evaluation on Final Model\n",
    "\n",
    "The model will be compared against a generative model, so we need more metrics to find a common ground </br>\n",
    "Metrics that will be calculated and the reasonings: </br>\n",
    "1. Classification Metrics\n",
    "    * Accuracy: Overall correctness\n",
    "    * F1 Score: Both Macro and Weighted -> Balances between Precision and Recall scores\n",
    "2. Text Generation Metrics\n",
    "    * BLEU: Standard for NLP, captures exact correctness, checks N-gram overlap. Up to 4-grams will be calculated.\n",
    "    * METEOR: Semantic focused, accounts for synonyms\n",
    "    * BERTScore: Checks semantic similarities\n",
    "    * Rouge: Emphasizes recall\n",
    "    * Exact string matching: Strictly match string outputs\n",
    "3. Answer Type Specific Metrics\n",
    "    * Accuracy, F1 and exact matching for OPEN and CLOSED answer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllMetricsCalculator:\n",
    "    def __init__(self, model, test_dataset, answer_vocab):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.answer_vocab = answer_vocab\n",
    "\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.results_dir = Path(FINAL_MODEL_PATH)\n",
    "\n",
    "    def get_predictions(self, batch_size):\n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_pred_texts = []\n",
    "        all_target_texts = []\n",
    "        all_question_types = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Getting predictions'):\n",
    "                images = batch['image'].to(device)\n",
    "                questions = batch['question'].to(device)\n",
    "                question_lengths = batch['question_lengths'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits = self.model(images, questions, question_lengths)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                # Convert to text\n",
    "                for pred_idx, target_idx, qid in zip(predictions.cpu().tolist(), \n",
    "                                                       answers.cpu().tolist(), \n",
    "                                                       batch['id']):\n",
    "                    pred_text = self.answer_vocab.itos.get(pred_idx, '<unk>')\n",
    "                    target_text = self.answer_vocab.itos.get(target_idx, '<unk>')\n",
    "                    \n",
    "                    all_predictions.append(pred_idx)\n",
    "                    all_targets.append(target_idx)\n",
    "                    all_pred_texts.append(pred_text)\n",
    "                    all_target_texts.append(target_text)\n",
    "                    \n",
    "                    # Get question type\n",
    "                    item = next((x for x in self.test_dataset.data if x['qid'] == qid), None)\n",
    "                    if item:\n",
    "                        q_type = item.get('answer_type', 'UNKNOWN').upper()\n",
    "                        all_question_types.append(q_type)\n",
    "                    else:\n",
    "                        all_question_types.append('UNKNOWN')\n",
    "        \n",
    "        return all_predictions, all_targets, all_pred_texts, all_target_texts, all_question_types\n",
    "    \n",
    "    def calculate_classification_metrics(self, predictions, targets):\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(targets, predictions) * 100,\n",
    "            'macro_f1': f1_score(targets, predictions, average='macro') * 100,\n",
    "            'weighted_f1': f1_score(targets, predictions, average='weighted') * 100,\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_bleu_scores(self, predictions, references):\n",
    "        smoothing = SmoothingFunction().method1\n",
    "        \n",
    "        bleu_scores = {\n",
    "            'bleu1': [],\n",
    "            'bleu2': [],\n",
    "            'bleu3': [],\n",
    "            'bleu4': []\n",
    "        }\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Tokenize\n",
    "            pred_tokens = pred.lower().split()\n",
    "            ref_tokens = [ref.lower().split()]\n",
    "            \n",
    "            # Calculate BLEU scores up to 4-grams\n",
    "            try:\n",
    "                bleu1 = sentence_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "                bleu2 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "                bleu3 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "                bleu4 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "                \n",
    "                bleu_scores['bleu1'].append(bleu1)\n",
    "                bleu_scores['bleu2'].append(bleu2)\n",
    "                bleu_scores['bleu3'].append(bleu3)\n",
    "                bleu_scores['bleu4'].append(bleu4)\n",
    "            except:\n",
    "                bleu_scores['bleu1'].append(0.0)\n",
    "                bleu_scores['bleu2'].append(0.0)\n",
    "                bleu_scores['bleu3'].append(0.0)\n",
    "                bleu_scores['bleu4'].append(0.0)\n",
    "        \n",
    "        # Average scores\n",
    "        metrics = {\n",
    "            'bleu1': np.mean(bleu_scores['bleu1']) * 100,\n",
    "            'bleu2': np.mean(bleu_scores['bleu2']) * 100,\n",
    "            'bleu3': np.mean(bleu_scores['bleu3']) * 100,\n",
    "            'bleu4': np.mean(bleu_scores['bleu4']) * 100,\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_rouge_scores(self, predictions, references):\n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            scores = self.rouge_scorer.score(ref, pred)\n",
    "            rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "            rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "            rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        return {\n",
    "            'rouge1': np.mean(rouge1_scores) * 100,\n",
    "            'rouge2': np.mean(rouge2_scores) * 100,\n",
    "            'rougeL': np.mean(rougeL_scores) * 100,\n",
    "        }\n",
    "    \n",
    "    def calculate_meteor_score(self, predictions, references):\n",
    "        meteor_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_tokens = pred.lower().split()\n",
    "            ref_tokens = ref.lower().split()\n",
    "            \n",
    "            try:\n",
    "                score = meteor_score([ref_tokens], pred_tokens)\n",
    "                meteor_scores.append(score)\n",
    "            except:\n",
    "                meteor_scores.append(0.0)\n",
    "        \n",
    "        return {\n",
    "            'meteor': np.mean(meteor_scores) * 100\n",
    "        }\n",
    "    \n",
    "    def calculate_bertscore(self, predictions, references):\n",
    "        # Using a lightweight model for faster computation instead of full BERT\n",
    "        P, R, F1 = bert_score(\n",
    "            predictions, \n",
    "            references, \n",
    "            lang='en',\n",
    "            model_type='distilbert-base-uncased',\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'bertscore_precision': P.mean().item() * 100,\n",
    "            'bertscore_recall': R.mean().item() * 100,\n",
    "            'bertscore_f1': F1.mean().item() * 100,\n",
    "        }\n",
    "    \n",
    "    def calculate_exact_match(self, predictions, references):\n",
    "        exact_matches = sum(pred.lower().strip() == ref.lower().strip() \n",
    "                          for pred, ref in zip(predictions, references))\n",
    "        \n",
    "        return {\n",
    "            'exact_match': (exact_matches / len(predictions)) * 100\n",
    "        }\n",
    "    \n",
    "    def calculate_type_specific_metrics(self, predictions, targets, pred_texts, target_texts, question_types):\n",
    "        type_metrics = defaultdict(lambda: {\n",
    "            'predictions': [],\n",
    "            'targets': [],\n",
    "            'pred_texts': [],\n",
    "            'target_texts': []\n",
    "        })\n",
    "        \n",
    "        # Group by type\n",
    "        for pred, target, pred_text, target_text, q_type in zip(\n",
    "            predictions, targets, pred_texts, target_texts, question_types\n",
    "        ):\n",
    "            type_metrics[q_type]['predictions'].append(pred)\n",
    "            type_metrics[q_type]['targets'].append(target)\n",
    "            type_metrics[q_type]['pred_texts'].append(pred_text)\n",
    "            type_metrics[q_type]['target_texts'].append(target_text)\n",
    "        \n",
    "        # Calculate metrics for each type\n",
    "        results = {}\n",
    "        for q_type, data in type_metrics.items():\n",
    "            if len(data['predictions']) > 0:\n",
    "                results[q_type] = {\n",
    "                    'accuracy': accuracy_score(data['targets'], data['predictions']) * 100,\n",
    "                    'f1': f1_score(data['targets'], data['predictions'], average='macro', zero_division=0) * 100,\n",
    "                    'exact_match': sum(p.lower() == t.lower() for p, t in zip(data['pred_texts'], data['target_texts'])) / len(data['pred_texts']) * 100,\n",
    "                    'count': len(data['predictions'])\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def evaluate_all_metrics(self, batch_size):\n",
    "        print(\"CALCULATING ALL METRICS\")\n",
    "\n",
    "        # Get predictions and ground truth\n",
    "        predictions, targets, pred_texts, target_texts, question_types = self.get_predictions(batch_size)\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        metrics = {}\n",
    "        \n",
    "        # 1. Classification Metrics (Accuracy, F1, Precision, Recall)\n",
    "        print(\"\\nCalculating Classification Metrics...\")\n",
    "        metrics['classification'] = self.calculate_classification_metrics(predictions, targets)\n",
    "        \n",
    "        # 2. BLEU Scores\n",
    "        print(\"Calculating BLEU (1-4) Scores...\")\n",
    "        metrics['bleu'] = self.calculate_bleu_scores(pred_texts, target_texts)\n",
    "\n",
    "        # 3. Rouge Scores\n",
    "        print(\"Calculating ROUGE Scores...\")\n",
    "        metrics['rouge'] = self.calculate_rouge_scores(pred_texts, target_texts)\n",
    "        \n",
    "        # 3. METEOR Score\n",
    "        print(\"Calculating METEOR Score...\")\n",
    "        metrics['meteor'] = self.calculate_meteor_score(pred_texts, target_texts)\n",
    "        \n",
    "        # 4. BERTScore\n",
    "        print(\"Calculating BERTScore... (Might take some time)\")\n",
    "        metrics['bertscore'] = self.calculate_bertscore(pred_texts, target_texts)\n",
    "        \n",
    "        # 5. Exact Match\n",
    "        print(\"Calculating Exact Match...\")\n",
    "        metrics['exact_match'] = self.calculate_exact_match(pred_texts, target_texts)\n",
    "        \n",
    "        # 6. Type-specific metrics\n",
    "        print(\"Calculating Type-Specific Metrics...\")\n",
    "        metrics['by_type'] = self.calculate_type_specific_metrics(\n",
    "            predictions, targets, pred_texts, target_texts, question_types\n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print_all_metrics(metrics)\n",
    "        \n",
    "        # Save results\n",
    "        self.save_metrics(metrics)\n",
    "        \n",
    "        # Create comparison plots\n",
    "        plot_type_specific_comparison(metrics['by_type'], FINAL_MODEL_PATH)\n",
    "        plot_ngram_analysis(metrics['bleu'], FINAL_MODEL_PATH)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def save_metrics(self, metrics):\n",
    "        output_file = self.results_dir / 'all_metrics_baseline.json'\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nAll Metrics saved to: {output_file}\")\n",
    "\n",
    "    def load_metrics(self):\n",
    "        input_file = self.results_dir / 'all_metrics_baseline.json'\n",
    "\n",
    "        with open(input_file, 'r') as f:\n",
    "            return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15633d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = VQA_ResNet_BiLSTM_Attention(\n",
    "    vocab_size=len(question_vocab),\n",
    "    num_classes=len(answer_vocab),\n",
    "    embed_dim=best_params['best_params']['embed_dim'],\n",
    "    lstm_hidden=best_params['best_params']['lstm_hidden'],\n",
    "    lstm_num_layers=best_params['best_params']['lstm_num_layers'],\n",
    "    lstm_dropout=best_params['best_params']['lstm_dropout'],\n",
    "    pooling_strategy=best_params['best_params']['pooling_strategy'],\n",
    "    attention_heads=best_params['best_params']['attention_heads'],\n",
    "    fusion_dim=best_params['best_params']['fusion_dim'],\n",
    "    fusion_dropout=best_params['best_params']['fusion_dropout'],\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(FINAL_MODEL_PATH, 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0323c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator\n",
    "evaluator = AllMetricsCalculator(\n",
    "    model=model,\n",
    "    test_dataset=test_dataset,\n",
    "    answer_vocab=answer_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a930c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluator.evaluate_all_metrics(batch_size=best_params['best_params']['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad3ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=best_params['best_params']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=slake_collate_fn,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_pred_texts = []\n",
    "all_target_texts = []\n",
    "all_question_types = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = batch['image'].to(device)\n",
    "    questions = batch['question'].to(device)\n",
    "    question_lengths = batch['question_lengths'].to(device)\n",
    "    answers = batch['answer'].to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(images, questions, question_lengths)\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Convert to text\n",
    "    for pred_idx, target_idx, qid in zip(predictions.cpu().tolist(), \n",
    "                                            answers.cpu().tolist(), \n",
    "                                            batch['id']):\n",
    "        pred_text = answer_vocab.itos.get(pred_idx, '<unk>')\n",
    "        target_text = answer_vocab.itos.get(target_idx, '<unk>')\n",
    "        \n",
    "        all_predictions.append(pred_idx)\n",
    "        all_targets.append(target_idx)\n",
    "        all_pred_texts.append(pred_text)\n",
    "        all_target_texts.append(target_text)\n",
    "        \n",
    "        # Get question type\n",
    "        item = next((x for x in test_dataset.data if x['qid'] == qid), None)\n",
    "        if item:\n",
    "            q_type = item.get('answer_type', 'UNKNOWN').upper()\n",
    "            all_question_types.append(q_type)\n",
    "        else:\n",
    "            all_question_types.append('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a817744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictions\n",
    "predictions, targets, pred_texts, target_texts, question_types = evaluator.get_predictions(batch_size=best_params['best_params']['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [item['question'] for item in test_dataset.data if item is not None]\n",
    "images = [item['img_name'] for item in test_dataset.data if item is not None]\n",
    "\n",
    "# Print a few to verify\n",
    "print(\"Sample Questions:\")\n",
    "for i in range(min(5, len(questions))):\n",
    "    print(f\"  Question: {questions[i]}\")\n",
    "    print(f\"  Predicted Answer: {pred_texts[i]}\")\n",
    "    print(f\"  Ground Truth Answer: {target_texts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e39ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all wrong predictions\n",
    "wrong_predictions = []\n",
    "for i in range(len(pred_texts)):\n",
    "    if pred_texts[i].lower().strip() != target_texts[i].lower().strip():\n",
    "        wrong_predictions.append({\n",
    "            'question': questions[i],\n",
    "            'ground_truth': target_texts[i],\n",
    "            'prediction': pred_texts[i],\n",
    "            'image': images[i],\n",
    "            'question_type': question_types[i]\n",
    "        })\n",
    "\n",
    "len(wrong_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionalErrorAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.directional_terms = {\n",
    "            'lateral': ['left', 'right'],\n",
    "            'vertical': ['upper', 'lower', 'top', 'bottom'],\n",
    "            'medial': ['central', 'center', 'middle']\n",
    "        }\n",
    "    \n",
    "    def extract_directional_info(self, text):\n",
    "        text_lower = text.lower().strip()\n",
    "        \n",
    "        directions = {\n",
    "            'lateral': [],\n",
    "            'vertical': [],\n",
    "            'medial': []\n",
    "        }\n",
    "        \n",
    "        for direction_type, terms in self.directional_terms.items():\n",
    "            for term in terms:\n",
    "                if re.search(r'\\b' + term + r'\\b', text_lower):\n",
    "                    directions[direction_type].append(term)\n",
    "        \n",
    "        return directions\n",
    "    \n",
    "    def compare_directions(self, gt_directions, pred_directions):\n",
    "        for direction_type in ['lateral', 'vertical', 'medial']:\n",
    "            gt_terms_array = gt_directions[direction_type]\n",
    "            pred_terms_array = pred_directions[direction_type]\n",
    "            gt_terms = set(gt_directions[direction_type])\n",
    "            pred_terms = set(pred_directions[direction_type])\n",
    "\n",
    "            if direction_type == 'medial':\n",
    "                if not gt_terms and pred_terms:\n",
    "                    return 'extra_medial_direction'\n",
    "                elif gt_terms and not pred_terms:\n",
    "                    return 'missing_medial_direction'\n",
    "                \n",
    "            if direction_type == 'lateral':\n",
    "                if len(gt_terms_array) < len(pred_terms_array):\n",
    "                    return 'extra_lateral_direction'\n",
    "                elif len(gt_terms_array) > len(pred_terms_array):\n",
    "                    return 'missing_lateral_direction'\n",
    "                else:\n",
    "                    for i in range(len(gt_terms_array)):\n",
    "                        if gt_terms_array[i] != pred_terms_array[i]:\n",
    "                            return 'left_right_confusion'\n",
    "                        \n",
    "            if direction_type == 'vertical':\n",
    "                if len(gt_terms_array) < len(pred_terms_array):\n",
    "                    return 'extra_vertical_direction'\n",
    "                elif len(gt_terms_array) > len(pred_terms_array):\n",
    "                    return 'missing_vertical_direction'\n",
    "                else:\n",
    "                    for i in range(len(gt_terms_array)):\n",
    "                        if gt_terms_array[i] != pred_terms_array[i]:\n",
    "                            return 'upper_lower_confusion'\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def has_directional_info(self, text):\n",
    "        directions = self.extract_directional_info(text)\n",
    "        return any(len(terms) > 0 for terms in directions.values())\n",
    "    \n",
    "    def analyze_predictions(self, predictions, ground_truths, questions, images):\n",
    "        results = {\n",
    "            'errors': {\n",
    "                'left_right_confusion': [],\n",
    "                'upper_lower_confusion': [],\n",
    "                'partial_match': [],\n",
    "                'extra_medial_direction': [],\n",
    "                'missing_medial_direction': [],\n",
    "                'missing_lateral_direction': [],\n",
    "                'extra_lateral_direction': [],\n",
    "                'missing_vertical_direction': [],\n",
    "                'extra_vertical_direction': [],\n",
    "                'missing_direction': [],\n",
    "                'extra_direction': []\n",
    "            },\n",
    "            'correct': [],\n",
    "            'non_directional': []\n",
    "        }\n",
    "        \n",
    "        for pred, gt, question, img in zip(predictions, ground_truths, questions, images):\n",
    "            pred_lower = pred.lower().strip()\n",
    "            gt_lower = gt.lower().strip()\n",
    "            \n",
    "            has_gt_direction = self.has_directional_info(gt)\n",
    "            has_pred_direction = self.has_directional_info(pred)\n",
    "            \n",
    "            if not has_gt_direction and not has_pred_direction:\n",
    "                results['non_directional'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred,\n",
    "                    'correct': pred_lower == gt_lower\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            if not has_gt_direction and has_pred_direction:\n",
    "                results['errors']['extra_direction'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            if pred_lower == gt_lower:\n",
    "                results['correct'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            gt_directions = self.extract_directional_info(gt)\n",
    "            pred_directions = self.extract_directional_info(pred)\n",
    "            \n",
    "            if not has_pred_direction:\n",
    "                results['errors']['missing_direction'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred,\n",
    "                    'gt_directions': gt_directions\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            error_type = self.compare_directions(gt_directions, pred_directions)\n",
    "            \n",
    "            if error_type:\n",
    "                results['errors'][error_type].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred,\n",
    "                    'gt_directions': gt_directions,\n",
    "                    'pred_directions': pred_directions\n",
    "                })\n",
    "            else:\n",
    "                if gt_directions != pred_directions:\n",
    "                    results['errors']['partial_match'].append({\n",
    "                        'image': img,\n",
    "                        'question': question,\n",
    "                        'ground_truth': gt,\n",
    "                        'prediction': pred,\n",
    "                        'gt_directions': gt_directions,\n",
    "                        'pred_directions': pred_directions\n",
    "                    })\n",
    "                else:\n",
    "                    results['correct'].append({\n",
    "                        'image': img,\n",
    "                        'question': question,\n",
    "                        'ground_truth': gt,\n",
    "                        'prediction': pred,\n",
    "                        'note': 'Directionally correct but overall wrong'\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_statistics(self, results):\n",
    "        total_directional = (\n",
    "            len(results['correct']) + \n",
    "            sum(len(errors) for errors in results['errors'].values())\n",
    "        )\n",
    "        \n",
    "        if total_directional == 0:\n",
    "            return None\n",
    "        \n",
    "        stats = {\n",
    "            'total_directional_questions': total_directional,\n",
    "            'correct': len(results['correct']),\n",
    "            'directional_accuracy': 100 * len(results['correct']) / total_directional,\n",
    "            'error_breakdown': {}\n",
    "        }\n",
    "        \n",
    "        for error_type, errors in results['errors'].items():\n",
    "            count = len(errors)\n",
    "            stats['error_breakdown'][error_type] = {\n",
    "                'count': count,\n",
    "                'percentage': 100 * count / total_directional\n",
    "            }\n",
    "        \n",
    "        total_errors = sum(len(errors) for errors in results['errors'].values())\n",
    "        stats['total_errors'] = total_errors\n",
    "        stats['error_rate'] = 100 * total_errors / total_directional\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_analysis(self, results, stats):\n",
    "        print(\"DIRECTIONAL ERROR ANALYSIS\")\n",
    "        \n",
    "        if stats is None:\n",
    "            print(\"No directional questions found in dataset.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nTotal questions with directional info: {stats['total_directional_questions']}\")\n",
    "        print(f\"Correct: {stats['correct']} ({stats['directional_accuracy']:.2f}%)\")\n",
    "        print(f\"Errors: {stats['total_errors']} ({stats['error_rate']:.2f}%)\")\n",
    "        \n",
    "        print(\"Error Breakdown:\")\n",
    "        \n",
    "        sorted_errors = sorted(\n",
    "            stats['error_breakdown'].items(),\n",
    "            key=lambda x: x[1]['count'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for error_type, error_stats in sorted_errors:\n",
    "            if error_stats['count'] > 0:\n",
    "                error_name = error_type.replace('_', ' ').title()\n",
    "                print(f\"  {error_name:30s}: {error_stats['count']:3d} ({error_stats['percentage']:5.2f}%)\")\n",
    "        \n",
    "        for error_type, errors in results['errors'].items():\n",
    "            if len(errors) > 0:\n",
    "                print(f\"\\n {error_type.replace('_', ' ').title()} Examples:\")\n",
    "                for i, example in enumerate(errors[:3]):\n",
    "                    print(f\"\\n  Example {i+1}:\")\n",
    "                    print(f\"    Question:     {example['question']}\")\n",
    "                    print(f\"    Ground Truth: {example['ground_truth']}\")\n",
    "                    print(f\"    Prediction:   {example['prediction']}\")\n",
    "                    \n",
    "                    if 'gt_directions' in example and 'pred_directions' in example:\n",
    "                        print(f\"    GT Directions: {example['gt_directions']}\")\n",
    "                        print(f\"    Pred Directions: {example['pred_directions']}\")\n",
    "    \n",
    "    def save_detailed_report(self, results, stats, model_name, output_dir=FINAL_MODEL_PATH):\n",
    "        from pathlib import Path\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        report = {\n",
    "            'model': model_name,\n",
    "            'statistics': stats,\n",
    "            'examples': {}\n",
    "        }\n",
    "        \n",
    "        # Add examples for each error type\n",
    "        for error_type, errors in results['errors'].items():\n",
    "            report['examples'][error_type] = errors[:10]  # Save first 10 of each type\n",
    "        \n",
    "        # Add correct examples\n",
    "        report['examples']['correct'] = results['correct'][:10]\n",
    "        \n",
    "        output_file = output_dir / f'{model_name.lower()}_directional_analysis.json'\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(f\"Detailed report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be913e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_biomedblip_directional_errors(predictions, ground_truths, questions, images):\n",
    "    analyzer = DirectionalErrorAnalyzer()\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyzer.analyze_predictions(predictions, ground_truths, questions, images)\n",
    "    stats = analyzer.calculate_statistics(results)\n",
    "    \n",
    "    # Print results\n",
    "    analyzer.print_analysis(results, stats)\n",
    "    \n",
    "    # Save detailed report\n",
    "    analyzer.save_detailed_report(results, stats, 'Baseline')\n",
    "    \n",
    "    return results, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dabd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, stats = analyze_biomedblip_directional_errors(\n",
    "    pred_texts,\n",
    "    target_texts,\n",
    "    questions,\n",
    "    images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5c28e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
