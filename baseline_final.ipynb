{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc2e801",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32082585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import nltk\n",
    "import json\n",
    "import optuna\n",
    "import random\n",
    "import joblib\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf434d8",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a18d6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "MAX_NODES_PER_QUESTION = 10\n",
    "\n",
    "# Directory Information\n",
    "DATA_DIR = \"data/\"\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\n",
    "VOCABS_PATH = os.path.join(DATA_DIR, 'vocabs/')\n",
    "HYPERPARAMETERS_RESULT_PATH = os.path.join(DATA_DIR, 'tuning/')\n",
    "FINAL_MODEL_PATH = os.path.join(DATA_DIR, 'final_model/')\n",
    "\n",
    "# Huggingface Repository Information\n",
    "repo_id = \"BoKelvin/SLAKE\"\n",
    "repo_type = \"dataset\"\n",
    "img_file = \"imgs.zip\"\n",
    "\n",
    "# Seeding\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc6ba9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed():\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    torch.manual_seed(GLOBAL_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(GLOBAL_SEED)\n",
    "        torch.cuda.manual_seed_all(GLOBAL_SEED)\n",
    "        # For deterministic CuDNN operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0dd1b",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b40e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for downloading and extracting ZIP file\n",
    "def download_and_store_ZIP(filename, save_dir):\n",
    "    print(f\"Fetching file {filename} from {repo_id} repo\")\n",
    "\n",
    "    try:\n",
    "        # Caches the file locally and returns the path to the cached file\n",
    "        cached_zip_path = hf_hub_download(\n",
    "          repo_id=repo_id,\n",
    "          filename=filename,\n",
    "          repo_type=repo_type\n",
    "        )\n",
    "        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Extract the contents\n",
    "        print(f\"Extracting to {save_dir}...\")\n",
    "        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "\n",
    "        print(\"Extraction complete.\")\n",
    "        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or extract {filename}: {e}\")\n",
    "\n",
    "# Scoping to English only\n",
    "def filter_language(original):\n",
    "    return original.filter(lambda data: data['q_lang'] == 'en')\n",
    "\n",
    "# Download and store the dataset\n",
    "def download_and_store_english_dataset():\n",
    "    print(f\"Downloading dataset from {repo_id} repo\")\n",
    "\n",
    "    # Load from Hugging Face\n",
    "    original = load_dataset(repo_id)\n",
    "\n",
    "    # Scope to English Only\n",
    "    original = filter_language(original)\n",
    "\n",
    "    # Show the dataset formatting\n",
    "    pprint(original)\n",
    "\n",
    "    # Save the original dataset\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        os.makedirs(DATASET_PATH)\n",
    "\n",
    "    original.save_to_disk(DATASET_PATH)\n",
    "    return original\n",
    "\n",
    "# Download and store the image files\n",
    "def download_and_store_image():\n",
    "    download_and_store_ZIP(img_file, DATA_DIR)\n",
    "\n",
    "# Download necessary files\n",
    "def download_and_store_slake():\n",
    "    dataset = download_and_store_english_dataset()\n",
    "    download_and_store_image()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00591e",
   "metadata": {},
   "source": [
    "### Vocabulary Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b2e6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def build_word_vocabs(self, sentences):\n",
    "        counter = Counter()\n",
    "        start_index = len(self.stoi)\n",
    "\n",
    "        # 1. Count frequencies of all tokens in the tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        # 2. Add words that meet the frequency threshold\n",
    "        for word, count in counter.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = start_index\n",
    "                self.itos[start_index] = word\n",
    "                start_index += 1\n",
    "\n",
    "        print(f\"Vocabulary Built. Vocabulary Size: {len(self.stoi)}\")\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "            for token in tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe25ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies for questions and answers\n",
    "def build_vocabs(dataset):\n",
    "    questions = [item['question'] for item in dataset]\n",
    "    answers = [item['answer'] for item in dataset]\n",
    "\n",
    "    # Question Vocabulary\n",
    "    questvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "    questvocab_builder.build_word_vocabs(questions)\n",
    "    \n",
    "    # Answer Vocabulary\n",
    "    ansvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "\n",
    "    # Use a dummy tokenizer that just returns the whole lowercased string as one token\n",
    "    identity_tokenizer = lambda x: [x.lower().strip()]\n",
    "    ansvocab_builder.tokenize = identity_tokenizer\n",
    "\n",
    "    ansvocab_builder.build_word_vocabs(answers)\n",
    "\n",
    "    return questvocab_builder, ansvocab_builder\n",
    "\n",
    "# Save vocabularies to JSON files\n",
    "def save_vocabs(quest_vocab, ans_vocab):\n",
    "    if not os.path.exists(VOCABS_PATH):\n",
    "        os.makedirs(VOCABS_PATH)\n",
    "\n",
    "    # Save Question Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'question_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': quest_vocab.stoi, 'itos': quest_vocab.itos}, f)\n",
    "\n",
    "    # Save Answer Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'answer_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': ans_vocab.stoi, 'itos': ans_vocab.itos}, f)\n",
    "\n",
    "    print(\"Vocabularies saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416468b",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00620cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlakeDataset(Dataset):\n",
    "    def __init__(self, dataset, question_vocab, answer_vocab, transform=None, cache_images=True):\n",
    "        self.data = dataset\n",
    "        self.question_vocab = question_vocab\n",
    "        self.answer_vocab = answer_vocab\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "\n",
    "        # Caching\n",
    "        self.image_cache = {}\n",
    "        if self.cache_images:\n",
    "            print(f\"Caching images for into RAM...\")\n",
    "            # Get unique image names to avoid duplicate loading\n",
    "            unique_imgs = set(item['img_name'] for item in self.data)\n",
    "            \n",
    "            for img_name in unique_imgs:\n",
    "                path = os.path.join(IMAGE_PATH, img_name)\n",
    "                # Load and convert to RGB\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                \n",
    "                # Resize immediately to save RAM and CPU later\n",
    "                img = img.resize((224, 224)) \n",
    "                \n",
    "                self.image_cache[img_name] = img\n",
    "            print(f\"Cached {len(self.image_cache)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Image Processing\n",
    "        image_path = item['img_name']\n",
    "\n",
    "        if self.cache_images:\n",
    "            # Get from RAM\n",
    "            image = self.image_cache[image_path]\n",
    "        else:\n",
    "            # Load from Disk and Resize\n",
    "            img_path = os.path.join(IMAGE_PATH, image_path)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.resize((224, 224))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 2. Question Processing\n",
    "        question = item['question']\n",
    "        question_indices = self.question_vocab.numericalize(question)\n",
    "\n",
    "        # 3. Answer Processing\n",
    "        answer = str(item.get('answer', '')) # Answer may be missing in test set\n",
    "        answer_index = self.answer_vocab.numericalize(answer)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'question' : torch.tensor(question_indices),\n",
    "            'answer' : torch.tensor(answer_index, dtype=torch.long),\n",
    "            # Add original items for reference\n",
    "            'original_question': question,\n",
    "            'original_answer': answer,\n",
    "            # Add ID for tracking\n",
    "            'id': item['qid']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fff29b",
   "metadata": {},
   "source": [
    "### Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9a4154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slake_collate_fn(batch, pad_index=0):\n",
    "    # Separate different components\n",
    "    images = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    original_questions = []\n",
    "    original_answers = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        questions.append(item['question'])\n",
    "        answers.append(item['answer'])\n",
    "        original_questions.append(item['original_question'])\n",
    "        original_answers.append(item['original_answer'])\n",
    "        ids.append(item['id'])\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images)  # [batch_size, 3, H, W]\n",
    "    \n",
    "    # Get question lengths BEFORE padding\n",
    "    question_lengths = torch.tensor([len(q) for q in questions])\n",
    "    \n",
    "    # Pad questions to the longest sequence in THIS batch\n",
    "    # pad_sequence expects list of tensors, pads with 0 by default\n",
    "    questions_padded = pad_sequence(questions, batch_first=True, padding_value=pad_index)\n",
    "    # questions_padded: [batch_size, max_len_in_batch]\n",
    "    \n",
    "    # Handling answers\n",
    "    # Handling each answer as a single class\n",
    "    # answers = torch.stack(answers)\n",
    "    answers = torch.tensor([item['answer'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'question': questions_padded,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer': answers,\n",
    "        'original_question': original_questions,\n",
    "        'original_answer': original_answers,\n",
    "        'id': ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8d268",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d5a6559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Built. Vocabulary Size: 281\n",
      "Vocabulary Built. Vocabulary Size: 225\n",
      "Caching images for into RAM...\n",
      "Cached 450 images.\n",
      "Caching images for into RAM...\n",
      "Cached 96 images.\n",
      "Caching images for into RAM...\n",
      "Cached 96 images.\n"
     ]
    }
   ],
   "source": [
    "# Comment out if dataset is already downloaded\n",
    "# dataset = download_and_store_slake()\n",
    "\n",
    "# Uncomment if dataset is already downloaded\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Build vocabularies for training\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "question_vocab, answer_vocab = build_vocabs(train_data)\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create train dataset and dataloader\n",
    "train_dataset = SlakeDataset(train_data, question_vocab, answer_vocab, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=slake_collate_fn\n",
    ")\n",
    "\n",
    "validation_dataset = SlakeDataset(validation_data, question_vocab, answer_vocab, transform=transform)\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=slake_collate_fn\n",
    ")\n",
    "\n",
    "test_dataset = SlakeDataset(test_data, question_vocab, answer_vocab, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999865a2",
   "metadata": {},
   "source": [
    "## Modeling Baseline\n",
    "\n",
    "CNN with Bidirectional LSTM with Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19c65f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM with Self-Attention for question encoding\n",
    "class BiLSTMWithSelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=1, \n",
    "                 dropout=0.5, pooling_strategy='mean', attention_heads=8):\n",
    "        super(BiLSTMWithSelfAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        # BiLSTM outputs hidden_dim * 2 (forward + backward)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=attention_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "    def forward(self, questions, question_lengths=None):\n",
    "        # Embed questions\n",
    "        embeds = self.embedding(questions)  # [B, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequence if lengths provided (for efficiency)\n",
    "        if question_lengths is not None:\n",
    "            embeds = nn.utils.rnn.pack_padded_sequence(\n",
    "                embeds, question_lengths.cpu(), \n",
    "                batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        \n",
    "        # BiLSTM encoding\n",
    "        lstm_out, (hidden, cell) = self.bilstm(embeds)\n",
    "        \n",
    "        # Unpack if needed\n",
    "        if question_lengths is not None:\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "                lstm_out, batch_first=True\n",
    "            )\n",
    "        \n",
    "        # lstm_out: [B, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Self-attention: query = key = value = lstm_out\n",
    "        attn_out, attn_weights = self.attention(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            need_weights=True\n",
    "        )\n",
    "        \n",
    "        # Residual connection + Layer Norm\n",
    "        attn_out = self.layer_norm(lstm_out + attn_out)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        \n",
    "        # Pooling strategy - you can experiment with these:\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            question_feature = attn_out.mean(dim=1)  # [B, hidden_dim * 2]\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            question_feature = attn_out.max(dim=1)[0]\n",
    "        else:\n",
    "            # Last hidden state (concatenate forward and backward)\n",
    "            question_feature = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        return question_feature, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61a86567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete VQA model: ResNet34 + BiLSTM with Self-Attention\n",
    "class VQA_ResNet_BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, embed_dim=300, \n",
    "                 lstm_hidden=512, fusion_dim=1024, lstm_dropout=0.5, \n",
    "                 lstm_num_layers=1, attention_heads=8, fusion_dropout=0.5,\n",
    "                 pooling_strategy='mean'):\n",
    "        super(VQA_ResNet_BiLSTM_Attention, self).__init__()\n",
    "        \n",
    "        # Image encoder: ResNet34\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # Remove the final FC layer\n",
    "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.image_feature_dim = 512  # ResNet34 final layer\n",
    "        \n",
    "        # Question encoder: BiLSTM + Self-Attention\n",
    "        self.question_encoder = BiLSTMWithSelfAttention(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=lstm_hidden,\n",
    "            num_layers=lstm_num_layers,\n",
    "            dropout=lstm_dropout,\n",
    "            attention_heads=attention_heads,\n",
    "            pooling_strategy=pooling_strategy\n",
    "        )\n",
    "        self.question_feature_dim = lstm_hidden * 2  # Bidirectional\n",
    "        \n",
    "        # Multimodal fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim + self.question_feature_dim, fusion_dim),\n",
    "            nn.BatchNorm1d(fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fusion_dropout),\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.BatchNorm1d(fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fusion_dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n",
    "        \n",
    "    def forward(self, images, questions, question_lengths=None):\n",
    "        # Extract image features\n",
    "        img_features = self.image_encoder(images)  # [B, 512, 1, 1]\n",
    "        img_features = img_features.squeeze(-1).squeeze(-1)  # [B, 512]\n",
    "        \n",
    "        # Extract question features with attention\n",
    "        q_features, attn_weights = self.question_encoder(questions, question_lengths) # [B, lstm_hidden * 2]\n",
    "        \n",
    "        # Concatenate image and question features\n",
    "        combined = torch.cat([img_features, q_features], dim=1)\n",
    "        # combined: [B, 512 + lstm_hidden*2]\n",
    "        \n",
    "        # Fusion\n",
    "        fused = self.fusion(combined)  # [B, fusion_dim // 2]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)  # [B, num_classes]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc61c29",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    def __init__(self, train_dataset, validation_dataset, vocab_size, num_classes, \n",
    "                 n_trials=50):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_trials = n_trials\n",
    "        self.results_dir = Path(HYPERPARAMETERS_RESULT_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Track all trial results\n",
    "        self.trial_results = []\n",
    "\n",
    "    def train_single_epoch(self, model, dataloader, criterion, optimizer):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            questions = batch['question'].to(device)\n",
    "            question_lengths = batch['question_lengths'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(images, questions, question_lengths)\n",
    "            loss = criterion(logits, answers)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "        \n",
    "        return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "    def validate(self, model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images = batch['image'].to(device)\n",
    "                questions = batch['question'].to(device)\n",
    "                question_lengths = batch['question_lengths'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "                \n",
    "                logits = model(images, questions, question_lengths)\n",
    "                loss = criterion(logits, answers)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += (predictions == answers).sum().item()\n",
    "                total += answers.size(0)\n",
    "        \n",
    "        return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "    def config_BLSTM(self, trial):\n",
    "        return {\n",
    "            # Embedding parameters\n",
    "            'embed_dim': trial.suggest_categorical('embed_dim', [200, 300, 512]),\n",
    "\n",
    "            # LSTM parameters\n",
    "            'lstm_hidden': trial.suggest_categorical('lstm_hidden', [256, 512, 768, 1024]),\n",
    "            'lstm_num_layers': trial.suggest_int('lstm_num_layers', 1, 3),\n",
    "            'lstm_dropout': trial.suggest_float('lstm_dropout', 0.1, 0.6),\n",
    "            'pooling_strategy': trial.suggest_categorical('pooling_strategy', ['mean', 'max', 'last']),\n",
    "\n",
    "            # Attention parameters\n",
    "            'attention_heads': trial.suggest_categorical('attention_heads', [4, 8, 16]),\n",
    "\n",
    "            # Fusion parameters\n",
    "            'fusion_dim': trial.suggest_categorical('fusion_dim', [512, 1024, 2048]),\n",
    "            'fusion_dropout': trial.suggest_float('fusion_dropout', 0.2, 0.6),\n",
    "\n",
    "            # Training parameters\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
    "            'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n",
    "            'scheduler_step_size': trial.suggest_int('scheduler_step_size', 5, 15),\n",
    "            'scheduler_gamma': trial.suggest_float('scheduler_gamma', 0.3, 0.7),\n",
    "        }\n",
    "\n",
    "    def objective(self, trial):\n",
    "        print(f\"Trial {trial.number + 1}/{self.n_trials}\")\n",
    "\n",
    "        config = self.config_BLSTM(trial)\n",
    "        model = VQA_ResNet_BiLSTM_Attention(\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=config['embed_dim'],\n",
    "            lstm_hidden=config['lstm_hidden'],\n",
    "            lstm_num_layers=config['lstm_num_layers'],\n",
    "            attention_heads=config['attention_heads'],\n",
    "            fusion_dim=config['fusion_dim'],\n",
    "            lstm_dropout=config['lstm_dropout'],\n",
    "            fusion_dropout=config['fusion_dropout'],\n",
    "            pooling_strategy=config['pooling_strategy']\n",
    "        ).to(device)\n",
    "        \n",
    "        for param in model.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "        print(f\"Config: {json.dumps(config, indent=2)}\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.validation_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config['scheduler_step_size'],\n",
    "            gamma=config['scheduler_gamma']\n",
    "        )\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        threshold = 5\n",
    "        threshold_count = 0\n",
    "        max_epochs = 30\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss, train_acc = self.train_single_epoch(\n",
    "                model, train_loader, criterion, optimizer\n",
    "            )\n",
    "\n",
    "            val_loss, val_acc = self.validate(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                threshold_count = 0\n",
    "            else:\n",
    "                threshold_count += 1\n",
    "            \n",
    "            if threshold_count >= threshold:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        trial_result = {\n",
    "            'trial_number': trial.number,\n",
    "            'config': config,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'final_epoch': epoch + 1\n",
    "        }\n",
    "        self.trial_results.append(trial_result)\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def save_results(self, study):\n",
    "        # Save best parameters\n",
    "        best_params_path = self.results_dir / f'best_params_BLSTM.json'\n",
    "        with open(best_params_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'best_params': study.best_params,\n",
    "                'best_value': study.best_value,\n",
    "                'best_trial': study.best_trial.number\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        # Save all trial results\n",
    "        all_results_path = self.results_dir / f'all_trials_BLSTM.json'\n",
    "        with open(all_results_path, 'w') as f:\n",
    "            json.dump(self.trial_results, f, indent=2)\n",
    "        \n",
    "        # Save study\n",
    "        study_path = self.results_dir / f'study_BLSTM.pkl'\n",
    "        joblib.dump(study, study_path)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {self.results_dir}\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"STARTING HYPERPARAMETER TUNING FOR BLSTM MODEL\\n\")\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "            sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED)\n",
    "        )\n",
    "\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        self.save_results(study)\n",
    "\n",
    "        # Print best results\n",
    "        print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "        print(f\"Best Trial: {study.best_trial.number}\")\n",
    "        print(f\"Best Validation Accuracy: {study.best_value:.2f}%\\n\")\n",
    "        print(f\"Best Hyperparameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 03:35:25,239] A new study created in memory with name: no-name-345f46e2-6e47-4883-964b-7e17c3b1c099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING HYPERPARAMETER TUNING FOR BLSTM MODEL\n",
      "\n",
      "Trial 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33036\\1776512635.py:87: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33036\\1776512635.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n",
      "c:\\Users\\User\\Documents\\WOA7015 - Advanced ML\\Assignments\\AA\\aml_aa\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\WOA7015 - Advanced ML\\Assignments\\AA\\aml_aa\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.40055750587160444,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.3727780074568463,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 3.8396292998041685e-05,\n",
      "  \"weight_decay\": 1.2562773503807034e-05,\n",
      "  \"scheduler_step_size\": 10,\n",
      "  \"scheduler_gamma\": 0.6140703845572054\n",
      "}\n",
      "Epoch 1: Train Acc=11.53%, Val Acc=19.56%\n",
      "Epoch 2: Train Acc=26.88%, Val Acc=29.91%\n",
      "Epoch 3: Train Acc=37.12%, Val Acc=38.18%\n",
      "Epoch 4: Train Acc=45.96%, Val Acc=47.77%\n",
      "Epoch 5: Train Acc=51.96%, Val Acc=50.14%\n",
      "Epoch 6: Train Acc=56.52%, Val Acc=53.66%\n",
      "Epoch 7: Train Acc=60.20%, Val Acc=56.03%\n",
      "Epoch 8: Train Acc=62.33%, Val Acc=58.31%\n",
      "Epoch 9: Train Acc=65.14%, Val Acc=62.39%\n",
      "Epoch 10: Train Acc=65.87%, Val Acc=62.87%\n",
      "Epoch 11: Train Acc=69.12%, Val Acc=63.82%\n",
      "Epoch 12: Train Acc=70.30%, Val Acc=64.67%\n",
      "Epoch 13: Train Acc=71.42%, Val Acc=65.53%\n",
      "Epoch 14: Train Acc=72.05%, Val Acc=66.38%\n",
      "Epoch 15: Train Acc=72.78%, Val Acc=67.14%\n",
      "Epoch 16: Train Acc=73.82%, Val Acc=67.43%\n",
      "Epoch 17: Train Acc=74.55%, Val Acc=68.38%\n",
      "Epoch 18: Train Acc=75.89%, Val Acc=68.85%\n",
      "Epoch 19: Train Acc=76.82%, Val Acc=68.76%\n",
      "Epoch 20: Train Acc=77.90%, Val Acc=69.42%\n",
      "Epoch 21: Train Acc=79.47%, Val Acc=69.99%\n",
      "Epoch 22: Train Acc=78.98%, Val Acc=71.04%\n",
      "Epoch 23: Train Acc=79.73%, Val Acc=71.60%\n",
      "Epoch 24: Train Acc=80.95%, Val Acc=71.32%\n",
      "Epoch 25: Train Acc=80.93%, Val Acc=71.79%\n",
      "Epoch 26: Train Acc=81.40%, Val Acc=71.79%\n",
      "Epoch 27: Train Acc=82.42%, Val Acc=72.65%\n",
      "Epoch 28: Train Acc=82.31%, Val Acc=72.27%\n",
      "Epoch 29: Train Acc=83.23%, Val Acc=73.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 03:43:27,751] Trial 0 finished with value: 73.69420702754036 and parameters: {'embed_dim': 300, 'lstm_hidden': 256, 'lstm_num_layers': 3, 'lstm_dropout': 0.40055750587160444, 'pooling_strategy': 'last', 'attention_heads': 4, 'fusion_dim': 2048, 'fusion_dropout': 0.3727780074568463, 'batch_size': 32, 'learning_rate': 3.8396292998041685e-05, 'weight_decay': 1.2562773503807034e-05, 'scheduler_step_size': 10, 'scheduler_gamma': 0.6140703845572054}. Best is trial 0 with value: 73.69420702754036.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Acc=83.61%, Val Acc=73.69%\n",
      "Trial 2/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.5828160165372797,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.3035119926400068,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00012399967836846095,\n",
      "  \"weight_decay\": 3.5856126103453987e-06,\n",
      "  \"scheduler_step_size\": 15,\n",
      "  \"scheduler_gamma\": 0.6100531293444458\n",
      "}\n",
      "Epoch 1: Train Acc=39.03%, Val Acc=52.61%\n",
      "Epoch 2: Train Acc=58.24%, Val Acc=59.73%\n",
      "Epoch 3: Train Acc=63.69%, Val Acc=66.48%\n",
      "Epoch 4: Train Acc=67.82%, Val Acc=66.57%\n",
      "Epoch 5: Train Acc=70.79%, Val Acc=70.18%\n",
      "Epoch 6: Train Acc=73.88%, Val Acc=71.60%\n",
      "Epoch 7: Train Acc=75.85%, Val Acc=74.45%\n",
      "Epoch 8: Train Acc=76.99%, Val Acc=75.97%\n",
      "Epoch 9: Train Acc=79.18%, Val Acc=76.35%\n",
      "Epoch 10: Train Acc=80.85%, Val Acc=77.87%\n",
      "Epoch 11: Train Acc=82.37%, Val Acc=77.30%\n",
      "Epoch 12: Train Acc=82.68%, Val Acc=77.30%\n",
      "Epoch 13: Train Acc=85.02%, Val Acc=79.20%\n",
      "Epoch 14: Train Acc=86.24%, Val Acc=77.30%\n",
      "Epoch 15: Train Acc=87.17%, Val Acc=78.82%\n",
      "Epoch 16: Train Acc=90.30%, Val Acc=79.49%\n",
      "Epoch 17: Train Acc=90.67%, Val Acc=80.06%\n",
      "Epoch 18: Train Acc=91.77%, Val Acc=79.39%\n",
      "Epoch 19: Train Acc=92.58%, Val Acc=80.63%\n",
      "Epoch 20: Train Acc=92.70%, Val Acc=79.30%\n",
      "Epoch 21: Train Acc=93.64%, Val Acc=78.73%\n",
      "Epoch 22: Train Acc=93.92%, Val Acc=79.30%\n",
      "Epoch 23: Train Acc=94.33%, Val Acc=80.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 03:52:46,000] Trial 1 finished with value: 80.62678062678063 and parameters: {'embed_dim': 512, 'lstm_hidden': 512, 'lstm_num_layers': 3, 'lstm_dropout': 0.5828160165372797, 'pooling_strategy': 'mean', 'attention_heads': 4, 'fusion_dim': 2048, 'fusion_dropout': 0.3035119926400068, 'batch_size': 16, 'learning_rate': 0.00012399967836846095, 'weight_decay': 3.5856126103453987e-06, 'scheduler_step_size': 15, 'scheduler_gamma': 0.6100531293444458}. Best is trial 1 with value: 80.62678062678063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Acc=94.57%, Val Acc=79.58%\n",
      "Early stopping at epoch 24\n",
      "Trial 3/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.29433864484474104,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.508897907718663,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 0.0002592475660475158,\n",
      "  \"weight_decay\": 0.00015382308040278996,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.32961786069363613\n",
      "}\n",
      "Epoch 1: Train Acc=39.03%, Val Acc=51.66%\n",
      "Epoch 2: Train Acc=57.88%, Val Acc=62.01%\n",
      "Epoch 3: Train Acc=65.50%, Val Acc=67.14%\n",
      "Epoch 4: Train Acc=69.55%, Val Acc=68.28%\n",
      "Epoch 5: Train Acc=73.75%, Val Acc=71.13%\n",
      "Epoch 6: Train Acc=76.21%, Val Acc=71.60%\n",
      "Epoch 7: Train Acc=77.92%, Val Acc=75.12%\n",
      "Epoch 8: Train Acc=80.38%, Val Acc=76.07%\n",
      "Epoch 9: Train Acc=82.17%, Val Acc=74.64%\n",
      "Epoch 10: Train Acc=83.66%, Val Acc=77.59%\n",
      "Epoch 11: Train Acc=84.73%, Val Acc=77.11%\n",
      "Epoch 12: Train Acc=85.42%, Val Acc=78.44%\n",
      "Epoch 13: Train Acc=87.66%, Val Acc=77.11%\n",
      "Epoch 14: Train Acc=89.79%, Val Acc=77.87%\n",
      "Epoch 15: Train Acc=90.34%, Val Acc=77.49%\n",
      "Epoch 16: Train Acc=91.46%, Val Acc=77.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 03:56:50,383] Trial 2 finished with value: 78.44254510921178 and parameters: {'embed_dim': 200, 'lstm_hidden': 256, 'lstm_num_layers': 1, 'lstm_dropout': 0.29433864484474104, 'pooling_strategy': 'max', 'attention_heads': 8, 'fusion_dim': 2048, 'fusion_dropout': 0.508897907718663, 'batch_size': 64, 'learning_rate': 0.0002592475660475158, 'weight_decay': 0.00015382308040278996, 'scheduler_step_size': 13, 'scheduler_gamma': 0.32961786069363613}. Best is trial 1 with value: 80.62678062678063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Acc=91.56%, Val Acc=78.25%\n",
      "Early stopping at epoch 17\n",
      "Trial 4/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.464803089169032,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.4090931317527976,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 1.1557352816269867e-05,\n",
      "  \"weight_decay\": 8.11392957263784e-05,\n",
      "  \"scheduler_step_size\": 8,\n",
      "  \"scheduler_gamma\": 0.5034282764658811\n",
      "}\n",
      "Epoch 1: Train Acc=2.79%, Val Acc=22.03%\n",
      "Epoch 2: Train Acc=15.27%, Val Acc=30.01%\n",
      "Epoch 3: Train Acc=23.38%, Val Acc=34.85%\n",
      "Epoch 4: Train Acc=30.11%, Val Acc=37.61%\n",
      "Epoch 5: Train Acc=33.56%, Val Acc=39.60%\n",
      "Epoch 6: Train Acc=36.98%, Val Acc=41.79%\n",
      "Epoch 7: Train Acc=38.40%, Val Acc=43.59%\n",
      "Epoch 8: Train Acc=41.29%, Val Acc=44.25%\n",
      "Epoch 9: Train Acc=42.87%, Val Acc=44.92%\n",
      "Epoch 10: Train Acc=42.94%, Val Acc=46.34%\n",
      "Epoch 11: Train Acc=44.79%, Val Acc=46.82%\n",
      "Epoch 12: Train Acc=44.83%, Val Acc=47.29%\n",
      "Epoch 13: Train Acc=46.05%, Val Acc=46.91%\n",
      "Epoch 14: Train Acc=46.70%, Val Acc=47.77%\n",
      "Epoch 15: Train Acc=47.08%, Val Acc=47.77%\n",
      "Epoch 16: Train Acc=47.75%, Val Acc=49.00%\n",
      "Epoch 17: Train Acc=47.81%, Val Acc=49.19%\n",
      "Epoch 18: Train Acc=47.88%, Val Acc=49.38%\n",
      "Epoch 19: Train Acc=48.59%, Val Acc=49.48%\n",
      "Epoch 20: Train Acc=49.54%, Val Acc=50.14%\n",
      "Epoch 21: Train Acc=51.03%, Val Acc=50.43%\n",
      "Epoch 22: Train Acc=49.73%, Val Acc=49.67%\n",
      "Epoch 23: Train Acc=48.99%, Val Acc=50.05%\n",
      "Epoch 24: Train Acc=49.85%, Val Acc=50.05%\n",
      "Epoch 25: Train Acc=51.01%, Val Acc=50.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:04:31,953] Trial 3 finished with value: 50.427350427350426 and parameters: {'embed_dim': 512, 'lstm_hidden': 256, 'lstm_num_layers': 1, 'lstm_dropout': 0.464803089169032, 'pooling_strategy': 'max', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.4090931317527976, 'batch_size': 16, 'learning_rate': 1.1557352816269867e-05, 'weight_decay': 8.11392957263784e-05, 'scheduler_step_size': 8, 'scheduler_gamma': 0.5034282764658811}. Best is trial 1 with value: 80.62678062678063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Acc=50.74%, Val Acc=50.14%\n",
      "Early stopping at epoch 26\n",
      "Trial 5/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.5648488261712865,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.32720138998874554,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 0.0004325432427964555,\n",
      "  \"weight_decay\": 0.00038211294416912254,\n",
      "  \"scheduler_step_size\": 5,\n",
      "  \"scheduler_gamma\": 0.5042989210310262\n",
      "}\n",
      "Epoch 1: Train Acc=31.86%, Val Acc=45.49%\n",
      "Epoch 2: Train Acc=56.07%, Val Acc=60.97%\n",
      "Epoch 3: Train Acc=67.78%, Val Acc=64.67%\n",
      "Epoch 4: Train Acc=73.82%, Val Acc=68.66%\n",
      "Epoch 5: Train Acc=78.41%, Val Acc=71.70%\n",
      "Epoch 6: Train Acc=83.74%, Val Acc=73.50%\n",
      "Epoch 7: Train Acc=85.97%, Val Acc=73.03%\n",
      "Epoch 8: Train Acc=87.88%, Val Acc=74.93%\n",
      "Epoch 9: Train Acc=89.69%, Val Acc=74.55%\n",
      "Epoch 10: Train Acc=91.38%, Val Acc=74.55%\n",
      "Epoch 11: Train Acc=93.29%, Val Acc=74.93%\n",
      "Epoch 12: Train Acc=94.41%, Val Acc=75.31%\n",
      "Epoch 13: Train Acc=95.34%, Val Acc=76.35%\n",
      "Epoch 14: Train Acc=95.97%, Val Acc=75.78%\n",
      "Epoch 15: Train Acc=95.79%, Val Acc=77.11%\n",
      "Epoch 16: Train Acc=96.50%, Val Acc=76.64%\n",
      "Epoch 17: Train Acc=97.30%, Val Acc=77.40%\n",
      "Epoch 18: Train Acc=97.56%, Val Acc=76.73%\n",
      "Epoch 19: Train Acc=97.56%, Val Acc=76.92%\n",
      "Epoch 20: Train Acc=97.56%, Val Acc=77.02%\n",
      "Epoch 21: Train Acc=97.62%, Val Acc=77.49%\n",
      "Epoch 22: Train Acc=97.62%, Val Acc=77.59%\n",
      "Epoch 23: Train Acc=98.37%, Val Acc=77.49%\n",
      "Epoch 24: Train Acc=98.13%, Val Acc=77.59%\n",
      "Epoch 25: Train Acc=98.35%, Val Acc=77.21%\n",
      "Epoch 26: Train Acc=98.54%, Val Acc=77.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:10:51,651] Trial 4 finished with value: 77.58784425451093 and parameters: {'embed_dim': 200, 'lstm_hidden': 256, 'lstm_num_layers': 1, 'lstm_dropout': 0.5648488261712865, 'pooling_strategy': 'last', 'attention_heads': 16, 'fusion_dim': 2048, 'fusion_dropout': 0.32720138998874554, 'batch_size': 64, 'learning_rate': 0.0004325432427964555, 'weight_decay': 0.00038211294416912254, 'scheduler_step_size': 5, 'scheduler_gamma': 0.5042989210310262}. Best is trial 1 with value: 80.62678062678063.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Acc=98.27%, Val Acc=77.49%\n",
      "Early stopping at epoch 27\n",
      "Trial 6/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.28181480118964697,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.22059150049999576,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 1.9489008462344228e-05,\n",
      "  \"weight_decay\": 2.9400741309033083e-05,\n",
      "  \"scheduler_step_size\": 15,\n",
      "  \"scheduler_gamma\": 0.39682210860460015\n",
      "}\n",
      "Epoch 1: Train Acc=17.58%, Val Acc=39.32%\n",
      "Epoch 2: Train Acc=42.08%, Val Acc=47.48%\n",
      "Epoch 3: Train Acc=50.64%, Val Acc=50.33%\n",
      "Epoch 4: Train Acc=53.91%, Val Acc=54.13%\n",
      "Epoch 5: Train Acc=57.92%, Val Acc=55.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:12:39,578] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=59.56%, Val Acc=59.92%\n",
      "Trial 7/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.14514488502720416,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.45806911616377993,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.0007472397689332936,\n",
      "  \"weight_decay\": 2.585608890731339e-06,\n",
      "  \"scheduler_step_size\": 8,\n",
      "  \"scheduler_gamma\": 0.3453894084962356\n",
      "}\n",
      "Epoch 1: Train Acc=47.14%, Val Acc=56.79%\n",
      "Epoch 2: Train Acc=61.29%, Val Acc=65.24%\n",
      "Epoch 3: Train Acc=66.52%, Val Acc=69.23%\n",
      "Epoch 4: Train Acc=70.89%, Val Acc=70.94%\n",
      "Epoch 5: Train Acc=73.92%, Val Acc=75.12%\n",
      "Epoch 6: Train Acc=76.72%, Val Acc=75.02%\n",
      "Epoch 7: Train Acc=77.64%, Val Acc=75.97%\n",
      "Epoch 8: Train Acc=79.55%, Val Acc=76.26%\n",
      "Epoch 9: Train Acc=83.05%, Val Acc=79.11%\n",
      "Epoch 10: Train Acc=85.48%, Val Acc=79.87%\n",
      "Epoch 11: Train Acc=85.99%, Val Acc=79.01%\n",
      "Epoch 12: Train Acc=86.93%, Val Acc=78.44%\n",
      "Epoch 13: Train Acc=88.05%, Val Acc=79.68%\n",
      "Epoch 14: Train Acc=88.29%, Val Acc=81.01%\n",
      "Epoch 15: Train Acc=89.21%, Val Acc=80.82%\n",
      "Epoch 16: Train Acc=89.39%, Val Acc=80.91%\n",
      "Epoch 17: Train Acc=91.62%, Val Acc=80.72%\n",
      "Epoch 18: Train Acc=92.34%, Val Acc=81.20%\n",
      "Epoch 19: Train Acc=92.62%, Val Acc=80.06%\n",
      "Epoch 20: Train Acc=92.88%, Val Acc=81.10%\n",
      "Epoch 21: Train Acc=93.17%, Val Acc=81.01%\n",
      "Epoch 22: Train Acc=93.25%, Val Acc=81.39%\n",
      "Epoch 23: Train Acc=93.33%, Val Acc=81.10%\n",
      "Epoch 24: Train Acc=93.72%, Val Acc=81.29%\n",
      "Epoch 25: Train Acc=94.21%, Val Acc=80.82%\n",
      "Epoch 26: Train Acc=94.71%, Val Acc=80.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:19:51,618] Trial 6 finished with value: 81.38651471984805 and parameters: {'embed_dim': 300, 'lstm_hidden': 256, 'lstm_num_layers': 2, 'lstm_dropout': 0.14514488502720416, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.45806911616377993, 'batch_size': 32, 'learning_rate': 0.0007472397689332936, 'weight_decay': 2.585608890731339e-06, 'scheduler_step_size': 8, 'scheduler_gamma': 0.3453894084962356}. Best is trial 6 with value: 81.38651471984805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Acc=94.57%, Val Acc=81.10%\n",
      "Early stopping at epoch 27\n",
      "Trial 8/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.1465513839029496,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.4568126584617151,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 0.00016325185294676912,\n",
      "  \"weight_decay\": 1.0655924993232579e-06,\n",
      "  \"scheduler_step_size\": 6,\n",
      "  \"scheduler_gamma\": 0.5654007076432224\n",
      "}\n",
      "Epoch 1: Train Acc=29.15%, Val Acc=45.58%\n",
      "Epoch 2: Train Acc=47.94%, Val Acc=52.80%\n",
      "Epoch 3: Train Acc=54.20%, Val Acc=56.89%\n",
      "Epoch 4: Train Acc=58.14%, Val Acc=60.02%\n",
      "Epoch 5: Train Acc=61.01%, Val Acc=61.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:21:19,149] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=63.02%, Val Acc=62.58%\n",
      "Trial 9/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 1024,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.26269984907963384,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.5892042219009782,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.0003887072196612053,\n",
      "  \"weight_decay\": 3.2204108362516767e-05,\n",
      "  \"scheduler_step_size\": 11,\n",
      "  \"scheduler_gamma\": 0.4970070775275455\n",
      "}\n",
      "Epoch 1: Train Acc=34.80%, Val Acc=52.33%\n",
      "Epoch 2: Train Acc=52.29%, Val Acc=59.54%\n",
      "Epoch 3: Train Acc=57.57%, Val Acc=60.59%\n",
      "Epoch 4: Train Acc=60.52%, Val Acc=63.72%\n",
      "Epoch 5: Train Acc=63.87%, Val Acc=68.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:23:00,965] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=67.74%, Val Acc=71.13%\n",
      "Trial 10/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 1024,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.5574321951102242,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.5404546686067427,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 0.000745262979291264,\n",
      "  \"weight_decay\": 0.00012248682856804866,\n",
      "  \"scheduler_step_size\": 11,\n",
      "  \"scheduler_gamma\": 0.3388705975083074\n",
      "}\n",
      "Epoch 1: Train Acc=27.16%, Val Acc=42.17%\n",
      "Epoch 2: Train Acc=42.53%, Val Acc=48.15%\n",
      "Epoch 3: Train Acc=48.26%, Val Acc=51.85%\n",
      "Epoch 4: Train Acc=53.59%, Val Acc=55.75%\n",
      "Epoch 5: Train Acc=57.39%, Val Acc=59.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:25:10,474] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=59.73%, Val Acc=61.92%\n",
      "Trial 11/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.10898093780741006,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.4551887020844243,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.0009654400943128413,\n",
      "  \"weight_decay\": 1.2120237967851038e-06,\n",
      "  \"scheduler_step_size\": 8,\n",
      "  \"scheduler_gamma\": 0.4143417233780396\n",
      "}\n",
      "Epoch 1: Train Acc=43.83%, Val Acc=53.18%\n",
      "Epoch 2: Train Acc=56.05%, Val Acc=61.82%\n",
      "Epoch 3: Train Acc=62.00%, Val Acc=65.53%\n",
      "Epoch 4: Train Acc=65.93%, Val Acc=70.37%\n",
      "Epoch 5: Train Acc=69.38%, Val Acc=71.04%\n",
      "Epoch 6: Train Acc=72.11%, Val Acc=71.70%\n",
      "Epoch 7: Train Acc=74.34%, Val Acc=75.40%\n",
      "Epoch 8: Train Acc=75.67%, Val Acc=75.59%\n",
      "Epoch 9: Train Acc=79.91%, Val Acc=77.87%\n",
      "Epoch 10: Train Acc=81.91%, Val Acc=77.68%\n",
      "Epoch 11: Train Acc=83.13%, Val Acc=78.73%\n",
      "Epoch 12: Train Acc=83.86%, Val Acc=78.25%\n",
      "Epoch 13: Train Acc=84.85%, Val Acc=79.11%\n",
      "Epoch 14: Train Acc=85.16%, Val Acc=78.44%\n",
      "Epoch 15: Train Acc=86.32%, Val Acc=77.87%\n",
      "Epoch 16: Train Acc=87.11%, Val Acc=78.35%\n",
      "Epoch 17: Train Acc=88.15%, Val Acc=78.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:31:00,587] Trial 10 finished with value: 79.10731244064577 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.10898093780741006, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.4551887020844243, 'batch_size': 32, 'learning_rate': 0.0009654400943128413, 'weight_decay': 1.2120237967851038e-06, 'scheduler_step_size': 8, 'scheduler_gamma': 0.4143417233780396}. Best is trial 6 with value: 81.38651471984805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Acc=89.98%, Val Acc=78.06%\n",
      "Early stopping at epoch 18\n",
      "Trial 12/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.18002692399577636,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.26747884120423054,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 7.560676161561469e-05,\n",
      "  \"weight_decay\": 4.62137155906234e-06,\n",
      "  \"scheduler_step_size\": 15,\n",
      "  \"scheduler_gamma\": 0.6862778301767893\n",
      "}\n",
      "Epoch 1: Train Acc=42.98%, Val Acc=54.51%\n",
      "Epoch 2: Train Acc=59.44%, Val Acc=63.25%\n",
      "Epoch 3: Train Acc=66.05%, Val Acc=68.47%\n",
      "Epoch 4: Train Acc=70.85%, Val Acc=70.75%\n",
      "Epoch 5: Train Acc=73.41%, Val Acc=73.31%\n",
      "Epoch 6: Train Acc=76.52%, Val Acc=73.79%\n",
      "Epoch 7: Train Acc=78.13%, Val Acc=75.21%\n",
      "Epoch 8: Train Acc=79.12%, Val Acc=77.59%\n",
      "Epoch 9: Train Acc=81.13%, Val Acc=78.16%\n",
      "Epoch 10: Train Acc=83.15%, Val Acc=77.40%\n",
      "Epoch 11: Train Acc=84.53%, Val Acc=78.25%\n",
      "Epoch 12: Train Acc=85.48%, Val Acc=79.96%\n",
      "Epoch 13: Train Acc=86.09%, Val Acc=78.25%\n",
      "Epoch 14: Train Acc=86.62%, Val Acc=79.01%\n",
      "Epoch 15: Train Acc=88.90%, Val Acc=79.87%\n",
      "Epoch 16: Train Acc=89.82%, Val Acc=80.53%\n",
      "Epoch 17: Train Acc=90.87%, Val Acc=80.53%\n",
      "Epoch 18: Train Acc=90.89%, Val Acc=80.53%\n",
      "Epoch 19: Train Acc=91.91%, Val Acc=79.96%\n",
      "Epoch 20: Train Acc=92.32%, Val Acc=81.39%\n",
      "Epoch 21: Train Acc=93.03%, Val Acc=79.96%\n",
      "Epoch 22: Train Acc=93.13%, Val Acc=81.10%\n",
      "Epoch 23: Train Acc=93.90%, Val Acc=80.06%\n",
      "Epoch 24: Train Acc=93.82%, Val Acc=80.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:39:47,131] Trial 11 finished with value: 81.38651471984805 and parameters: {'embed_dim': 512, 'lstm_hidden': 512, 'lstm_num_layers': 2, 'lstm_dropout': 0.18002692399577636, 'pooling_strategy': 'mean', 'attention_heads': 4, 'fusion_dim': 1024, 'fusion_dropout': 0.26747884120423054, 'batch_size': 16, 'learning_rate': 7.560676161561469e-05, 'weight_decay': 4.62137155906234e-06, 'scheduler_step_size': 15, 'scheduler_gamma': 0.6862778301767893}. Best is trial 6 with value: 81.38651471984805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Acc=93.92%, Val Acc=80.63%\n",
      "Early stopping at epoch 25\n",
      "Trial 13/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.19371488037668727,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.2966219731921359,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 5.22924439423503e-05,\n",
      "  \"weight_decay\": 5.150051250657589e-06,\n",
      "  \"scheduler_step_size\": 8,\n",
      "  \"scheduler_gamma\": 0.69633964205143\n",
      "}\n",
      "Epoch 1: Train Acc=40.48%, Val Acc=53.94%\n",
      "Epoch 2: Train Acc=56.74%, Val Acc=60.30%\n",
      "Epoch 3: Train Acc=62.59%, Val Acc=65.34%\n",
      "Epoch 4: Train Acc=66.74%, Val Acc=68.76%\n",
      "Epoch 5: Train Acc=70.99%, Val Acc=71.04%\n",
      "Epoch 6: Train Acc=72.60%, Val Acc=72.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:42:41,577] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc=75.44%, Val Acc=74.36%\n",
      "Trial 14/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.19442709974855255,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.22854437146535594,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 5.97779956549715e-05,\n",
      "  \"weight_decay\": 3.9430234470278625e-06,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.6917702367090977\n",
      "}\n",
      "Epoch 1: Train Acc=42.04%, Val Acc=52.61%\n",
      "Epoch 2: Train Acc=57.33%, Val Acc=60.11%\n",
      "Epoch 3: Train Acc=64.02%, Val Acc=65.43%\n",
      "Epoch 4: Train Acc=68.20%, Val Acc=67.71%\n",
      "Epoch 5: Train Acc=71.84%, Val Acc=70.47%\n",
      "Epoch 6: Train Acc=73.80%, Val Acc=72.74%\n",
      "Epoch 7: Train Acc=76.66%, Val Acc=75.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:45:30,142] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Acc=77.52%, Val Acc=75.69%\n",
      "Trial 15/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.199833066193224,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.37655252041135395,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 7.078985985186175e-05,\n",
      "  \"weight_decay\": 9.631808357770794e-06,\n",
      "  \"scheduler_step_size\": 7,\n",
      "  \"scheduler_gamma\": 0.4065156005441934\n",
      "}\n",
      "Epoch 1: Train Acc=29.90%, Val Acc=47.96%\n",
      "Epoch 2: Train Acc=51.96%, Val Acc=54.80%\n",
      "Epoch 3: Train Acc=56.92%, Val Acc=58.88%\n",
      "Epoch 4: Train Acc=62.49%, Val Acc=62.01%\n",
      "Epoch 5: Train Acc=65.42%, Val Acc=64.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:47:07,854] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=68.67%, Val Acc=68.47%\n",
      "Trial 16/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.37004751005679687,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.4410950175168563,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 2.9880920724717516e-05,\n",
      "  \"weight_decay\": 2.251237048622318e-06,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.4419701522340522\n",
      "}\n",
      "Epoch 1: Train Acc=21.24%, Val Acc=40.93%\n",
      "Epoch 2: Train Acc=41.86%, Val Acc=49.29%\n",
      "Epoch 3: Train Acc=47.20%, Val Acc=52.14%\n",
      "Epoch 4: Train Acc=53.06%, Val Acc=54.23%\n",
      "Epoch 5: Train Acc=55.44%, Val Acc=58.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:49:11,886] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=57.74%, Val Acc=60.02%\n",
      "Trial 17/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 1024,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.10204194829999272,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.5168322782845036,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.00018418776817629215,\n",
      "  \"weight_decay\": 1.0858099672196784e-05,\n",
      "  \"scheduler_step_size\": 9,\n",
      "  \"scheduler_gamma\": 0.6272528100753763\n",
      "}\n",
      "Epoch 1: Train Acc=38.97%, Val Acc=52.42%\n",
      "Epoch 2: Train Acc=55.97%, Val Acc=60.68%\n",
      "Epoch 3: Train Acc=63.18%, Val Acc=65.91%\n",
      "Epoch 4: Train Acc=67.70%, Val Acc=67.71%\n",
      "Epoch 5: Train Acc=70.48%, Val Acc=69.99%\n",
      "Epoch 6: Train Acc=73.49%, Val Acc=73.50%\n",
      "Epoch 7: Train Acc=75.08%, Val Acc=75.78%\n",
      "Epoch 8: Train Acc=77.23%, Val Acc=74.93%\n",
      "Epoch 9: Train Acc=77.94%, Val Acc=76.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 04:53:18,709] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc=79.83%, Val Acc=77.11%\n",
      "Trial 18/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.2433266697763442,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.256029206876264,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 7.933393888686259e-05,\n",
      "  \"weight_decay\": 2.575361027088515e-05,\n",
      "  \"scheduler_step_size\": 11,\n",
      "  \"scheduler_gamma\": 0.30190950400634087\n",
      "}\n",
      "Epoch 1: Train Acc=43.97%, Val Acc=55.27%\n",
      "Epoch 2: Train Acc=59.93%, Val Acc=63.72%\n",
      "Epoch 3: Train Acc=65.46%, Val Acc=66.67%\n",
      "Epoch 4: Train Acc=69.32%, Val Acc=71.79%\n",
      "Epoch 5: Train Acc=73.43%, Val Acc=71.89%\n",
      "Epoch 6: Train Acc=75.38%, Val Acc=74.26%\n",
      "Epoch 7: Train Acc=77.15%, Val Acc=75.69%\n",
      "Epoch 8: Train Acc=78.88%, Val Acc=76.45%\n",
      "Epoch 9: Train Acc=80.73%, Val Acc=77.49%\n",
      "Epoch 10: Train Acc=82.15%, Val Acc=77.40%\n",
      "Epoch 11: Train Acc=83.25%, Val Acc=77.97%\n",
      "Epoch 12: Train Acc=85.63%, Val Acc=78.35%\n",
      "Epoch 13: Train Acc=86.28%, Val Acc=78.82%\n",
      "Epoch 14: Train Acc=86.89%, Val Acc=78.73%\n",
      "Epoch 15: Train Acc=87.92%, Val Acc=79.49%\n",
      "Epoch 16: Train Acc=88.23%, Val Acc=79.39%\n",
      "Epoch 17: Train Acc=88.21%, Val Acc=78.54%\n",
      "Epoch 18: Train Acc=88.68%, Val Acc=80.25%\n",
      "Epoch 19: Train Acc=89.23%, Val Acc=79.49%\n",
      "Epoch 20: Train Acc=89.65%, Val Acc=80.06%\n",
      "Epoch 21: Train Acc=90.45%, Val Acc=79.58%\n",
      "Epoch 22: Train Acc=90.04%, Val Acc=81.01%\n",
      "Epoch 23: Train Acc=90.91%, Val Acc=80.72%\n",
      "Epoch 24: Train Acc=91.26%, Val Acc=80.63%\n",
      "Epoch 25: Train Acc=91.06%, Val Acc=81.01%\n",
      "Epoch 26: Train Acc=91.99%, Val Acc=80.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:04:28,372] Trial 17 finished with value: 81.00664767331433 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.2433266697763442, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.256029206876264, 'batch_size': 16, 'learning_rate': 7.933393888686259e-05, 'weight_decay': 2.575361027088515e-05, 'scheduler_step_size': 11, 'scheduler_gamma': 0.30190950400634087}. Best is trial 6 with value: 81.38651471984805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Acc=91.89%, Val Acc=80.34%\n",
      "Early stopping at epoch 27\n",
      "Trial 19/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.33490741713763517,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.34500449343116324,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.0005035312561480005,\n",
      "  \"weight_decay\": 1.8239034879561889e-06,\n",
      "  \"scheduler_step_size\": 6,\n",
      "  \"scheduler_gamma\": 0.5595793669185372\n",
      "}\n",
      "Epoch 1: Train Acc=44.77%, Val Acc=55.65%\n",
      "Epoch 2: Train Acc=58.45%, Val Acc=62.30%\n",
      "Epoch 3: Train Acc=64.20%, Val Acc=67.05%\n",
      "Epoch 4: Train Acc=67.45%, Val Acc=68.57%\n",
      "Epoch 5: Train Acc=70.60%, Val Acc=69.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:06:32,336] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=72.11%, Val Acc=71.13%\n",
      "Trial 20/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.154217746870419,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.2627364708298777,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.0001256935767310123,\n",
      "  \"weight_decay\": 0.000995956756696024,\n",
      "  \"scheduler_step_size\": 14,\n",
      "  \"scheduler_gamma\": 0.45777990569558663\n",
      "}\n",
      "Epoch 1: Train Acc=39.15%, Val Acc=51.09%\n",
      "Epoch 2: Train Acc=57.57%, Val Acc=60.02%\n",
      "Epoch 3: Train Acc=63.08%, Val Acc=61.44%\n",
      "Epoch 4: Train Acc=67.01%, Val Acc=65.62%\n",
      "Epoch 5: Train Acc=70.30%, Val Acc=70.18%\n",
      "Epoch 6: Train Acc=73.55%, Val Acc=73.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:08:31,999] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc=76.15%, Val Acc=74.93%\n",
      "Trial 21/50\n",
      "Config: {\n",
      "  \"embed_dim\": 512,\n",
      "  \"lstm_hidden\": 512,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.4651308776948577,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.4128566105134371,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.0002775471589709571,\n",
      "  \"weight_decay\": 5.98269085691006e-06,\n",
      "  \"scheduler_step_size\": 10,\n",
      "  \"scheduler_gamma\": 0.3698693310462612\n",
      "}\n",
      "Epoch 1: Train Acc=42.57%, Val Acc=55.46%\n",
      "Epoch 2: Train Acc=58.10%, Val Acc=62.49%\n",
      "Epoch 3: Train Acc=63.47%, Val Acc=65.43%\n",
      "Epoch 4: Train Acc=67.27%, Val Acc=69.14%\n",
      "Epoch 5: Train Acc=69.97%, Val Acc=72.08%\n",
      "Epoch 6: Train Acc=72.58%, Val Acc=70.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:10:57,382] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc=74.12%, Val Acc=72.84%\n",
      "Trial 22/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.23860767846393605,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.20241346814865135,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 8.241801827719421e-05,\n",
      "  \"weight_decay\": 2.0003988826927767e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.30130447518095055\n",
      "}\n",
      "Epoch 1: Train Acc=44.93%, Val Acc=55.08%\n",
      "Epoch 2: Train Acc=61.31%, Val Acc=64.48%\n",
      "Epoch 3: Train Acc=67.23%, Val Acc=68.85%\n",
      "Epoch 4: Train Acc=70.30%, Val Acc=71.23%\n",
      "Epoch 5: Train Acc=74.16%, Val Acc=73.41%\n",
      "Epoch 6: Train Acc=76.72%, Val Acc=75.88%\n",
      "Epoch 7: Train Acc=79.57%, Val Acc=77.40%\n",
      "Epoch 8: Train Acc=80.10%, Val Acc=78.82%\n",
      "Epoch 9: Train Acc=82.23%, Val Acc=77.68%\n",
      "Epoch 10: Train Acc=83.82%, Val Acc=78.44%\n",
      "Epoch 11: Train Acc=84.43%, Val Acc=77.78%\n",
      "Epoch 12: Train Acc=86.46%, Val Acc=80.15%\n",
      "Epoch 13: Train Acc=88.66%, Val Acc=80.15%\n",
      "Epoch 14: Train Acc=89.92%, Val Acc=79.58%\n",
      "Epoch 15: Train Acc=90.28%, Val Acc=80.72%\n",
      "Epoch 16: Train Acc=90.38%, Val Acc=79.96%\n",
      "Epoch 17: Train Acc=90.93%, Val Acc=79.96%\n",
      "Epoch 18: Train Acc=91.54%, Val Acc=80.15%\n",
      "Epoch 19: Train Acc=92.40%, Val Acc=81.01%\n",
      "Epoch 20: Train Acc=91.97%, Val Acc=80.34%\n",
      "Epoch 21: Train Acc=92.50%, Val Acc=80.72%\n",
      "Epoch 22: Train Acc=92.64%, Val Acc=79.77%\n",
      "Epoch 23: Train Acc=93.49%, Val Acc=80.91%\n",
      "Epoch 24: Train Acc=93.62%, Val Acc=81.48%\n",
      "Epoch 25: Train Acc=94.51%, Val Acc=80.91%\n",
      "Epoch 26: Train Acc=94.45%, Val Acc=81.39%\n",
      "Epoch 27: Train Acc=95.00%, Val Acc=81.58%\n",
      "Epoch 28: Train Acc=94.65%, Val Acc=81.58%\n",
      "Epoch 29: Train Acc=94.98%, Val Acc=82.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:23:16,682] Trial 21 finished with value: 82.14624881291547 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.23860767846393605, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.20241346814865135, 'batch_size': 16, 'learning_rate': 8.241801827719421e-05, 'weight_decay': 2.0003988826927767e-05, 'scheduler_step_size': 12, 'scheduler_gamma': 0.30130447518095055}. Best is trial 21 with value: 82.14624881291547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Acc=94.78%, Val Acc=81.48%\n",
      "Trial 23/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.21540057110458077,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.2008680540472517,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 9.603247494024323e-05,\n",
      "  \"weight_decay\": 2.346586192695657e-06,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.30844372646615426\n",
      "}\n",
      "Epoch 1: Train Acc=46.41%, Val Acc=58.40%\n",
      "Epoch 2: Train Acc=62.72%, Val Acc=65.15%\n",
      "Epoch 3: Train Acc=67.96%, Val Acc=68.95%\n",
      "Epoch 4: Train Acc=72.13%, Val Acc=74.64%\n",
      "Epoch 5: Train Acc=75.16%, Val Acc=75.97%\n",
      "Epoch 6: Train Acc=77.68%, Val Acc=76.54%\n",
      "Epoch 7: Train Acc=80.06%, Val Acc=78.16%\n",
      "Epoch 8: Train Acc=81.74%, Val Acc=77.68%\n",
      "Epoch 9: Train Acc=83.41%, Val Acc=78.54%\n",
      "Epoch 10: Train Acc=84.63%, Val Acc=79.39%\n",
      "Epoch 11: Train Acc=84.83%, Val Acc=79.68%\n",
      "Epoch 12: Train Acc=86.36%, Val Acc=78.82%\n",
      "Epoch 13: Train Acc=89.67%, Val Acc=79.96%\n",
      "Epoch 14: Train Acc=90.59%, Val Acc=80.15%\n",
      "Epoch 15: Train Acc=91.64%, Val Acc=79.87%\n",
      "Epoch 16: Train Acc=91.66%, Val Acc=80.72%\n",
      "Epoch 17: Train Acc=92.72%, Val Acc=81.29%\n",
      "Epoch 18: Train Acc=92.91%, Val Acc=81.01%\n",
      "Epoch 19: Train Acc=92.64%, Val Acc=81.20%\n",
      "Epoch 20: Train Acc=94.19%, Val Acc=80.72%\n",
      "Epoch 21: Train Acc=93.98%, Val Acc=81.48%\n",
      "Epoch 22: Train Acc=94.23%, Val Acc=81.29%\n",
      "Epoch 23: Train Acc=94.53%, Val Acc=82.34%\n",
      "Epoch 24: Train Acc=94.90%, Val Acc=81.29%\n",
      "Epoch 25: Train Acc=95.30%, Val Acc=81.96%\n",
      "Epoch 26: Train Acc=96.63%, Val Acc=81.67%\n",
      "Epoch 27: Train Acc=95.85%, Val Acc=81.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:34:47,520] Trial 22 finished with value: 82.33618233618233 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.21540057110458077, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.2008680540472517, 'batch_size': 16, 'learning_rate': 9.603247494024323e-05, 'weight_decay': 2.346586192695657e-06, 'scheduler_step_size': 12, 'scheduler_gamma': 0.30844372646615426}. Best is trial 22 with value: 82.33618233618233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Acc=96.32%, Val Acc=82.24%\n",
      "Early stopping at epoch 28\n",
      "Trial 24/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.2121850971057852,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.20030552250447733,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 3.563868675866262e-05,\n",
      "  \"weight_decay\": 2.229861407288159e-06,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.35135197797352924\n",
      "}\n",
      "Epoch 1: Train Acc=40.98%, Val Acc=52.14%\n",
      "Epoch 2: Train Acc=56.66%, Val Acc=57.93%\n",
      "Epoch 3: Train Acc=61.54%, Val Acc=63.53%\n",
      "Epoch 4: Train Acc=66.27%, Val Acc=67.43%\n",
      "Epoch 5: Train Acc=68.71%, Val Acc=69.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:37:16,546] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=71.40%, Val Acc=72.74%\n",
      "Trial 25/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.1379219418965763,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.23474501416629087,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00011641099201709428,\n",
      "  \"weight_decay\": 1.7779000805353512e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.30852002797833167\n",
      "}\n",
      "Epoch 1: Train Acc=46.66%, Val Acc=60.02%\n",
      "Epoch 2: Train Acc=62.98%, Val Acc=66.57%\n",
      "Epoch 3: Train Acc=68.57%, Val Acc=70.56%\n",
      "Epoch 4: Train Acc=72.94%, Val Acc=71.98%\n",
      "Epoch 5: Train Acc=76.09%, Val Acc=74.93%\n",
      "Epoch 6: Train Acc=78.25%, Val Acc=75.40%\n",
      "Epoch 7: Train Acc=80.32%, Val Acc=76.45%\n",
      "Epoch 8: Train Acc=82.52%, Val Acc=78.54%\n",
      "Epoch 9: Train Acc=83.72%, Val Acc=79.20%\n",
      "Epoch 10: Train Acc=83.98%, Val Acc=79.11%\n",
      "Epoch 11: Train Acc=85.89%, Val Acc=79.68%\n",
      "Epoch 12: Train Acc=88.01%, Val Acc=78.06%\n",
      "Epoch 13: Train Acc=90.47%, Val Acc=79.96%\n",
      "Epoch 14: Train Acc=91.40%, Val Acc=80.15%\n",
      "Epoch 15: Train Acc=92.11%, Val Acc=80.44%\n",
      "Epoch 16: Train Acc=93.01%, Val Acc=80.25%\n",
      "Epoch 17: Train Acc=92.95%, Val Acc=80.53%\n",
      "Epoch 18: Train Acc=93.74%, Val Acc=79.96%\n",
      "Epoch 19: Train Acc=93.70%, Val Acc=80.34%\n",
      "Epoch 20: Train Acc=94.10%, Val Acc=80.53%\n",
      "Epoch 21: Train Acc=94.45%, Val Acc=81.10%\n",
      "Epoch 22: Train Acc=94.27%, Val Acc=80.63%\n",
      "Epoch 23: Train Acc=95.36%, Val Acc=81.86%\n",
      "Epoch 24: Train Acc=95.89%, Val Acc=80.53%\n",
      "Epoch 25: Train Acc=96.95%, Val Acc=81.20%\n",
      "Epoch 26: Train Acc=96.50%, Val Acc=81.20%\n",
      "Epoch 27: Train Acc=96.54%, Val Acc=81.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:48:55,710] Trial 24 finished with value: 81.8613485280152 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.1379219418965763, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.23474501416629087, 'batch_size': 16, 'learning_rate': 0.00011641099201709428, 'weight_decay': 1.7779000805353512e-05, 'scheduler_step_size': 12, 'scheduler_gamma': 0.30852002797833167}. Best is trial 22 with value: 82.33618233618233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Acc=96.93%, Val Acc=81.10%\n",
      "Early stopping at epoch 28\n",
      "Trial 26/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.24326103883854563,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.20095980873924058,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00010370024712497679,\n",
      "  \"weight_decay\": 4.6640868030022685e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.3065845196648759\n",
      "}\n",
      "Epoch 1: Train Acc=46.98%, Val Acc=58.88%\n",
      "Epoch 2: Train Acc=62.86%, Val Acc=63.53%\n",
      "Epoch 3: Train Acc=68.10%, Val Acc=70.75%\n",
      "Epoch 4: Train Acc=72.17%, Val Acc=73.31%\n",
      "Epoch 5: Train Acc=75.42%, Val Acc=72.93%\n",
      "Epoch 6: Train Acc=78.17%, Val Acc=77.40%\n",
      "Epoch 7: Train Acc=79.79%, Val Acc=78.25%\n",
      "Epoch 8: Train Acc=81.36%, Val Acc=78.73%\n",
      "Epoch 9: Train Acc=83.05%, Val Acc=78.06%\n",
      "Epoch 10: Train Acc=84.53%, Val Acc=79.11%\n",
      "Epoch 11: Train Acc=85.77%, Val Acc=79.30%\n",
      "Epoch 12: Train Acc=86.83%, Val Acc=79.96%\n",
      "Epoch 13: Train Acc=89.73%, Val Acc=80.06%\n",
      "Epoch 14: Train Acc=91.14%, Val Acc=80.72%\n",
      "Epoch 15: Train Acc=91.97%, Val Acc=81.39%\n",
      "Epoch 16: Train Acc=92.11%, Val Acc=80.25%\n",
      "Epoch 17: Train Acc=92.99%, Val Acc=80.15%\n",
      "Epoch 18: Train Acc=92.27%, Val Acc=81.20%\n",
      "Epoch 19: Train Acc=93.17%, Val Acc=80.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 05:57:15,199] Trial 25 finished with value: 81.38651471984805 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.24326103883854563, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.20095980873924058, 'batch_size': 16, 'learning_rate': 0.00010370024712497679, 'weight_decay': 4.6640868030022685e-05, 'scheduler_step_size': 12, 'scheduler_gamma': 0.3065845196648759}. Best is trial 22 with value: 82.33618233618233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Acc=93.56%, Val Acc=81.39%\n",
      "Early stopping at epoch 20\n",
      "Trial 27/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.31082050004268313,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.2462783109606976,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00018737016919861724,\n",
      "  \"weight_decay\": 1.5691034963179286e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.37714866737175196\n",
      "}\n",
      "Epoch 1: Train Acc=48.36%, Val Acc=58.78%\n",
      "Epoch 2: Train Acc=62.31%, Val Acc=66.00%\n",
      "Epoch 3: Train Acc=68.04%, Val Acc=71.42%\n",
      "Epoch 4: Train Acc=72.33%, Val Acc=73.69%\n",
      "Epoch 5: Train Acc=75.89%, Val Acc=75.31%\n",
      "Epoch 6: Train Acc=78.21%, Val Acc=77.49%\n",
      "Epoch 7: Train Acc=80.18%, Val Acc=78.44%\n",
      "Epoch 8: Train Acc=81.62%, Val Acc=79.58%\n",
      "Epoch 9: Train Acc=83.57%, Val Acc=78.92%\n",
      "Epoch 10: Train Acc=85.10%, Val Acc=79.01%\n",
      "Epoch 11: Train Acc=85.93%, Val Acc=79.01%\n",
      "Epoch 12: Train Acc=86.99%, Val Acc=79.01%\n",
      "Epoch 13: Train Acc=90.30%, Val Acc=80.72%\n",
      "Epoch 14: Train Acc=92.05%, Val Acc=80.44%\n",
      "Epoch 15: Train Acc=92.48%, Val Acc=80.91%\n",
      "Epoch 16: Train Acc=93.19%, Val Acc=81.10%\n",
      "Epoch 17: Train Acc=93.01%, Val Acc=80.72%\n",
      "Epoch 18: Train Acc=93.84%, Val Acc=80.91%\n",
      "Epoch 19: Train Acc=94.45%, Val Acc=81.29%\n",
      "Epoch 20: Train Acc=94.82%, Val Acc=81.39%\n",
      "Epoch 21: Train Acc=95.10%, Val Acc=80.82%\n",
      "Epoch 22: Train Acc=95.47%, Val Acc=82.24%\n",
      "Epoch 23: Train Acc=95.65%, Val Acc=80.91%\n",
      "Epoch 24: Train Acc=95.69%, Val Acc=81.67%\n",
      "Epoch 25: Train Acc=96.89%, Val Acc=81.39%\n",
      "Epoch 26: Train Acc=97.60%, Val Acc=80.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:06:24,958] Trial 26 finished with value: 82.2412155745489 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 1, 'lstm_dropout': 0.31082050004268313, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.2462783109606976, 'batch_size': 16, 'learning_rate': 0.00018737016919861724, 'weight_decay': 1.5691034963179286e-05, 'scheduler_step_size': 12, 'scheduler_gamma': 0.37714866737175196}. Best is trial 22 with value: 82.33618233618233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Acc=97.21%, Val Acc=81.20%\n",
      "Early stopping at epoch 27\n",
      "Trial 28/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.3168838559369421,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.2907171378156054,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00019366831169173062,\n",
      "  \"weight_decay\": 5.792024379581856e-05,\n",
      "  \"scheduler_step_size\": 14,\n",
      "  \"scheduler_gamma\": 0.38055888268018634\n",
      "}\n",
      "Epoch 1: Train Acc=47.33%, Val Acc=58.02%\n",
      "Epoch 2: Train Acc=60.97%, Val Acc=66.38%\n",
      "Epoch 3: Train Acc=68.49%, Val Acc=71.32%\n",
      "Epoch 4: Train Acc=72.78%, Val Acc=72.65%\n",
      "Epoch 5: Train Acc=75.22%, Val Acc=76.16%\n",
      "Epoch 6: Train Acc=77.88%, Val Acc=75.21%\n",
      "Epoch 7: Train Acc=79.26%, Val Acc=75.88%\n",
      "Epoch 8: Train Acc=81.56%, Val Acc=78.35%\n",
      "Epoch 9: Train Acc=82.07%, Val Acc=78.16%\n",
      "Epoch 10: Train Acc=82.70%, Val Acc=78.25%\n",
      "Epoch 11: Train Acc=83.47%, Val Acc=78.82%\n",
      "Epoch 12: Train Acc=86.26%, Val Acc=79.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:10:48,380] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc=86.77%, Val Acc=78.92%\n",
      "Trial 29/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.3789876390185368,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.2401164530194369,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.0002675272773452018,\n",
      "  \"weight_decay\": 7.107277994545453e-06,\n",
      "  \"scheduler_step_size\": 10,\n",
      "  \"scheduler_gamma\": 0.3669861643636776\n",
      "}\n",
      "Epoch 1: Train Acc=48.57%, Val Acc=60.30%\n",
      "Epoch 2: Train Acc=65.72%, Val Acc=69.33%\n",
      "Epoch 3: Train Acc=72.37%, Val Acc=73.79%\n",
      "Epoch 4: Train Acc=75.91%, Val Acc=76.54%\n",
      "Epoch 5: Train Acc=79.73%, Val Acc=75.31%\n",
      "Epoch 6: Train Acc=81.56%, Val Acc=77.30%\n",
      "Epoch 7: Train Acc=84.61%, Val Acc=75.97%\n",
      "Epoch 8: Train Acc=85.46%, Val Acc=79.11%\n",
      "Epoch 9: Train Acc=88.78%, Val Acc=79.30%\n",
      "Epoch 10: Train Acc=89.29%, Val Acc=77.97%\n",
      "Epoch 11: Train Acc=93.60%, Val Acc=79.68%\n",
      "Epoch 12: Train Acc=95.45%, Val Acc=79.49%\n",
      "Epoch 13: Train Acc=95.61%, Val Acc=80.63%\n",
      "Epoch 14: Train Acc=96.42%, Val Acc=80.91%\n",
      "Epoch 15: Train Acc=96.75%, Val Acc=79.96%\n",
      "Epoch 16: Train Acc=97.15%, Val Acc=80.06%\n",
      "Epoch 17: Train Acc=97.70%, Val Acc=79.49%\n",
      "Epoch 18: Train Acc=97.42%, Val Acc=79.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:16:37,548] Trial 28 finished with value: 80.91168091168092 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 1, 'lstm_dropout': 0.3789876390185368, 'pooling_strategy': 'last', 'attention_heads': 16, 'fusion_dim': 2048, 'fusion_dropout': 0.2401164530194369, 'batch_size': 16, 'learning_rate': 0.0002675272773452018, 'weight_decay': 7.107277994545453e-06, 'scheduler_step_size': 10, 'scheduler_gamma': 0.3669861643636776}. Best is trial 22 with value: 82.33618233618233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Acc=97.68%, Val Acc=80.25%\n",
      "Early stopping at epoch 19\n",
      "Trial 30/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.31242365979727865,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.27906911534377693,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 4.713763057391677e-05,\n",
      "  \"weight_decay\": 1.5914366405259002e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.4419942382773924\n",
      "}\n",
      "Epoch 1: Train Acc=30.35%, Val Acc=42.36%\n",
      "Epoch 2: Train Acc=47.14%, Val Acc=50.71%\n",
      "Epoch 3: Train Acc=52.73%, Val Acc=52.80%\n",
      "Epoch 4: Train Acc=56.98%, Val Acc=56.22%\n",
      "Epoch 5: Train Acc=59.63%, Val Acc=60.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:18:37,992] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=61.68%, Val Acc=61.35%\n",
      "Trial 31/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.3996536940266127,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.3238524298403014,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 2.2661030242192633e-05,\n",
      "  \"weight_decay\": 1.8099155274390934e-05,\n",
      "  \"scheduler_step_size\": 14,\n",
      "  \"scheduler_gamma\": 0.32737580445824555\n",
      "}\n",
      "Epoch 1: Train Acc=17.67%, Val Acc=34.09%\n",
      "Epoch 2: Train Acc=39.52%, Val Acc=45.77%\n",
      "Epoch 3: Train Acc=50.50%, Val Acc=50.33%\n",
      "Epoch 4: Train Acc=55.52%, Val Acc=54.70%\n",
      "Epoch 5: Train Acc=58.53%, Val Acc=58.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:21:21,284] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=61.72%, Val Acc=64.29%\n",
      "Trial 32/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.23174609150786846,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.23909221787767984,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 9.661086747833152e-05,\n",
      "  \"weight_decay\": 1.9183727415949337e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.30719269759210593\n",
      "}\n",
      "Epoch 1: Train Acc=46.57%, Val Acc=57.45%\n",
      "Epoch 2: Train Acc=60.76%, Val Acc=65.15%\n",
      "Epoch 3: Train Acc=66.44%, Val Acc=69.33%\n",
      "Epoch 4: Train Acc=70.91%, Val Acc=71.98%\n",
      "Epoch 5: Train Acc=74.53%, Val Acc=74.74%\n",
      "Epoch 6: Train Acc=76.34%, Val Acc=75.02%\n",
      "Epoch 7: Train Acc=78.23%, Val Acc=77.02%\n",
      "Epoch 8: Train Acc=79.77%, Val Acc=76.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:25:03,344] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc=81.93%, Val Acc=77.68%\n",
      "Trial 33/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.274640886806649,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.21409041068083134,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00013332015113264425,\n",
      "  \"weight_decay\": 3.930221156967291e-05,\n",
      "  \"scheduler_step_size\": 11,\n",
      "  \"scheduler_gamma\": 0.32224160945830155\n",
      "}\n",
      "Epoch 1: Train Acc=47.27%, Val Acc=58.12%\n",
      "Epoch 2: Train Acc=60.11%, Val Acc=65.05%\n",
      "Epoch 3: Train Acc=67.43%, Val Acc=66.38%\n",
      "Epoch 4: Train Acc=71.38%, Val Acc=69.99%\n",
      "Epoch 5: Train Acc=74.93%, Val Acc=75.40%\n",
      "Epoch 6: Train Acc=77.41%, Val Acc=73.60%\n",
      "Epoch 7: Train Acc=78.94%, Val Acc=77.30%\n",
      "Epoch 8: Train Acc=80.36%, Val Acc=77.97%\n",
      "Epoch 9: Train Acc=82.66%, Val Acc=78.44%\n",
      "Epoch 10: Train Acc=83.15%, Val Acc=78.44%\n",
      "Epoch 11: Train Acc=85.30%, Val Acc=77.11%\n",
      "Epoch 12: Train Acc=88.57%, Val Acc=78.63%\n",
      "Epoch 13: Train Acc=89.98%, Val Acc=79.58%\n",
      "Epoch 14: Train Acc=89.92%, Val Acc=79.96%\n",
      "Epoch 15: Train Acc=91.32%, Val Acc=80.72%\n",
      "Epoch 16: Train Acc=91.50%, Val Acc=80.63%\n",
      "Epoch 17: Train Acc=92.07%, Val Acc=79.77%\n",
      "Epoch 18: Train Acc=92.54%, Val Acc=80.53%\n",
      "Epoch 19: Train Acc=93.31%, Val Acc=80.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:34:46,372] Trial 32 finished with value: 80.72174738841406 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 3, 'lstm_dropout': 0.274640886806649, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.21409041068083134, 'batch_size': 16, 'learning_rate': 0.00013332015113264425, 'weight_decay': 3.930221156967291e-05, 'scheduler_step_size': 11, 'scheduler_gamma': 0.32224160945830155}. Best is trial 22 with value: 82.33618233618233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Acc=93.56%, Val Acc=80.72%\n",
      "Early stopping at epoch 20\n",
      "Trial 34/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.16556256559379687,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.23092104046935436,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00010345483171807115,\n",
      "  \"weight_decay\": 9.368386876407338e-06,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.35564191057291783\n",
      "}\n",
      "Epoch 1: Train Acc=46.86%, Val Acc=55.84%\n",
      "Epoch 2: Train Acc=61.58%, Val Acc=67.71%\n",
      "Epoch 3: Train Acc=69.16%, Val Acc=70.94%\n",
      "Epoch 4: Train Acc=73.12%, Val Acc=73.69%\n",
      "Epoch 5: Train Acc=75.77%, Val Acc=76.07%\n",
      "Epoch 6: Train Acc=78.61%, Val Acc=78.54%\n",
      "Epoch 7: Train Acc=80.95%, Val Acc=76.83%\n",
      "Epoch 8: Train Acc=81.72%, Val Acc=78.25%\n",
      "Epoch 9: Train Acc=84.47%, Val Acc=77.87%\n",
      "Epoch 10: Train Acc=85.93%, Val Acc=79.87%\n",
      "Epoch 11: Train Acc=86.62%, Val Acc=79.39%\n",
      "Epoch 12: Train Acc=88.01%, Val Acc=81.20%\n",
      "Epoch 13: Train Acc=88.96%, Val Acc=80.15%\n",
      "Epoch 14: Train Acc=91.48%, Val Acc=80.53%\n",
      "Epoch 15: Train Acc=92.17%, Val Acc=81.67%\n",
      "Epoch 16: Train Acc=93.03%, Val Acc=81.48%\n",
      "Epoch 17: Train Acc=93.72%, Val Acc=81.48%\n",
      "Epoch 18: Train Acc=93.72%, Val Acc=81.58%\n",
      "Epoch 19: Train Acc=94.49%, Val Acc=81.86%\n",
      "Epoch 20: Train Acc=94.08%, Val Acc=81.48%\n",
      "Epoch 21: Train Acc=94.86%, Val Acc=81.67%\n",
      "Epoch 22: Train Acc=95.04%, Val Acc=81.96%\n",
      "Epoch 23: Train Acc=95.67%, Val Acc=81.58%\n",
      "Epoch 24: Train Acc=95.91%, Val Acc=82.15%\n",
      "Epoch 25: Train Acc=95.67%, Val Acc=81.67%\n",
      "Epoch 26: Train Acc=95.67%, Val Acc=81.77%\n",
      "Epoch 27: Train Acc=97.13%, Val Acc=82.05%\n",
      "Epoch 28: Train Acc=97.44%, Val Acc=82.34%\n",
      "Epoch 29: Train Acc=97.38%, Val Acc=82.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:45:00,139] Trial 33 finished with value: 82.52611585944919 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 1, 'lstm_dropout': 0.16556256559379687, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.23092104046935436, 'batch_size': 16, 'learning_rate': 0.00010345483171807115, 'weight_decay': 9.368386876407338e-06, 'scheduler_step_size': 13, 'scheduler_gamma': 0.35564191057291783}. Best is trial 33 with value: 82.52611585944919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Acc=97.24%, Val Acc=82.53%\n",
      "Trial 35/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.2267130235566649,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.24467578368528078,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00015469231158035326,\n",
      "  \"weight_decay\": 8.068575587154546e-06,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.3546222785562367\n",
      "}\n",
      "Epoch 1: Train Acc=48.00%, Val Acc=55.37%\n",
      "Epoch 2: Train Acc=62.37%, Val Acc=65.62%\n",
      "Epoch 3: Train Acc=69.57%, Val Acc=71.51%\n",
      "Epoch 4: Train Acc=73.49%, Val Acc=72.93%\n",
      "Epoch 5: Train Acc=76.84%, Val Acc=76.54%\n",
      "Epoch 6: Train Acc=78.63%, Val Acc=77.40%\n",
      "Epoch 7: Train Acc=80.99%, Val Acc=78.06%\n",
      "Epoch 8: Train Acc=82.42%, Val Acc=77.68%\n",
      "Epoch 9: Train Acc=83.66%, Val Acc=79.20%\n",
      "Epoch 10: Train Acc=85.63%, Val Acc=81.01%\n",
      "Epoch 11: Train Acc=86.32%, Val Acc=79.68%\n",
      "Epoch 12: Train Acc=88.09%, Val Acc=79.20%\n",
      "Epoch 13: Train Acc=88.41%, Val Acc=80.63%\n",
      "Epoch 14: Train Acc=92.23%, Val Acc=80.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:50:02,828] Trial 34 finished with value: 81.00664767331433 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 1, 'lstm_dropout': 0.2267130235566649, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.24467578368528078, 'batch_size': 16, 'learning_rate': 0.00015469231158035326, 'weight_decay': 8.068575587154546e-06, 'scheduler_step_size': 13, 'scheduler_gamma': 0.3546222785562367}. Best is trial 33 with value: 82.52611585944919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc=92.60%, Val Acc=81.01%\n",
      "Early stopping at epoch 15\n",
      "Trial 36/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.16697843387764186,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.30911572619502825,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 0.00023114510315342773,\n",
      "  \"weight_decay\": 1.1824858391208949e-05,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.37814393489710274\n",
      "}\n",
      "Epoch 1: Train Acc=49.44%, Val Acc=61.63%\n",
      "Epoch 2: Train Acc=67.21%, Val Acc=69.33%\n",
      "Epoch 3: Train Acc=74.26%, Val Acc=73.98%\n",
      "Epoch 4: Train Acc=77.03%, Val Acc=76.54%\n",
      "Epoch 5: Train Acc=80.93%, Val Acc=77.68%\n",
      "Epoch 6: Train Acc=84.59%, Val Acc=79.01%\n",
      "Epoch 7: Train Acc=87.27%, Val Acc=76.54%\n",
      "Epoch 8: Train Acc=89.18%, Val Acc=79.30%\n",
      "Epoch 9: Train Acc=90.83%, Val Acc=78.06%\n",
      "Epoch 10: Train Acc=92.99%, Val Acc=78.92%\n",
      "Epoch 11: Train Acc=94.90%, Val Acc=79.01%\n",
      "Epoch 12: Train Acc=95.47%, Val Acc=80.06%\n",
      "Epoch 13: Train Acc=96.75%, Val Acc=79.49%\n",
      "Epoch 14: Train Acc=98.35%, Val Acc=79.96%\n",
      "Epoch 15: Train Acc=99.29%, Val Acc=80.91%\n",
      "Epoch 16: Train Acc=99.37%, Val Acc=80.91%\n",
      "Epoch 17: Train Acc=99.33%, Val Acc=81.10%\n",
      "Epoch 18: Train Acc=99.53%, Val Acc=81.10%\n",
      "Epoch 19: Train Acc=99.41%, Val Acc=81.20%\n",
      "Epoch 20: Train Acc=99.67%, Val Acc=81.58%\n",
      "Epoch 21: Train Acc=99.63%, Val Acc=81.20%\n",
      "Epoch 22: Train Acc=99.57%, Val Acc=81.29%\n",
      "Epoch 23: Train Acc=99.63%, Val Acc=81.67%\n",
      "Epoch 24: Train Acc=99.61%, Val Acc=81.29%\n",
      "Epoch 25: Train Acc=99.65%, Val Acc=80.91%\n",
      "Epoch 26: Train Acc=99.78%, Val Acc=81.48%\n",
      "Epoch 27: Train Acc=99.82%, Val Acc=81.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:57:27,115] Trial 35 finished with value: 81.67141500474834 and parameters: {'embed_dim': 200, 'lstm_hidden': 768, 'lstm_num_layers': 1, 'lstm_dropout': 0.16697843387764186, 'pooling_strategy': 'mean', 'attention_heads': 8, 'fusion_dim': 2048, 'fusion_dropout': 0.30911572619502825, 'batch_size': 64, 'learning_rate': 0.00023114510315342773, 'weight_decay': 1.1824858391208949e-05, 'scheduler_step_size': 13, 'scheduler_gamma': 0.37814393489710274}. Best is trial 33 with value: 82.52611585944919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Acc=99.84%, Val Acc=81.39%\n",
      "Early stopping at epoch 28\n",
      "Trial 37/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.2968711329728083,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.34780022263096266,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 8.12617284534278e-05,\n",
      "  \"weight_decay\": 7.329832179570696e-05,\n",
      "  \"scheduler_step_size\": 14,\n",
      "  \"scheduler_gamma\": 0.3940832864559873\n",
      "}\n",
      "Epoch 1: Train Acc=39.54%, Val Acc=52.52%\n",
      "Epoch 2: Train Acc=54.83%, Val Acc=57.55%\n",
      "Epoch 3: Train Acc=61.41%, Val Acc=61.92%\n",
      "Epoch 4: Train Acc=65.05%, Val Acc=66.10%\n",
      "Epoch 5: Train Acc=68.45%, Val Acc=68.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 06:59:30,627] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=71.40%, Val Acc=72.27%\n",
      "Trial 38/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.25896001051722767,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 2048,\n",
      "  \"fusion_dropout\": 0.2164343314596158,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00035031125007460194,\n",
      "  \"weight_decay\": 0.0001218798333951343,\n",
      "  \"scheduler_step_size\": 10,\n",
      "  \"scheduler_gamma\": 0.3322269553392636\n",
      "}\n",
      "Epoch 1: Train Acc=51.03%, Val Acc=62.11%\n",
      "Epoch 2: Train Acc=64.93%, Val Acc=66.48%\n",
      "Epoch 3: Train Acc=69.91%, Val Acc=71.70%\n",
      "Epoch 4: Train Acc=73.43%, Val Acc=74.64%\n",
      "Epoch 5: Train Acc=76.74%, Val Acc=75.50%\n",
      "Epoch 6: Train Acc=79.16%, Val Acc=73.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:01:55,611] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc=81.81%, Val Acc=75.02%\n",
      "Trial 39/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.456276291767765,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.27656016044883625,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 4.233659152501275e-05,\n",
      "  \"weight_decay\": 3.4124205611172227e-06,\n",
      "  \"scheduler_step_size\": 11,\n",
      "  \"scheduler_gamma\": 0.43373793688063556\n",
      "}\n",
      "Epoch 1: Train Acc=24.76%, Val Acc=41.50%\n",
      "Epoch 2: Train Acc=47.39%, Val Acc=48.05%\n",
      "Epoch 3: Train Acc=53.26%, Val Acc=51.76%\n",
      "Epoch 4: Train Acc=55.84%, Val Acc=55.84%\n",
      "Epoch 5: Train Acc=58.93%, Val Acc=57.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:03:30,013] Trial 38 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=61.19%, Val Acc=60.11%\n",
      "Trial 40/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 1024,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.12822465964058055,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.2547272770155921,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 6.304645291946576e-05,\n",
      "  \"weight_decay\": 2.7567511488420024e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.48416277776571404\n",
      "}\n",
      "Epoch 1: Train Acc=36.08%, Val Acc=51.47%\n",
      "Epoch 2: Train Acc=56.82%, Val Acc=60.02%\n",
      "Epoch 3: Train Acc=64.55%, Val Acc=67.43%\n",
      "Epoch 4: Train Acc=70.24%, Val Acc=69.14%\n",
      "Epoch 5: Train Acc=73.59%, Val Acc=72.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:05:30,015] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=76.46%, Val Acc=74.93%\n",
      "Trial 41/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.345699082636836,\n",
      "  \"pooling_strategy\": \"max\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.20315796272133255,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 9.523587762121582e-05,\n",
      "  \"weight_decay\": 1.484874563364815e-06,\n",
      "  \"scheduler_step_size\": 15,\n",
      "  \"scheduler_gamma\": 0.35343448212417067\n",
      "}\n",
      "Epoch 1: Train Acc=36.76%, Val Acc=52.14%\n",
      "Epoch 2: Train Acc=54.69%, Val Acc=55.94%\n",
      "Epoch 3: Train Acc=59.10%, Val Acc=61.25%\n",
      "Epoch 4: Train Acc=63.65%, Val Acc=64.77%\n",
      "Epoch 5: Train Acc=67.01%, Val Acc=66.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:07:26,615] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=69.26%, Val Acc=68.00%\n",
      "Trial 42/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.12938121519500323,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.22864381445798318,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00011838116202661061,\n",
      "  \"weight_decay\": 1.7359038425837982e-05,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.32245260548140836\n",
      "}\n",
      "Epoch 1: Train Acc=47.41%, Val Acc=57.55%\n",
      "Epoch 2: Train Acc=62.45%, Val Acc=67.43%\n",
      "Epoch 3: Train Acc=68.75%, Val Acc=72.74%\n",
      "Epoch 4: Train Acc=73.86%, Val Acc=72.55%\n",
      "Epoch 5: Train Acc=76.36%, Val Acc=73.50%\n",
      "Epoch 6: Train Acc=78.43%, Val Acc=77.97%\n",
      "Epoch 7: Train Acc=80.42%, Val Acc=78.82%\n",
      "Epoch 8: Train Acc=82.50%, Val Acc=77.40%\n",
      "Epoch 9: Train Acc=84.29%, Val Acc=78.54%\n",
      "Epoch 10: Train Acc=85.08%, Val Acc=78.73%\n",
      "Epoch 11: Train Acc=86.66%, Val Acc=79.39%\n",
      "Epoch 12: Train Acc=87.99%, Val Acc=80.06%\n",
      "Epoch 13: Train Acc=89.27%, Val Acc=79.30%\n",
      "Epoch 14: Train Acc=91.93%, Val Acc=79.77%\n",
      "Epoch 15: Train Acc=93.21%, Val Acc=81.10%\n",
      "Epoch 16: Train Acc=93.45%, Val Acc=81.39%\n",
      "Epoch 17: Train Acc=94.49%, Val Acc=81.29%\n",
      "Epoch 18: Train Acc=94.80%, Val Acc=80.91%\n",
      "Epoch 19: Train Acc=95.24%, Val Acc=80.06%\n",
      "Epoch 20: Train Acc=95.24%, Val Acc=80.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:16:01,169] Trial 41 finished with value: 81.38651471984805 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.12938121519500323, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.22864381445798318, 'batch_size': 16, 'learning_rate': 0.00011838116202661061, 'weight_decay': 1.7359038425837982e-05, 'scheduler_step_size': 13, 'scheduler_gamma': 0.32245260548140836}. Best is trial 33 with value: 82.52611585944919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Acc=95.55%, Val Acc=80.06%\n",
      "Early stopping at epoch 21\n",
      "Trial 43/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.17249042679427726,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.22696072064912268,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.00013983387260539418,\n",
      "  \"weight_decay\": 1.2567528088673197e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.30564700545815354\n",
      "}\n",
      "Epoch 1: Train Acc=47.92%, Val Acc=56.79%\n",
      "Epoch 2: Train Acc=63.31%, Val Acc=68.47%\n",
      "Epoch 3: Train Acc=70.20%, Val Acc=71.60%\n",
      "Epoch 4: Train Acc=73.23%, Val Acc=74.93%\n",
      "Epoch 5: Train Acc=75.77%, Val Acc=75.21%\n",
      "Epoch 6: Train Acc=78.57%, Val Acc=77.40%\n",
      "Epoch 7: Train Acc=80.81%, Val Acc=76.64%\n",
      "Epoch 8: Train Acc=82.09%, Val Acc=78.73%\n",
      "Epoch 9: Train Acc=83.90%, Val Acc=78.92%\n",
      "Epoch 10: Train Acc=85.34%, Val Acc=78.63%\n",
      "Epoch 11: Train Acc=87.09%, Val Acc=77.97%\n",
      "Epoch 12: Train Acc=87.15%, Val Acc=79.58%\n",
      "Epoch 13: Train Acc=90.83%, Val Acc=80.53%\n",
      "Epoch 14: Train Acc=92.76%, Val Acc=81.39%\n",
      "Epoch 15: Train Acc=93.15%, Val Acc=81.58%\n",
      "Epoch 16: Train Acc=93.76%, Val Acc=80.72%\n",
      "Epoch 17: Train Acc=94.12%, Val Acc=81.20%\n",
      "Epoch 18: Train Acc=94.15%, Val Acc=81.20%\n",
      "Epoch 19: Train Acc=94.39%, Val Acc=81.86%\n",
      "Epoch 20: Train Acc=94.88%, Val Acc=80.82%\n",
      "Epoch 21: Train Acc=95.12%, Val Acc=81.48%\n",
      "Epoch 22: Train Acc=96.02%, Val Acc=81.01%\n",
      "Epoch 23: Train Acc=95.75%, Val Acc=81.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:25:53,953] Trial 42 finished with value: 81.8613485280152 and parameters: {'embed_dim': 300, 'lstm_hidden': 768, 'lstm_num_layers': 2, 'lstm_dropout': 0.17249042679427726, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.22696072064912268, 'batch_size': 16, 'learning_rate': 0.00013983387260539418, 'weight_decay': 1.2567528088673197e-05, 'scheduler_step_size': 12, 'scheduler_gamma': 0.30564700545815354}. Best is trial 33 with value: 82.52611585944919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Acc=96.10%, Val Acc=81.48%\n",
      "Early stopping at epoch 24\n",
      "Trial 44/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 3,\n",
      "  \"lstm_dropout\": 0.21500756646056607,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.25151563608257194,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.0002129173365126322,\n",
      "  \"weight_decay\": 2.195026499679023e-05,\n",
      "  \"scheduler_step_size\": 11,\n",
      "  \"scheduler_gamma\": 0.3385017184990845\n",
      "}\n",
      "Epoch 1: Train Acc=46.76%, Val Acc=57.93%\n",
      "Epoch 2: Train Acc=61.21%, Val Acc=63.53%\n",
      "Epoch 3: Train Acc=67.15%, Val Acc=70.09%\n",
      "Epoch 4: Train Acc=71.27%, Val Acc=72.84%\n",
      "Epoch 5: Train Acc=74.55%, Val Acc=73.98%\n",
      "Epoch 6: Train Acc=76.68%, Val Acc=76.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:29:18,978] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc=78.76%, Val Acc=75.40%\n",
      "Trial 45/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.12898934447995677,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.27674356677386086,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 0.00010653395483504207,\n",
      "  \"weight_decay\": 1.3734462540842627e-05,\n",
      "  \"scheduler_step_size\": 13,\n",
      "  \"scheduler_gamma\": 0.3001948185676037\n",
      "}\n",
      "Epoch 1: Train Acc=44.07%, Val Acc=55.46%\n",
      "Epoch 2: Train Acc=60.01%, Val Acc=61.73%\n",
      "Epoch 3: Train Acc=65.14%, Val Acc=66.38%\n",
      "Epoch 4: Train Acc=69.91%, Val Acc=69.71%\n",
      "Epoch 5: Train Acc=72.39%, Val Acc=72.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:31:03,639] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=76.58%, Val Acc=74.64%\n",
      "Trial 46/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 256,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.2879178468330031,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.21895219326044876,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 0.0001714541875003529,\n",
      "  \"weight_decay\": 8.92896256329662e-06,\n",
      "  \"scheduler_step_size\": 10,\n",
      "  \"scheduler_gamma\": 0.38773125690999277\n",
      "}\n",
      "Epoch 1: Train Acc=45.29%, Val Acc=56.79%\n",
      "Epoch 2: Train Acc=61.62%, Val Acc=67.24%\n",
      "Epoch 3: Train Acc=69.47%, Val Acc=70.18%\n",
      "Epoch 4: Train Acc=73.94%, Val Acc=73.69%\n",
      "Epoch 5: Train Acc=77.03%, Val Acc=74.83%\n",
      "Epoch 6: Train Acc=79.43%, Val Acc=76.73%\n",
      "Epoch 7: Train Acc=81.95%, Val Acc=77.87%\n",
      "Epoch 8: Train Acc=83.88%, Val Acc=77.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:33:46,074] Trial 45 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Acc=84.90%, Val Acc=76.45%\n",
      "Trial 47/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 1024,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.18429286669021216,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.3017325849426975,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 1.3870732366327855e-05,\n",
      "  \"weight_decay\": 3.7000563821143885e-05,\n",
      "  \"scheduler_step_size\": 12,\n",
      "  \"scheduler_gamma\": 0.41725080436330364\n",
      "}\n",
      "Epoch 1: Train Acc=29.07%, Val Acc=43.30%\n",
      "Epoch 2: Train Acc=46.53%, Val Acc=49.76%\n",
      "Epoch 3: Train Acc=54.28%, Val Acc=55.46%\n",
      "Epoch 4: Train Acc=56.92%, Val Acc=57.26%\n",
      "Epoch 5: Train Acc=59.69%, Val Acc=60.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:36:59,561] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=62.29%, Val Acc=60.59%\n",
      "Trial 48/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.2574441590785044,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 16,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.23630160820889315,\n",
      "  \"batch_size\": 16,\n",
      "  \"learning_rate\": 6.151447930359639e-05,\n",
      "  \"weight_decay\": 6.0242970765311736e-06,\n",
      "  \"scheduler_step_size\": 9,\n",
      "  \"scheduler_gamma\": 0.3629305342880615\n",
      "}\n",
      "Epoch 1: Train Acc=42.49%, Val Acc=53.56%\n",
      "Epoch 2: Train Acc=58.10%, Val Acc=61.92%\n",
      "Epoch 3: Train Acc=64.02%, Val Acc=66.48%\n",
      "Epoch 4: Train Acc=68.20%, Val Acc=68.00%\n",
      "Epoch 5: Train Acc=71.19%, Val Acc=71.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:39:26,187] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=73.80%, Val Acc=73.31%\n",
      "Trial 49/50\n",
      "Config: {\n",
      "  \"embed_dim\": 200,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 1,\n",
      "  \"lstm_dropout\": 0.15606702980724435,\n",
      "  \"pooling_strategy\": \"last\",\n",
      "  \"attention_heads\": 4,\n",
      "  \"fusion_dim\": 1024,\n",
      "  \"fusion_dropout\": 0.21490645514786105,\n",
      "  \"batch_size\": 64,\n",
      "  \"learning_rate\": 8.507861969499592e-05,\n",
      "  \"weight_decay\": 0.00035592413675060534,\n",
      "  \"scheduler_step_size\": 14,\n",
      "  \"scheduler_gamma\": 0.32345108651055415\n",
      "}\n",
      "Epoch 1: Train Acc=24.96%, Val Acc=42.17%\n",
      "Epoch 2: Train Acc=48.44%, Val Acc=50.33%\n",
      "Epoch 3: Train Acc=58.16%, Val Acc=58.40%\n",
      "Epoch 4: Train Acc=64.53%, Val Acc=63.25%\n",
      "Epoch 5: Train Acc=68.77%, Val Acc=66.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:40:53,069] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Acc=72.27%, Val Acc=69.33%\n",
      "Trial 50/50\n",
      "Config: {\n",
      "  \"embed_dim\": 300,\n",
      "  \"lstm_hidden\": 768,\n",
      "  \"lstm_num_layers\": 2,\n",
      "  \"lstm_dropout\": 0.21571362611279488,\n",
      "  \"pooling_strategy\": \"mean\",\n",
      "  \"attention_heads\": 8,\n",
      "  \"fusion_dim\": 512,\n",
      "  \"fusion_dropout\": 0.26270788666146716,\n",
      "  \"batch_size\": 32,\n",
      "  \"learning_rate\": 0.00031378200349501653,\n",
      "  \"weight_decay\": 5.397637232393564e-05,\n",
      "  \"scheduler_step_size\": 9,\n",
      "  \"scheduler_gamma\": 0.5600693473391279\n",
      "}\n",
      "Epoch 1: Train Acc=43.95%, Val Acc=53.28%\n",
      "Epoch 2: Train Acc=58.89%, Val Acc=62.01%\n",
      "Epoch 3: Train Acc=65.14%, Val Acc=65.72%\n",
      "Epoch 4: Train Acc=69.97%, Val Acc=72.17%\n",
      "Epoch 5: Train Acc=73.59%, Val Acc=74.17%\n",
      "Epoch 6: Train Acc=75.54%, Val Acc=75.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-18 07:43:06,525] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Acc=77.54%, Val Acc=75.31%\n",
      "HYPERPARAMETER TUNING COMPLETE\n",
      "Best Trial: 33\n",
      "Best Validation Accuracy: 82.53%\n",
      "\n",
      "Best Hyperparameters:\n",
      "  embed_dim: 300\n",
      "  lstm_hidden: 768\n",
      "  lstm_num_layers: 1\n",
      "  lstm_dropout: 0.16556256559379687\n",
      "  pooling_strategy: mean\n",
      "  attention_heads: 16\n",
      "  fusion_dim: 1024\n",
      "  fusion_dropout: 0.23092104046935436\n",
      "  batch_size: 16\n",
      "  learning_rate: 0.00010345483171807115\n",
      "  weight_decay: 9.368386876407338e-06\n",
      "  scheduler_step_size: 13\n",
      "  scheduler_gamma: 0.35564191057291783\n"
     ]
    }
   ],
   "source": [
    "# Tune Hyperparameters for a BLSTM model\n",
    "tuner = HyperparameterTuner(\n",
    "    vocab_size=len(question_vocab),\n",
    "    num_classes=len(answer_vocab),\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    n_trials=50,\n",
    "    basic_model=False\n",
    ")\n",
    "\n",
    "# Run tuning\n",
    "study = tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "018251b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_params': {'embed_dim': 300,\n",
       "  'lstm_hidden': 768,\n",
       "  'lstm_num_layers': 1,\n",
       "  'lstm_dropout': 0.16556256559379687,\n",
       "  'pooling_strategy': 'mean',\n",
       "  'attention_heads': 16,\n",
       "  'fusion_dim': 1024,\n",
       "  'fusion_dropout': 0.23092104046935436,\n",
       "  'batch_size': 16,\n",
       "  'learning_rate': 0.00010345483171807115,\n",
       "  'weight_decay': 9.368386876407338e-06,\n",
       "  'scheduler_step_size': 13,\n",
       "  'scheduler_gamma': 0.35564191057291783},\n",
       " 'best_value': 82.52611585944919,\n",
       " 'best_trial': 33}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best hyperparameter for the model\n",
    "best_params_path = os.path.join(HYPERPARAMETERS_RESULT_PATH, 'best_params_BLSTM.json')\n",
    "with open(best_params_path, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3362a",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98886ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModelTrainer:\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, \n",
    "                 best_params, vocab_size, num_classes):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.best_params = best_params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        # Create results directory\n",
    "        self.results_dir = Path(FINAL_MODEL_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def train_epoch(self, model, dataloader, criterion, optimizer):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            questions = batch['question'].to(device)\n",
    "            question_lengths = batch['question_lengths'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(images, questions, question_lengths)\n",
    "            loss = criterion(logits, answers)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        return total_loss / len(dataloader), 100 * correct / total\n",
    "    \n",
    "    def validate(self, model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Validating'):\n",
    "                images = batch['image'].to(device)\n",
    "                questions = batch['question'].to(device)\n",
    "                question_lengths = batch['question_lengths'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "                \n",
    "                logits = model(images, questions, question_lengths)\n",
    "                loss = criterion(logits, answers)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += (predictions == answers).sum().item()\n",
    "                total += answers.size(0)\n",
    "        \n",
    "        return total_loss / len(dataloader), 100 * correct / total\n",
    "    \n",
    "    def final_evaluation(self, model):\n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Store all predictions and results\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_ids = []\n",
    "        \n",
    "        # Type-specific tracking\n",
    "        type_stats = {\n",
    "            'CLOSED': {'correct': 0, 'total': 0, 'predictions': [], 'targets': []},\n",
    "            'OPEN': {'correct': 0, 'total': 0, 'predictions': [], 'targets': []}\n",
    "        }\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Testing'):\n",
    "                images = batch['image'].to(device)\n",
    "                questions = batch['question'].to(device)\n",
    "                question_lengths = batch['question_lengths'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "                \n",
    "                logits = model(images, questions, question_lengths)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().tolist())\n",
    "                all_targets.extend(answers.cpu().tolist())\n",
    "                all_ids.extend(batch['id'])\n",
    "        \n",
    "        # Categorize by answer type\n",
    "        for pred, target, qid in zip(all_predictions, all_targets, all_ids):\n",
    "            # Find the question in the dataset\n",
    "            item = next((x for x in self.test_dataset.data if x['qid'] == qid), None)\n",
    "            \n",
    "            if item is not None:\n",
    "                answer_type = item.get('answer_type', 'OPEN').upper()\n",
    "                \n",
    "                # Ensure answer_type is in our tracking dict\n",
    "                if answer_type not in type_stats:\n",
    "                    type_stats[answer_type] = {\n",
    "                        'correct': 0, 'total': 0, \n",
    "                        'predictions': [], 'targets': []\n",
    "                    }\n",
    "                \n",
    "                type_stats[answer_type]['total'] += 1\n",
    "                type_stats[answer_type]['predictions'].append(pred)\n",
    "                type_stats[answer_type]['targets'].append(target)\n",
    "                \n",
    "                if pred == target:\n",
    "                    type_stats[answer_type]['correct'] += 1\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        overall_correct = sum(p == t for p, t in zip(all_predictions, all_targets))\n",
    "        overall_total = len(all_predictions)\n",
    "        overall_acc = 100 * overall_correct / overall_total if overall_total > 0 else 0\n",
    "        \n",
    "        type_accuracies = {}\n",
    "        for answer_type, stats in type_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                acc = 100 * stats['correct'] / stats['total']\n",
    "                type_accuracies[answer_type] = acc\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"DETAILED EVALUATION RESULTS\")\n",
    "        print(f\"Overall Accuracy: {overall_acc:.2f}% ({overall_correct}/{overall_total})\")\n",
    "        print(f\"\\nPerrformance on Answer Types:\")\n",
    "        \n",
    "        for answer_type in sorted(type_stats.keys()):\n",
    "            stats = type_stats[answer_type]\n",
    "            if stats['total'] > 0:\n",
    "                acc = type_accuracies[answer_type]\n",
    "                print(f\"  {answer_type:12s}: {acc:6.2f}% ({stats['correct']:4d}/{stats['total']:4d})\")\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        results = {\n",
    "            'overall_accuracy': overall_acc,\n",
    "            'overall_correct': overall_correct,\n",
    "            'overall_total': overall_total,\n",
    "            'type_accuracies': type_accuracies,\n",
    "            'type_stats': {\n",
    "                answer_type: {\n",
    "                    'accuracy': type_accuracies.get(answer_type, 0),\n",
    "                    'correct': stats['correct'],\n",
    "                    'total': stats['total']\n",
    "                }\n",
    "                for answer_type, stats in type_stats.items()\n",
    "            },\n",
    "            'predictions': all_predictions,\n",
    "            'targets': all_targets,\n",
    "            'ids': all_ids\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train(self, num_epochs=100, threshold=15, save_every=10):\n",
    "        print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
    "        print(f\"Training for up to {num_epochs} epochs\")\n",
    "        print(f\"Early stopping threshold: {threshold} epochs\")\n",
    "\n",
    "        print(f\"\\nBest hyperparameters:\")\n",
    "        print(json.dumps(self.best_params, indent=2))\n",
    "        \n",
    "        # 1. Create model with best hyperparameters\n",
    "        model = VQA_ResNet_BiLSTM_Attention(\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=self.best_params['best_params']['embed_dim'],\n",
    "            lstm_hidden=self.best_params['best_params']['lstm_hidden'],\n",
    "            lstm_num_layers=self.best_params['best_params']['lstm_num_layers'],\n",
    "            lstm_dropout=self.best_params['best_params']['lstm_dropout'],\n",
    "            pooling_strategy=self.best_params['best_params']['pooling_strategy'],\n",
    "            attention_heads=self.best_params['best_params']['attention_heads'],\n",
    "            fusion_dim=self.best_params['best_params']['fusion_dim'],\n",
    "            fusion_dropout=self.best_params['best_params']['fusion_dropout'],\n",
    "        ).to(device)\n",
    "        \n",
    "        # 2. Create dataloaders with best batch size\n",
    "        batch_size = self.best_params['best_params']['batch_size']\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 3. Setup optimizer and scheduler with best parameters\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.best_params['best_params']['learning_rate'],\n",
    "            weight_decay=self.best_params['best_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.best_params['best_params']['scheduler_step_size'],\n",
    "            gamma=self.best_params['best_params']['scheduler_gamma']\n",
    "        )\n",
    "        \n",
    "        # 4. Training loop\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "        threshold_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch(\n",
    "                model, train_loader, criterion, optimizer\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = self.validate(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "            \n",
    "            # Get current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                threshold_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                self.save_checkpoint(\n",
    "                    model, optimizer, epoch, val_acc, \n",
    "                    filename='best_model.pth'\n",
    "                )\n",
    "                print(f\"New best model found with Val Acc: {val_acc:.2f}%\")\n",
    "            else:\n",
    "                threshold_counter += 1\n",
    "                print(f\"No improvement ({threshold_counter}/{threshold})\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if (epoch + 1) % save_every == 0:\n",
    "                self.save_checkpoint(\n",
    "                    model, optimizer, epoch, val_acc,\n",
    "                    filename=f'checkpoint_epoch_{epoch+1}.pth'\n",
    "                )\n",
    "            \n",
    "            # Early stopping check\n",
    "            if threshold_counter >= threshold:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "                break\n",
    "        \n",
    "        # 5. Load best model and evaluate on test set\n",
    "        print(\"FINAL EVALUATION ON TEST SET\")\n",
    "        \n",
    "        self.load_checkpoint(model, 'best_model.pth')\n",
    "        test_results = self.final_evaluation(model)\n",
    "        \n",
    "        # 6. Save training history and results\n",
    "        self.save_results(test_results, best_epoch, best_val_acc)\n",
    "        \n",
    "        # 7. Plot training curves\n",
    "        self.plot_training_curves()\n",
    "        \n",
    "        print(\"TRAINING COMPLETE!\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "        print(f\"\\nTest Set Results:\")\n",
    "        print(f\"  Overall Accuracy: {test_results['overall_accuracy']:.2f}%\")\n",
    "        if 'type_accuracies' in test_results:\n",
    "            print(f\"\\n  By Answer Type:\")\n",
    "            for answer_type in sorted(test_results['type_accuracies'].keys()):\n",
    "                acc = test_results['type_accuracies'][answer_type]\n",
    "                total = test_results['type_stats'][answer_type]['total']\n",
    "                print(f\"    {answer_type:12s}: {acc:6.2f}% ({total:4d} samples)\")\n",
    "        print(f\"\\nResults saved to: {self.results_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return model, test_results\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, epoch, val_acc, filename):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'best_params': self.best_params,\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, self.results_dir / filename)\n",
    "    \n",
    "    def load_checkpoint(self, model, filename):\n",
    "        checkpoint = torch.load(self.results_dir / filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from {filename} (Epoch {checkpoint['epoch']}, Val Acc: {checkpoint['val_acc']:.2f}%)\")\n",
    "    \n",
    "    def save_results(self, test_results, best_epoch, best_val_acc):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        results = {\n",
    "            'timestamp': timestamp,\n",
    "            'best_hyperparameters': self.best_params,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'test_results': {\n",
    "                'overall_accuracy': test_results['overall_accuracy'],\n",
    "                'overall_correct': test_results['overall_correct'],\n",
    "                'overall_total': test_results['overall_total'],\n",
    "                'type_accuracies': test_results['type_accuracies'],\n",
    "                'type_stats': test_results['type_stats']\n",
    "            },\n",
    "            'training_history': self.history\n",
    "        }\n",
    "        \n",
    "        with open(self.results_dir / f'final_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Create type-specific accuracy plot\n",
    "        self.plot_type_accuracies(test_results['type_accuracies'], test_results['type_stats'])\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        _, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        axes[0, 1].plot(epochs, self.history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, self.history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        axes[1, 0].plot(epochs, self.history['learning_rates'], 'g-', linewidth=2)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation accuracy with best marker\n",
    "        axes[1, 1].plot(epochs, self.history['val_acc'], 'r-', linewidth=2)\n",
    "        best_epoch = np.argmax(self.history['val_acc']) + 1\n",
    "        best_acc = max(self.history['val_acc'])\n",
    "        axes[1, 1].scatter([best_epoch], [best_acc], color='gold', s=200, \n",
    "                          marker='*', edgecolors='black', linewidths=2, \n",
    "                          label=f'Best: {best_acc:.2f}% (Epoch {best_epoch})', zorder=5)\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Validation Accuracy (%)')\n",
    "        axes[1, 1].set_title('Validation Accuracy Progress')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.results_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_type_accuracies(self, type_accuracies, type_stats):\n",
    "        if not type_accuracies:\n",
    "            return\n",
    "        \n",
    "        _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Sort by answer type\n",
    "        answer_types = sorted(type_accuracies.keys())\n",
    "        accuracies = [type_accuracies[t] for t in answer_types]\n",
    "        totals = [type_stats[t]['total'] for t in answer_types]\n",
    "        \n",
    "        # Bar plot of accuracies\n",
    "        colors = ['#2ecc71' if acc >= 80 else '#f39c12' if acc >= 70 else '#e74c3c' \n",
    "                  for acc in accuracies]\n",
    "        bars = ax1.bar(answer_types, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax1.set_xlabel('Answer Type', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Accuracy by Answer Type', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylim([0, 100])\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acc:.1f}%',\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Pie chart of sample distribution\n",
    "        colors_pie = ['#3498db', '#e67e22', '#9b59b6', '#1abc9c'][:len(answer_types)]\n",
    "        wedges, texts, autotexts = ax2.pie(totals, labels=answer_types, autopct='%1.1f%%',\n",
    "                                            colors=colors_pie, startangle=90,\n",
    "                                            textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "        ax2.set_title('Sample Distribution by Answer Type', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add legend with counts\n",
    "        legend_labels = [f'{t}: {type_stats[t][\"total\"]} samples' for t in answer_types]\n",
    "        ax2.legend(legend_labels, loc='best', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.results_dir / 'type_specific_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Type-specific accuracy plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a14827f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_trainer = FinalModelTrainer(\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    test_dataset,\n",
    "    best_params,\n",
    "    len(question_vocab),\n",
    "    len(answer_vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0a99076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\n",
      "Training for up to 100 epochs\n",
      "Early stopping threshold: 15 epochs\n",
      "\n",
      "Best hyperparameters:\n",
      "{\n",
      "  \"best_params\": {\n",
      "    \"embed_dim\": 300,\n",
      "    \"lstm_hidden\": 768,\n",
      "    \"lstm_num_layers\": 1,\n",
      "    \"lstm_dropout\": 0.16556256559379687,\n",
      "    \"pooling_strategy\": \"mean\",\n",
      "    \"attention_heads\": 16,\n",
      "    \"fusion_dim\": 1024,\n",
      "    \"fusion_dropout\": 0.23092104046935436,\n",
      "    \"batch_size\": 16,\n",
      "    \"learning_rate\": 0.00010345483171807115,\n",
      "    \"weight_decay\": 9.368386876407338e-06,\n",
      "    \"scheduler_step_size\": 13,\n",
      "    \"scheduler_gamma\": 0.35564191057291783\n",
      "  },\n",
      "  \"best_value\": 82.52611585944919,\n",
      "  \"best_trial\": 33\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\WOA7015 - Advanced ML\\Assignments\\AA\\aml_aa\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\Documents\\WOA7015 - Advanced ML\\Assignments\\AA\\aml_aa\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|         | 18/308 [00:43<11:36,  2.40s/it, loss=4.3515, acc=17.36%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_model, test_results = \u001b[43mfinal_model_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 252\u001b[39m, in \u001b[36mFinalModelTrainer.train\u001b[39m\u001b[34m(self, num_epochs, threshold, save_every)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m train_loss, train_acc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    257\u001b[39m val_loss, val_acc = \u001b[38;5;28mself\u001b[39m.validate(\n\u001b[32m    258\u001b[39m     model, val_loader, criterion\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mFinalModelTrainer.train_epoch\u001b[39m\u001b[34m(self, model, dataloader, criterion, optimizer)\u001b[39m\n\u001b[32m     45\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m predictions = torch.argmax(logits, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     50\u001b[39m correct += (predictions == answers).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "final_model, test_results = final_model_trainer.train(\n",
    "    num_epochs=100,\n",
    "    threshold=15,\n",
    "    save_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a5659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
