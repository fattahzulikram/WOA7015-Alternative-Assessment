{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc2e801",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32082585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\WOA7015 - Advanced ML\\Assignments\\AA\\aml_aa\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import nltk\n",
    "import json\n",
    "import optuna\n",
    "import random\n",
    "import joblib\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf434d8",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18d6bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "MAX_NODES_PER_QUESTION = 10\n",
    "\n",
    "# Directory Information\n",
    "DATA_DIR = \"data/\"\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\n",
    "VOCABS_PATH = os.path.join(DATA_DIR, 'vocabs/')\n",
    "HYPERPARAMETERS_RESULT_PATH = os.path.join(DATA_DIR, 'tuning/')\n",
    "FINAL_MODEL_PATH = os.path.join(DATA_DIR, 'final_model/')\n",
    "\n",
    "# Huggingface Repository Information\n",
    "repo_id = \"BoKelvin/SLAKE\"\n",
    "repo_type = \"dataset\"\n",
    "img_file = \"imgs.zip\"\n",
    "\n",
    "# Seeding\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc6ba9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed():\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    torch.manual_seed(GLOBAL_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(GLOBAL_SEED)\n",
    "        torch.cuda.manual_seed_all(GLOBAL_SEED)\n",
    "        # For deterministic CuDNN operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0dd1b",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "### Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40e0ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for downloading and extracting ZIP file\n",
    "def download_and_store_ZIP(filename, save_dir):\n",
    "    print(f\"Fetching file {filename} from {repo_id} repo\")\n",
    "\n",
    "    try:\n",
    "        # Caches the file locally and returns the path to the cached file\n",
    "        cached_zip_path = hf_hub_download(\n",
    "          repo_id=repo_id,\n",
    "          filename=filename,\n",
    "          repo_type=repo_type\n",
    "        )\n",
    "        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Extract the contents\n",
    "        print(f\"Extracting to {save_dir}...\")\n",
    "        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "\n",
    "        print(\"Extraction complete.\")\n",
    "        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or extract {filename}: {e}\")\n",
    "\n",
    "# Scoping to English only\n",
    "def filter_language(original):\n",
    "    return original.filter(lambda data: data['q_lang'] == 'en')\n",
    "\n",
    "# Download and store the dataset\n",
    "def download_and_store_english_dataset():\n",
    "    print(f\"Downloading dataset from {repo_id} repo\")\n",
    "\n",
    "    # Load from Hugging Face\n",
    "    original = load_dataset(repo_id)\n",
    "\n",
    "    # Scope to English Only\n",
    "    original = filter_language(original)\n",
    "\n",
    "    # Show the dataset formatting\n",
    "    pprint(original)\n",
    "\n",
    "    # Save the original dataset\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        os.makedirs(DATASET_PATH)\n",
    "\n",
    "    original.save_to_disk(DATASET_PATH)\n",
    "    return original\n",
    "\n",
    "# Download and store the image files\n",
    "def download_and_store_image():\n",
    "    download_and_store_ZIP(img_file, DATA_DIR)\n",
    "\n",
    "# Download necessary files\n",
    "def download_and_store_slake():\n",
    "    dataset = download_and_store_english_dataset()\n",
    "    download_and_store_image()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00591e",
   "metadata": {},
   "source": [
    "### Vocabulary Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2e6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyBuilder:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def build_word_vocabs(self, sentences):\n",
    "        counter = Counter()\n",
    "        start_index = len(self.stoi)\n",
    "\n",
    "        # 1. Count frequencies of all tokens in the tokenized sentences\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenize(sentence)\n",
    "            counter.update(tokens)\n",
    "\n",
    "        # 2. Add words that meet the frequency threshold\n",
    "        for word, count in counter.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = start_index\n",
    "                self.itos[start_index] = word\n",
    "                start_index += 1\n",
    "\n",
    "        print(f\"Vocabulary Built. Vocabulary Size: {len(self.stoi)}\")\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n",
    "            for token in tokens\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe25ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies for questions and answers\n",
    "def build_vocabs(dataset):\n",
    "    questions = [item['question'] for item in dataset]\n",
    "    answers = [item['answer'] for item in dataset]\n",
    "\n",
    "    # Question Vocabulary\n",
    "    questvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "    questvocab_builder.build_word_vocabs(questions)\n",
    "    \n",
    "    # Answer Vocabulary\n",
    "    ansvocab_builder = VocabularyBuilder(min_freq=1)\n",
    "\n",
    "    # Use a dummy tokenizer that just returns the whole lowercased string as one token\n",
    "    identity_tokenizer = lambda x: [x.lower().strip()]\n",
    "    ansvocab_builder.tokenize = identity_tokenizer\n",
    "\n",
    "    ansvocab_builder.build_word_vocabs(answers)\n",
    "\n",
    "    return questvocab_builder, ansvocab_builder\n",
    "\n",
    "# Save vocabularies to JSON files\n",
    "def save_vocabs(quest_vocab, ans_vocab):\n",
    "    if not os.path.exists(VOCABS_PATH):\n",
    "        os.makedirs(VOCABS_PATH)\n",
    "\n",
    "    # Save Question Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'question_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': quest_vocab.stoi, 'itos': quest_vocab.itos}, f)\n",
    "\n",
    "    # Save Answer Vocabulary\n",
    "    with open(os.path.join(VOCABS_PATH, 'answer_vocab.json'), 'w') as f:\n",
    "        json.dump({'stoi': ans_vocab.stoi, 'itos': ans_vocab.itos}, f)\n",
    "\n",
    "    print(\"Vocabularies saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416468b",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00620cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlakeDataset(Dataset):\n",
    "    def __init__(self, dataset, question_vocab, answer_vocab, transform=None, cache_images=True):\n",
    "        self.data = dataset\n",
    "        self.question_vocab = question_vocab\n",
    "        self.answer_vocab = answer_vocab\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "\n",
    "        # Caching\n",
    "        self.image_cache = {}\n",
    "        if self.cache_images:\n",
    "            print(f\"Caching images for into RAM...\")\n",
    "            # Get unique image names to avoid duplicate loading\n",
    "            unique_imgs = set(item['img_name'] for item in self.data)\n",
    "            \n",
    "            for img_name in unique_imgs:\n",
    "                path = os.path.join(IMAGE_PATH, img_name)\n",
    "                # Load and convert to RGB\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                \n",
    "                # Resize immediately to save RAM and CPU later\n",
    "                img = img.resize((224, 224)) \n",
    "                \n",
    "                self.image_cache[img_name] = img\n",
    "            print(f\"Cached {len(self.image_cache)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Image Processing\n",
    "        image_path = item['img_name']\n",
    "\n",
    "        if self.cache_images:\n",
    "            # Get from RAM\n",
    "            image = self.image_cache[image_path]\n",
    "        else:\n",
    "            # Load from Disk and Resize\n",
    "            img_path = os.path.join(IMAGE_PATH, image_path)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.resize((224, 224))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 2. Question Processing\n",
    "        question = item['question']\n",
    "        question_indices = self.question_vocab.numericalize(question)\n",
    "\n",
    "        # 3. Answer Processing\n",
    "        answer = str(item.get('answer', '')) # Answer may be missing in test set\n",
    "        answer_index = self.answer_vocab.numericalize(answer)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'question' : torch.tensor(question_indices),\n",
    "            'answer' : torch.tensor(answer_index, dtype=torch.long),\n",
    "            # Add original items for reference\n",
    "            'original_question': question,\n",
    "            'original_answer': answer,\n",
    "            # Add ID for tracking\n",
    "            'id': item['qid']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fff29b",
   "metadata": {},
   "source": [
    "### Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slake_collate_fn(batch, pad_index=0):\n",
    "    # Separate different components\n",
    "    images = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    original_questions = []\n",
    "    original_answers = []\n",
    "    ids = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        questions.append(item['question'])\n",
    "        answers.append(item['answer'])\n",
    "        original_questions.append(item['original_question'])\n",
    "        original_answers.append(item['original_answer'])\n",
    "        ids.append(item['id'])\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images)  # [batch_size, 3, H, W]\n",
    "    \n",
    "    # Get question lengths BEFORE padding\n",
    "    question_lengths = torch.tensor([len(q) for q in questions])\n",
    "    \n",
    "    # Pad questions to the longest sequence in THIS batch\n",
    "    # pad_sequence expects list of tensors, pads with 0 by default\n",
    "    questions_padded = pad_sequence(questions, batch_first=True, padding_value=pad_index)\n",
    "    # questions_padded: [batch_size, max_len_in_batch]\n",
    "    \n",
    "    # Handling answers\n",
    "    # Handling each answer as a single class\n",
    "    # answers = torch.stack(answers)\n",
    "    answers = torch.tensor([item['answer'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'question': questions_padded,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer': answers,\n",
    "        'original_question': original_questions,\n",
    "        'original_answer': original_answers,\n",
    "        'id': ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f8d268",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a6559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Built. Vocabulary Size: 281\n",
      "Vocabulary Built. Vocabulary Size: 225\n",
      "Caching images for into RAM...\n"
     ]
    }
   ],
   "source": [
    "# Comment out if dataset is already downloaded\n",
    "# dataset = download_and_store_slake()\n",
    "\n",
    "# Uncomment if dataset is already downloaded\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Build vocabularies for training\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "question_vocab, answer_vocab = build_vocabs(train_data)\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create train dataset and dataloader\n",
    "train_dataset = SlakeDataset(train_data, question_vocab, answer_vocab, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=slake_collate_fn\n",
    ")\n",
    "\n",
    "validation_dataset = SlakeDataset(validation_data, question_vocab, answer_vocab, transform=transform)\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=slake_collate_fn\n",
    ")\n",
    "\n",
    "test_dataset = SlakeDataset(test_data, question_vocab, answer_vocab, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999865a2",
   "metadata": {},
   "source": [
    "## Modeling Baseline\n",
    "\n",
    "CNN with Bidirectional LSTM with Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c65f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM with Self-Attention for question encoding\n",
    "class BiLSTMWithSelfAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=1, \n",
    "                 dropout=0.5, pooling_strategy='mean', attention_heads=8):\n",
    "        super(BiLSTMWithSelfAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pooling_strategy = pooling_strategy\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Self-attention mechanism\n",
    "        # BiLSTM outputs hidden_dim * 2 (forward + backward)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=attention_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        \n",
    "    def forward(self, questions, question_lengths=None):\n",
    "        # Embed questions\n",
    "        embeds = self.embedding(questions)  # [B, seq_len, embed_dim]\n",
    "        embeds = self.dropout(embeds)\n",
    "        \n",
    "        # Pack sequence if lengths provided (for efficiency)\n",
    "        if question_lengths is not None:\n",
    "            embeds = nn.utils.rnn.pack_padded_sequence(\n",
    "                embeds, question_lengths.cpu(), \n",
    "                batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        \n",
    "        # BiLSTM encoding\n",
    "        lstm_out, (hidden, cell) = self.bilstm(embeds)\n",
    "        \n",
    "        # Unpack if needed\n",
    "        if question_lengths is not None:\n",
    "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "                lstm_out, batch_first=True\n",
    "            )\n",
    "        \n",
    "        # lstm_out: [B, seq_len, hidden_dim * 2]\n",
    "        \n",
    "        # Self-attention: query = key = value = lstm_out\n",
    "        attn_out, attn_weights = self.attention(\n",
    "            query=lstm_out,\n",
    "            key=lstm_out,\n",
    "            value=lstm_out,\n",
    "            need_weights=True\n",
    "        )\n",
    "        \n",
    "        # Residual connection + Layer Norm\n",
    "        attn_out = self.layer_norm(lstm_out + attn_out)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        \n",
    "        # Pooling strategy - you can experiment with these:\n",
    "        if self.pooling_strategy == 'mean':\n",
    "            question_feature = attn_out.mean(dim=1)  # [B, hidden_dim * 2]\n",
    "        elif self.pooling_strategy == 'max':\n",
    "            question_feature = attn_out.max(dim=1)[0]\n",
    "        else:\n",
    "            # Last hidden state (concatenate forward and backward)\n",
    "            question_feature = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        return question_feature, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a86567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete VQA model: ResNet34 + BiLSTM with Self-Attention\n",
    "class VQA_ResNet_BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, embed_dim=300, \n",
    "                 lstm_hidden=512, fusion_dim=1024, lstm_dropout=0.5, \n",
    "                 lstm_num_layers=1, attention_heads=8, fusion_dropout=0.5,\n",
    "                 pooling_strategy='mean'):\n",
    "        super(VQA_ResNet_BiLSTM_Attention, self).__init__()\n",
    "        \n",
    "        # Image encoder: ResNet34\n",
    "        resnet = models.resnet34(pretrained=True)\n",
    "        # Remove the final FC layer\n",
    "        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.image_feature_dim = 512  # ResNet34 final layer\n",
    "        \n",
    "        # Question encoder: BiLSTM + Self-Attention\n",
    "        self.question_encoder = BiLSTMWithSelfAttention(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            hidden_dim=lstm_hidden,\n",
    "            num_layers=lstm_num_layers,\n",
    "            dropout=lstm_dropout,\n",
    "            attention_heads=attention_heads,\n",
    "            pooling_strategy=pooling_strategy\n",
    "        )\n",
    "        self.question_feature_dim = lstm_hidden * 2  # Bidirectional\n",
    "        \n",
    "        # Multimodal fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.image_feature_dim + self.question_feature_dim, fusion_dim),\n",
    "            nn.BatchNorm1d(fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fusion_dropout),\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.BatchNorm1d(fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(fusion_dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n",
    "        \n",
    "    def forward(self, images, questions, question_lengths=None):\n",
    "        # Extract image features\n",
    "        img_features = self.image_encoder(images)  # [B, 512, 1, 1]\n",
    "        img_features = img_features.squeeze(-1).squeeze(-1)  # [B, 512]\n",
    "        \n",
    "        # Extract question features with attention\n",
    "        q_features, attn_weights = self.question_encoder(questions, question_lengths) # [B, lstm_hidden * 2]\n",
    "        \n",
    "        # Concatenate image and question features\n",
    "        combined = torch.cat([img_features, q_features], dim=1)\n",
    "        # combined: [B, 512 + lstm_hidden*2]\n",
    "        \n",
    "        # Fusion\n",
    "        fused = self.fusion(combined)  # [B, fusion_dim // 2]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused)  # [B, num_classes]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc61c29",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        questions = batch['question'].to(device)\n",
    "        question_lengths = batch['question_lengths'].to(device)\n",
    "        answers = batch['answer'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(images, questions, question_lengths)\n",
    "        loss = criterion(logits, answers)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct += (predictions == answers).sum().item()\n",
    "        total += answers.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total\n",
    "\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Validating'):\n",
    "            images = batch['image'].to(device)\n",
    "            questions = batch['question'].to(device)\n",
    "            question_lengths = batch['question_lengths'].to(device)\n",
    "            answers = batch['answer'].to(device)\n",
    "            \n",
    "            logits = model(images, questions, question_lengths)\n",
    "            loss = criterion(logits, answers)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == answers).sum().item()\n",
    "            total += answers.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    def __init__(self, train_dataset, validation_dataset, vocab_size, num_classes, \n",
    "                 n_trials=50):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.validation_dataset = validation_dataset\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_trials = n_trials\n",
    "        self.results_dir = Path(HYPERPARAMETERS_RESULT_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Track all trial results\n",
    "        self.trial_results = []\n",
    "\n",
    "    def config_BLSTM(self, trial):\n",
    "        return {\n",
    "            # Embedding parameters\n",
    "            'embed_dim': trial.suggest_categorical('embed_dim', [200, 300, 512]),\n",
    "\n",
    "            # LSTM parameters\n",
    "            'lstm_hidden': trial.suggest_categorical('lstm_hidden', [256, 512, 768, 1024]),\n",
    "            'lstm_num_layers': trial.suggest_int('lstm_num_layers', 1, 3),\n",
    "            'lstm_dropout': trial.suggest_float('lstm_dropout', 0.1, 0.6),\n",
    "            'pooling_strategy': trial.suggest_categorical('pooling_strategy', ['mean', 'max', 'last']),\n",
    "\n",
    "            # Attention parameters\n",
    "            'attention_heads': trial.suggest_categorical('attention_heads', [4, 8, 16]),\n",
    "\n",
    "            # Fusion parameters\n",
    "            'fusion_dim': trial.suggest_categorical('fusion_dim', [512, 1024, 2048]),\n",
    "            'fusion_dropout': trial.suggest_float('fusion_dropout', 0.2, 0.6),\n",
    "\n",
    "            # Training parameters\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n",
    "            'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n",
    "            'scheduler_step_size': trial.suggest_int('scheduler_step_size', 5, 15),\n",
    "            'scheduler_gamma': trial.suggest_float('scheduler_gamma', 0.3, 0.7),\n",
    "        }\n",
    "\n",
    "    def objective(self, trial):\n",
    "        print(f\"Trial {trial.number + 1}/{self.n_trials}\")\n",
    "\n",
    "        config = self.config_BLSTM(trial)\n",
    "        model = VQA_ResNet_BiLSTM_Attention(\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=config['embed_dim'],\n",
    "            lstm_hidden=config['lstm_hidden'],\n",
    "            lstm_num_layers=config['lstm_num_layers'],\n",
    "            attention_heads=config['attention_heads'],\n",
    "            fusion_dim=config['fusion_dim'],\n",
    "            lstm_dropout=config['lstm_dropout'],\n",
    "            fusion_dropout=config['fusion_dropout'],\n",
    "            pooling_strategy=config['pooling_strategy']\n",
    "        ).to(device)\n",
    "        \n",
    "        for param in model.image_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "        print(f\"Config: {json.dumps(config, indent=2)}\")\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.validation_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=config['scheduler_step_size'],\n",
    "            gamma=config['scheduler_gamma']\n",
    "        )\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        threshold = 5\n",
    "        threshold_count = 0\n",
    "        max_epochs = 30\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer\n",
    "            )\n",
    "\n",
    "            val_loss, val_acc = validate(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                threshold_count = 0\n",
    "            else:\n",
    "                threshold_count += 1\n",
    "            \n",
    "            if threshold_count >= threshold:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            trial.report(val_acc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        trial_result = {\n",
    "            'trial_number': trial.number,\n",
    "            'config': config,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'final_epoch': epoch + 1\n",
    "        }\n",
    "        self.trial_results.append(trial_result)\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    def save_results(self, study):\n",
    "        # Save best parameters\n",
    "        best_params_path = self.results_dir / f'best_params_BLSTM.json'\n",
    "        with open(best_params_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'best_params': study.best_params,\n",
    "                'best_value': study.best_value,\n",
    "                'best_trial': study.best_trial.number\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        # Save all trial results\n",
    "        all_results_path = self.results_dir / f'all_trials_BLSTM.json'\n",
    "        with open(all_results_path, 'w') as f:\n",
    "            json.dump(self.trial_results, f, indent=2)\n",
    "        \n",
    "        # Save study\n",
    "        study_path = self.results_dir / f'study_BLSTM.pkl'\n",
    "        joblib.dump(study, study_path)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {self.results_dir}\")\n",
    "\n",
    "    def run(self):\n",
    "        print(\"STARTING HYPERPARAMETER TUNING FOR BLSTM MODEL\\n\")\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
    "            sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED)\n",
    "        )\n",
    "\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        self.save_results(study)\n",
    "\n",
    "        # Print best results\n",
    "        print(\"HYPERPARAMETER TUNING COMPLETE\")\n",
    "        print(f\"Best Trial: {study.best_trial.number}\")\n",
    "        print(f\"Best Validation Accuracy: {study.best_value:.2f}%\\n\")\n",
    "        print(f\"Best Hyperparameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc453c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Hyperparameters for a BLSTM model\n",
    "tuner = HyperparameterTuner(\n",
    "    vocab_size=len(question_vocab),\n",
    "    num_classes=len(answer_vocab),\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    n_trials=50\n",
    ")\n",
    "\n",
    "# Run tuning\n",
    "study = tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018251b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best hyperparameter for the model\n",
    "best_params_path = os.path.join(HYPERPARAMETERS_RESULT_PATH, 'best_params_BLSTM.json')\n",
    "with open(best_params_path, 'r') as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3362a",
   "metadata": {},
   "source": [
    "## Train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98886ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModelTrainer:\n",
    "    def __init__(self, train_dataset, val_dataset, test_dataset, \n",
    "                 best_params, vocab_size, num_classes):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.best_params = best_params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        \n",
    "        # Create results directory\n",
    "        self.results_dir = Path(FINAL_MODEL_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def final_evaluation(self, model):\n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        # Store all predictions and results\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_ids = []\n",
    "        \n",
    "        # Type-specific tracking\n",
    "        type_stats = {\n",
    "            'CLOSED': {'correct': 0, 'total': 0, 'predictions': [], 'targets': []},\n",
    "            'OPEN': {'correct': 0, 'total': 0, 'predictions': [], 'targets': []}\n",
    "        }\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Testing'):\n",
    "                images = batch['image'].to(device)\n",
    "                questions = batch['question'].to(device)\n",
    "                question_lengths = batch['question_lengths'].to(device)\n",
    "                answers = batch['answer'].to(device)\n",
    "                \n",
    "                logits = model(images, questions, question_lengths)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().tolist())\n",
    "                all_targets.extend(answers.cpu().tolist())\n",
    "                all_ids.extend(batch['id'])\n",
    "        \n",
    "        # Categorize by answer type\n",
    "        for pred, target, qid in zip(all_predictions, all_targets, all_ids):\n",
    "            # Find the question in the dataset\n",
    "            item = next((x for x in self.test_dataset.data if x['qid'] == qid), None)\n",
    "            \n",
    "            if item is not None:\n",
    "                answer_type = item.get('answer_type', 'OPEN').upper()\n",
    "                \n",
    "                # Ensure answer_type is in our tracking dict\n",
    "                if answer_type not in type_stats:\n",
    "                    type_stats[answer_type] = {\n",
    "                        'correct': 0, 'total': 0, \n",
    "                        'predictions': [], 'targets': []\n",
    "                    }\n",
    "                \n",
    "                type_stats[answer_type]['total'] += 1\n",
    "                type_stats[answer_type]['predictions'].append(pred)\n",
    "                type_stats[answer_type]['targets'].append(target)\n",
    "                \n",
    "                if pred == target:\n",
    "                    type_stats[answer_type]['correct'] += 1\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        overall_correct = sum(p == t for p, t in zip(all_predictions, all_targets))\n",
    "        overall_total = len(all_predictions)\n",
    "        overall_acc = 100 * overall_correct / overall_total if overall_total > 0 else 0\n",
    "        \n",
    "        type_accuracies = {}\n",
    "        for answer_type, stats in type_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                acc = 100 * stats['correct'] / stats['total']\n",
    "                type_accuracies[answer_type] = acc\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"DETAILED EVALUATION RESULTS\")\n",
    "        print(f\"Overall Accuracy: {overall_acc:.2f}% ({overall_correct}/{overall_total})\")\n",
    "        print(f\"\\nPerrformance on Answer Types:\")\n",
    "        \n",
    "        for answer_type in sorted(type_stats.keys()):\n",
    "            stats = type_stats[answer_type]\n",
    "            if stats['total'] > 0:\n",
    "                acc = type_accuracies[answer_type]\n",
    "                print(f\"  {answer_type:12s}: {acc:6.2f}% ({stats['correct']:4d}/{stats['total']:4d})\")\n",
    "        \n",
    "        # Prepare results dictionary\n",
    "        results = {\n",
    "            'overall_accuracy': overall_acc,\n",
    "            'overall_correct': overall_correct,\n",
    "            'overall_total': overall_total,\n",
    "            'type_accuracies': type_accuracies,\n",
    "            'type_stats': {\n",
    "                answer_type: {\n",
    "                    'accuracy': type_accuracies.get(answer_type, 0),\n",
    "                    'correct': stats['correct'],\n",
    "                    'total': stats['total']\n",
    "                }\n",
    "                for answer_type, stats in type_stats.items()\n",
    "            },\n",
    "            'predictions': all_predictions,\n",
    "            'targets': all_targets,\n",
    "            'ids': all_ids\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train(self, num_epochs=100, threshold=15, save_every=10):\n",
    "        print(\"TRAINING FINAL MODEL WITH BEST HYPERPARAMETERS\")\n",
    "        print(f\"Training for up to {num_epochs} epochs\")\n",
    "        print(f\"Early stopping threshold: {threshold} epochs\")\n",
    "\n",
    "        print(f\"\\nBest hyperparameters:\")\n",
    "        print(json.dumps(self.best_params, indent=2))\n",
    "        \n",
    "        # 1. Create model with best hyperparameters\n",
    "        model = VQA_ResNet_BiLSTM_Attention(\n",
    "            vocab_size=self.vocab_size,\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=self.best_params['best_params']['embed_dim'],\n",
    "            lstm_hidden=self.best_params['best_params']['lstm_hidden'],\n",
    "            lstm_num_layers=self.best_params['best_params']['lstm_num_layers'],\n",
    "            lstm_dropout=self.best_params['best_params']['lstm_dropout'],\n",
    "            pooling_strategy=self.best_params['best_params']['pooling_strategy'],\n",
    "            attention_heads=self.best_params['best_params']['attention_heads'],\n",
    "            fusion_dim=self.best_params['best_params']['fusion_dim'],\n",
    "            fusion_dropout=self.best_params['best_params']['fusion_dropout'],\n",
    "        ).to(device)\n",
    "        \n",
    "        # 2. Create dataloaders with best batch size\n",
    "        batch_size = self.best_params['best_params']['batch_size']\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=slake_collate_fn,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 3. Setup optimizer and scheduler with best parameters\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=self.best_params['best_params']['learning_rate'],\n",
    "            weight_decay=self.best_params['best_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.best_params['best_params']['scheduler_step_size'],\n",
    "            gamma=self.best_params['best_params']['scheduler_gamma']\n",
    "        )\n",
    "        \n",
    "        # 4. Training loop\n",
    "        best_val_acc = 0.0\n",
    "        best_epoch = 0\n",
    "        threshold_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = validate(\n",
    "                model, val_loader, criterion\n",
    "            )\n",
    "            \n",
    "            # Get current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "            print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_epoch = epoch + 1\n",
    "                threshold_counter = 0\n",
    "                \n",
    "                # Save best model\n",
    "                self.save_checkpoint(\n",
    "                    model, optimizer, epoch, val_acc, \n",
    "                    filename='best_model.pth'\n",
    "                )\n",
    "                print(f\"New best model found with Val Acc: {val_acc:.2f}%\")\n",
    "            else:\n",
    "                threshold_counter += 1\n",
    "                print(f\"No improvement ({threshold_counter}/{threshold})\")\n",
    "            \n",
    "            # Save periodic checkpoint\n",
    "            if (epoch + 1) % save_every == 0:\n",
    "                self.save_checkpoint(\n",
    "                    model, optimizer, epoch, val_acc,\n",
    "                    filename=f'checkpoint_epoch_{epoch+1}.pth'\n",
    "                )\n",
    "            \n",
    "            # Early stopping check\n",
    "            if threshold_counter >= threshold:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                print(f\"Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "                break\n",
    "        \n",
    "        # 5. Load best model and evaluate on test set\n",
    "        print(\"FINAL EVALUATION ON TEST SET\")\n",
    "        \n",
    "        self.load_checkpoint(model, 'best_model.pth')\n",
    "        test_results = self.final_evaluation(model)\n",
    "        \n",
    "        # 6. Save training history and results\n",
    "        self.save_results(test_results, best_epoch, best_val_acc)\n",
    "        \n",
    "        # 7. Plot training curves\n",
    "        self.plot_training_curves()\n",
    "        \n",
    "        print(\"TRAINING COMPLETE!\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "        print(f\"\\nTest Set Results:\")\n",
    "        print(f\"  Overall Accuracy: {test_results['overall_accuracy']:.2f}%\")\n",
    "        if 'type_accuracies' in test_results:\n",
    "            print(f\"\\n  By Answer Type:\")\n",
    "            for answer_type in sorted(test_results['type_accuracies'].keys()):\n",
    "                acc = test_results['type_accuracies'][answer_type]\n",
    "                total = test_results['type_stats'][answer_type]['total']\n",
    "                print(f\"    {answer_type:12s}: {acc:6.2f}% ({total:4d} samples)\")\n",
    "        print(f\"\\nResults saved to: {self.results_dir}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return model, test_results\n",
    "    \n",
    "    def save_checkpoint(self, model, optimizer, epoch, val_acc, filename):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'best_params': self.best_params,\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, self.results_dir / filename)\n",
    "    \n",
    "    def load_checkpoint(self, model, filename):\n",
    "        checkpoint = torch.load(self.results_dir / filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from {filename} (Epoch {checkpoint['epoch']}, Val Acc: {checkpoint['val_acc']:.2f}%)\")\n",
    "    \n",
    "    def save_results(self, test_results, best_epoch, best_val_acc):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        results = {\n",
    "            'timestamp': timestamp,\n",
    "            'best_hyperparameters': self.best_params,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_val_acc': best_val_acc,\n",
    "            'test_results': {\n",
    "                'overall_accuracy': test_results['overall_accuracy'],\n",
    "                'overall_correct': test_results['overall_correct'],\n",
    "                'overall_total': test_results['overall_total'],\n",
    "                'type_accuracies': test_results['type_accuracies'],\n",
    "                'type_stats': test_results['type_stats']\n",
    "            },\n",
    "            'training_history': self.history\n",
    "        }\n",
    "        \n",
    "        with open(self.results_dir / f'final_results_{timestamp}.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Create type-specific accuracy plot\n",
    "        self.plot_type_accuracies(test_results['type_accuracies'], test_results['type_stats'])\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        _, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss curves\n",
    "        axes[0, 0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy curves\n",
    "        axes[0, 1].plot(epochs, self.history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, self.history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning rate\n",
    "        axes[1, 0].plot(epochs, self.history['learning_rates'], 'g-', linewidth=2)\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Validation accuracy with best marker\n",
    "        axes[1, 1].plot(epochs, self.history['val_acc'], 'r-', linewidth=2)\n",
    "        best_epoch = np.argmax(self.history['val_acc']) + 1\n",
    "        best_acc = max(self.history['val_acc'])\n",
    "        axes[1, 1].scatter([best_epoch], [best_acc], color='gold', s=200, \n",
    "                          marker='*', edgecolors='black', linewidths=2, \n",
    "                          label=f'Best: {best_acc:.2f}% (Epoch {best_epoch})', zorder=5)\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Validation Accuracy (%)')\n",
    "        axes[1, 1].set_title('Validation Accuracy Progress')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.results_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_type_accuracies(self, type_accuracies, type_stats):\n",
    "        if not type_accuracies:\n",
    "            return\n",
    "        \n",
    "        _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Sort by answer type\n",
    "        answer_types = sorted(type_accuracies.keys())\n",
    "        accuracies = [type_accuracies[t] for t in answer_types]\n",
    "        totals = [type_stats[t]['total'] for t in answer_types]\n",
    "        \n",
    "        # Bar plot of accuracies\n",
    "        colors = ['#2ecc71' if acc >= 80 else '#f39c12' if acc >= 70 else '#e74c3c' \n",
    "                  for acc in accuracies]\n",
    "        bars = ax1.bar(answer_types, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "        ax1.set_xlabel('Answer Type', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Accuracy by Answer Type', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylim([0, 100])\n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acc:.1f}%',\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Pie chart of sample distribution\n",
    "        colors_pie = ['#3498db', '#e67e22', '#9b59b6', '#1abc9c'][:len(answer_types)]\n",
    "        wedges, texts, autotexts = ax2.pie(totals, labels=answer_types, autopct='%1.1f%%',\n",
    "                                            colors=colors_pie, startangle=90,\n",
    "                                            textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "        ax2.set_title('Sample Distribution by Answer Type', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add legend with counts\n",
    "        legend_labels = [f'{t}: {type_stats[t][\"total\"]} samples' for t in answer_types]\n",
    "        ax2.legend(legend_labels, loc='best', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.results_dir / 'type_specific_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Type-specific accuracy plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14827f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_trainer = FinalModelTrainer(\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    test_dataset,\n",
    "    best_params,\n",
    "    len(question_vocab),\n",
    "    len(answer_vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a99076",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, test_results = final_model_trainer.train(\n",
    "    num_epochs=100,\n",
    "    threshold=15,\n",
    "    save_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a5659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
