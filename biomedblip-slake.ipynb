{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:33:49.974992Z",
     "iopub.status.busy": "2026-01-11T14:33:49.974768Z",
     "iopub.status.idle": "2026-01-11T14:34:20.928065Z",
     "shell.execute_reply": "2026-01-11T14:34:20.927443Z",
     "shell.execute_reply.started": "2026-01-11T14:33:49.974966Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "\n",
    "import zipfile\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import BlipForQuestionAnswering, BlipProcessor\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from utils import plot_ngram_analysis, print_all_metrics, plot_type_specific_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:20.929448Z",
     "iopub.status.busy": "2026-01-11T14:34:20.928797Z",
     "iopub.status.idle": "2026-01-11T14:34:20.935567Z",
     "shell.execute_reply": "2026-01-11T14:34:20.934861Z",
     "shell.execute_reply.started": "2026-01-11T14:34:20.929420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (480, 480)\n",
    "GLOBAL_SEED = 42\n",
    "\n",
    "# Data\n",
    "DATA_DIR = 'data/'\n",
    "DATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\n",
    "IMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\n",
    "TUNING_RESULTS_PATH = os.path.join(DATA_DIR, 'tuning/')\n",
    "FINAL_MODEL_PATH = os.path.join(DATA_DIR, 'final_model_BioMedBLIP/')\n",
    "\n",
    "# Huggingface Repository Information\n",
    "repo_id = \"BoKelvin/SLAKE\"\n",
    "repo_type = \"dataset\"\n",
    "img_file = \"imgs.zip\"\n",
    "\n",
    "# Model Definition\n",
    "BASE_MODEL_NAME = \"Salesforce/blip-vqa-base\"\n",
    "MODEL_NAME = \"biomedblip/biomedblip\"\n",
    "MODEL_SAVE_NAME = \"biomedblip\"\n",
    "CHECKPOINT_LOCATION = \"Best BLIP Finetuning/VQA_generation_SLAKE(BLIP-MIMIC&ROCO-10)-006.pth\"\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:20.936811Z",
     "iopub.status.busy": "2026-01-11T14:34:20.936487Z",
     "iopub.status.idle": "2026-01-11T14:34:21.217008Z",
     "shell.execute_reply": "2026-01-11T14:34:21.216379Z",
     "shell.execute_reply.started": "2026-01-11T14:34:20.936772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_global_seed():\n",
    "    random.seed(GLOBAL_SEED)\n",
    "    np.random.seed(GLOBAL_SEED)\n",
    "    torch.manual_seed(GLOBAL_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(GLOBAL_SEED)\n",
    "        torch.cuda.manual_seed_all(GLOBAL_SEED)\n",
    "        # For deterministic CuDNN operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_global_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:21.241659Z",
     "iopub.status.busy": "2026-01-11T14:34:21.241421Z",
     "iopub.status.idle": "2026-01-11T14:34:21.256381Z",
     "shell.execute_reply": "2026-01-11T14:34:21.255866Z",
     "shell.execute_reply.started": "2026-01-11T14:34:21.241638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Utility function for downloading and extracting ZIP file\n",
    "def download_and_store_ZIP(filename, save_dir):\n",
    "    print(f\"Fetching file {filename} from {repo_id} repo\")\n",
    "\n",
    "    try:\n",
    "        # Caches the file locally and returns the path to the cached file\n",
    "        cached_zip_path = hf_hub_download(\n",
    "          repo_id=repo_id,\n",
    "          filename=filename,\n",
    "          repo_type=repo_type\n",
    "        )\n",
    "        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n",
    "\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Extract the contents\n",
    "        print(f\"Extracting to {save_dir}...\")\n",
    "        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(save_dir)\n",
    "\n",
    "        print(\"Extraction complete.\")\n",
    "        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download or extract {filename}: {e}\")\n",
    "\n",
    "# Scoping to English only\n",
    "def filter_language(original):\n",
    "    return original.filter(lambda data: data['q_lang'] == 'en')\n",
    "\n",
    "# Download and store the dataset\n",
    "def download_and_store_english_dataset():\n",
    "    print(f\"Downloading dataset from {repo_id} repo\")\n",
    "\n",
    "    # Load from Hugging Face\n",
    "    original = load_dataset(repo_id)\n",
    "\n",
    "    # Scope to English Only\n",
    "    original = filter_language(original)\n",
    "\n",
    "    # Show the dataset formatting\n",
    "    pprint(original)\n",
    "\n",
    "    # Save the original dataset\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        os.makedirs(DATASET_PATH)\n",
    "\n",
    "    original.save_to_disk(DATASET_PATH)\n",
    "    return original\n",
    "\n",
    "# Download and store the image files\n",
    "def download_and_store_image():\n",
    "    download_and_store_ZIP(img_file, DATA_DIR)\n",
    "\n",
    "# Download necessary files\n",
    "def download_and_store_slake():\n",
    "    dataset = download_and_store_english_dataset()\n",
    "    download_and_store_image()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:21.257316Z",
     "iopub.status.busy": "2026-01-11T14:34:21.257093Z",
     "iopub.status.idle": "2026-01-11T14:34:21.272842Z",
     "shell.execute_reply": "2026-01-11T14:34:21.272229Z",
     "shell.execute_reply.started": "2026-01-11T14:34:21.257296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def clean_text(self, text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.replace('\\n', ' ')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess_question(self, question):\n",
    "        question = self.clean_text(question)\n",
    "        if not question.endswith('?'):\n",
    "            question += '?'\n",
    "        \n",
    "        return question\n",
    "    \n",
    "    def preprocess_answer(self, answer):\n",
    "        answer = self.clean_text(answer)\n",
    "        return answer\n",
    "    \n",
    "    def handle_slake_specifics(self, item):\n",
    "        if item.get('q_lang') == 'en':\n",
    "            question = self.preprocess_question(item['question'])\n",
    "            answer = self.preprocess_answer(item['answer'])\n",
    "            \n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'answer_type': item.get('answer_type'),\n",
    "                'img_name': item['img_name']\n",
    "            }\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:21.274096Z",
     "iopub.status.busy": "2026-01-11T14:34:21.273575Z",
     "iopub.status.idle": "2026-01-11T14:34:21.292123Z",
     "shell.execute_reply": "2026-01-11T14:34:21.291438Z",
     "shell.execute_reply.started": "2026-01-11T14:34:21.274074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SLAKEDatasetBioMedBLIP(Dataset):\n",
    "    def __init__(self, data, processor, transform=None, cache_images=True):\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        processed_data = []\n",
    "        for item in self.data:\n",
    "            processed = self.text_preprocessor.handle_slake_specifics(item)\n",
    "            processed_data.append(processed)\n",
    "        self.data = processed_data\n",
    "\n",
    "        self.vocab_size = len(self.processor.tokenizer)\n",
    "        self.pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            self.pad_token_id = 0\n",
    "\n",
    "        # Caching\n",
    "        self.image_cache = {}\n",
    "        if self.cache_images:\n",
    "            print(f\"Caching images for into RAM...\")\n",
    "            # Get unique image names to avoid duplicate loading\n",
    "            unique_imgs = set(item['img_name'] for item in self.data)\n",
    "            \n",
    "            for img_name in unique_imgs:\n",
    "                path = os.path.join(IMAGE_PATH, img_name)\n",
    "                # Load and convert to RGB\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                \n",
    "                # Resize immediately to save RAM and CPU later\n",
    "                img = img.resize((224, 224)) \n",
    "                \n",
    "                self.image_cache[img_name] = img\n",
    "            print(f\"Cached {len(self.image_cache)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Image Processing\n",
    "        image_path = item['img_name']\n",
    "\n",
    "        if self.cache_images:\n",
    "            # Get from RAM\n",
    "            image = self.image_cache[image_path]\n",
    "        else:\n",
    "            # Load from Disk and Resize\n",
    "            img_path = os.path.join(IMAGE_PATH, image_path)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.resize((224, 224))\n",
    "\n",
    "        # 2. Process using processor\n",
    "        question = item['question']\n",
    "        answer = str(item.get('answer', ''))\n",
    "        \n",
    "\n",
    "        # Process with BLIP processor\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=question,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "        # Process Target (Answer)\n",
    "        answer_encoding = self.processor.tokenizer(\n",
    "            answer,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            max_length=32,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Clamp token IDs to valid range\n",
    "        labels = answer_encoding['input_ids'].squeeze(0)\n",
    "        labels = torch.clamp(labels, max=self.vocab_size - 1)\n",
    "        \n",
    "        # Replace padding token IDs with -100 (ignored in loss)\n",
    "        labels[labels == self.pad_token_id] = -100\n",
    "        encoding['labels'] = labels\n",
    "\n",
    "        # Add metadata\n",
    "        encoding['question_type'] = item.get('answer_type', 'UNKNOWN')\n",
    "        encoding['answer_text'] = answer\n",
    "        encoding['question_text'] = question\n",
    "        encoding['id'] = item.get('qid', idx)\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:21.293368Z",
     "iopub.status.busy": "2026-01-11T14:34:21.293100Z",
     "iopub.status.idle": "2026-01-11T14:34:21.310312Z",
     "shell.execute_reply": "2026-01-11T14:34:21.309733Z",
     "shell.execute_reply.started": "2026-01-11T14:34:21.293345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Stack tensors\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    \n",
    "    # Collect metadata\n",
    "    question_types = [item['question_type'] for item in batch]\n",
    "    answers = [item['answer_text'] for item in batch]\n",
    "    questions = [item['question_text'] for item in batch]\n",
    "    ids = [item['id'] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'question_types': question_types,\n",
    "        'answers': answers,\n",
    "        'questions': questions,\n",
    "        'ids': ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:21.327888Z",
     "iopub.status.busy": "2026-01-11T14:34:21.327620Z",
     "iopub.status.idle": "2026-01-11T14:34:28.736096Z",
     "shell.execute_reply": "2026-01-11T14:34:28.735423Z",
     "shell.execute_reply.started": "2026-01-11T14:34:21.327858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset = download_and_store_slake()\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "\n",
    "# Toggle to turn on/off data testing\n",
    "test_data = True\n",
    "\n",
    "if test_data:\n",
    "    # Initialize processor\n",
    "    processor = BlipProcessor.from_pretrained(\n",
    "        \"Salesforce/blip-vqa-base\",\n",
    "        use_fast=True\n",
    "    )\n",
    "\n",
    "    # Get dataset for testing\n",
    "    test_data = dataset['test']\n",
    "    test_dataset = SLAKEDatasetBioMedBLIP(\n",
    "        data=test_data,\n",
    "        processor=processor,\n",
    "        cache_images=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Test loading a batch\n",
    "    print(\"\\nTesting data loading...\")\n",
    "    batch = next(iter(test_loader))\n",
    "\n",
    "    print(f\"Batch keys: {batch.keys()}\")\n",
    "    print(f\"Pixel values shape: {batch['pixel_values'].shape}\")\n",
    "    print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"Sample questions: {batch['questions'][:2]}\")\n",
    "    print(f\"Sample answers: {batch['answers'][:2]}\")\n",
    "    print(f\"Question types: {batch['question_types'][:2]}\")\n",
    "    print(f\"IDs: {batch['ids'][:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BioMedBLIP From Checkpoint Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.737286Z",
     "iopub.status.busy": "2026-01-11T14:34:28.737022Z",
     "iopub.status.idle": "2026-01-11T14:34:28.741788Z",
     "shell.execute_reply": "2026-01-11T14:34:28.741113Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.737264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_model_with_checkpoint():\n",
    "    local_checkpoint_path = hf_hub_download(repo_id=MODEL_NAME, filename=CHECKPOINT_LOCATION)\n",
    "    print(f\"File {CHECKPOINT_LOCATION} downloaded to: {local_checkpoint_path}\")\n",
    "    \n",
    "    model = BlipForQuestionAnswering.from_pretrained(BASE_MODEL_NAME)\n",
    "    checkpoint = torch.load(local_checkpoint_path, map_location='cpu')\n",
    "    print(f\"Checkpoint Keys:\\n{checkpoint.keys()}\")\n",
    "    \n",
    "    model.load_state_dict(checkpoint[\"model\"], strict=False)\n",
    "\n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T19:49:33.678474Z",
     "iopub.status.busy": "2026-01-10T19:49:33.677695Z",
     "iopub.status.idle": "2026-01-10T19:50:06.970136Z",
     "shell.execute_reply": "2026-01-10T19:50:06.969303Z",
     "shell.execute_reply.started": "2026-01-10T19:49:33.678424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, chkpoint = load_model_with_checkpoint()\n",
    "\n",
    "# Check module names for LoRA\n",
    "with open(os.path.join(DATA_DIR, 'model_layers.txt'), 'w') as f:\n",
    "    for name, module in model.named_modules():\n",
    "        if \"attn\" in name.lower() or \"attention\" in name.lower():\n",
    "            f.write(str(name) + str(type(module)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.743125Z",
     "iopub.status.busy": "2026-01-11T14:34:28.742736Z",
     "iopub.status.idle": "2026-01-11T14:34:28.763289Z",
     "shell.execute_reply": "2026-01-11T14:34:28.762598Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.743088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(predictions, targets):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(targets, predictions) * 100,\n",
    "        'macro_f1': f1_score(targets, predictions, average='macro') * 100,\n",
    "        'weighted_f1': f1_score(targets, predictions, average='weighted') * 100,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_bleu_scores(predictions, references):\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    bleu_scores = {\n",
    "        'bleu1': [],\n",
    "        'bleu2': [],\n",
    "        'bleu3': [],\n",
    "        'bleu4': []\n",
    "    }\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Tokenize\n",
    "        pred_tokens = pred.lower().split()\n",
    "        ref_tokens = [ref.lower().split()]\n",
    "        \n",
    "        # Calculate BLEU scores up to 4-grams\n",
    "        try:\n",
    "            bleu1 = sentence_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "            bleu2 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "            bleu3 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "            bleu4 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "            \n",
    "            bleu_scores['bleu1'].append(bleu1)\n",
    "            bleu_scores['bleu2'].append(bleu2)\n",
    "            bleu_scores['bleu3'].append(bleu3)\n",
    "            bleu_scores['bleu4'].append(bleu4)\n",
    "        except:\n",
    "            bleu_scores['bleu1'].append(0.0)\n",
    "            bleu_scores['bleu2'].append(0.0)\n",
    "            bleu_scores['bleu3'].append(0.0)\n",
    "            bleu_scores['bleu4'].append(0.0)\n",
    "    \n",
    "    # Average scores\n",
    "    metrics = {\n",
    "        'bleu1': np.mean(bleu_scores['bleu1']) * 100,\n",
    "        'bleu2': np.mean(bleu_scores['bleu2']) * 100,\n",
    "        'bleu3': np.mean(bleu_scores['bleu3']) * 100,\n",
    "        'bleu4': np.mean(bleu_scores['bleu4']) * 100,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    rouge_scorer_helper = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = rouge_scorer_helper.score(ref, pred)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge1_scores) * 100,\n",
    "        'rouge2': np.mean(rouge2_scores) * 100,\n",
    "        'rougeL': np.mean(rougeL_scores) * 100,\n",
    "    }\n",
    "\n",
    "def calculate_meteor_score(predictions, references):\n",
    "    meteor_scores = []\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.lower().split()\n",
    "        ref_tokens = ref.lower().split()\n",
    "        \n",
    "        try:\n",
    "            score = meteor_score([ref_tokens], pred_tokens)\n",
    "            meteor_scores.append(score)\n",
    "        except:\n",
    "            meteor_scores.append(0.0)\n",
    "    \n",
    "    return {\n",
    "        'meteor': np.mean(meteor_scores) * 100\n",
    "    }\n",
    "\n",
    "def calculate_bertscore(predictions, references):\n",
    "    P, R, F1 = bert_score(\n",
    "        predictions, \n",
    "        references, \n",
    "        lang='en',\n",
    "        model_type='distilbert-base-uncased',\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'bertscore_precision': P.mean().item() * 100,\n",
    "        'bertscore_recall': R.mean().item() * 100,\n",
    "        'bertscore_f1': F1.mean().item() * 100,\n",
    "    }\n",
    "\n",
    "def calculate_exact_match(predictions, references):\n",
    "    exact_matches = sum(pred.lower().strip() == ref.lower().strip() \n",
    "                      for pred, ref in zip(predictions, references))\n",
    "    \n",
    "    return {\n",
    "        'exact_match': (exact_matches / len(predictions)) * 100\n",
    "    }\n",
    "\n",
    "def calculate_type_specific_metrics(pred_texts, target_texts, question_types):\n",
    "    type_metrics = defaultdict(lambda: {\n",
    "        'pred_texts': [],\n",
    "        'target_texts': []\n",
    "    })\n",
    "    \n",
    "    for pred_text, target_text, q_type in zip(\n",
    "        pred_texts, target_texts, question_types\n",
    "    ):\n",
    "        type_metrics[q_type]['pred_texts'].append(pred_text)\n",
    "        type_metrics[q_type]['target_texts'].append(target_text)\n",
    "    \n",
    "    results = {}\n",
    "    for q_type, data in type_metrics.items():\n",
    "        if len(data['pred_texts']) > 0:\n",
    "            results[q_type] = {\n",
    "                'accuracy': accuracy_score(data['target_texts'], data['pred_texts']) * 100,\n",
    "                'f1': f1_score(data['target_texts'], data['pred_texts'], average='macro', zero_division=0) * 100,\n",
    "                'exact_match': sum(p.lower() == t.lower() for p, t in zip(data['pred_texts'], data['target_texts'])) / len(data['pred_texts']) * 100,\n",
    "                'count': len(data['pred_texts'])\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_all_metrics(pred_texts, target_texts, question_types, model_state=\"Zero Shot\"):\n",
    "    print(\"CALCULATING ALL METRICS\")\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Classification Metrics (Accuracy, F1, Precision, Recall)\n",
    "    print(\"\\nCalculating Classification Metrics...\")\n",
    "    metrics['classification'] = calculate_classification_metrics(pred_texts, target_texts)\n",
    "    \n",
    "    # 2. BLEU Scores\n",
    "    print(\"Calculating BLEU (1-4) Scores...\")\n",
    "    metrics['bleu'] = calculate_bleu_scores(pred_texts, target_texts)\n",
    "\n",
    "    # 3. Rouge Scores\n",
    "    print(\"Calculating ROUGE Scores...\")\n",
    "    metrics['rouge'] = calculate_rouge_scores(pred_texts, target_texts)\n",
    "    \n",
    "    # 3. METEOR Score\n",
    "    print(\"Calculating METEOR Score...\")\n",
    "    metrics['meteor'] = calculate_meteor_score(pred_texts, target_texts)\n",
    "    \n",
    "    # 4. BERTScore\n",
    "    print(\"Calculating BERTScore... (Might take some time)\")\n",
    "    metrics['bertscore'] = calculate_bertscore(pred_texts, target_texts)\n",
    "    \n",
    "    # 5. Exact Match\n",
    "    print(\"Calculating Exact Match...\")\n",
    "    metrics['exact_match'] = calculate_exact_match(pred_texts, target_texts)\n",
    "    \n",
    "    # 6. Type-specific metrics\n",
    "    print(\"Calculating Type-Specific Metrics...\")\n",
    "    metrics['by_type'] = calculate_type_specific_metrics(\n",
    "        pred_texts, target_texts, question_types\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print_all_metrics(metrics, f'BioMedBLIP {model_state} Evaluation')\n",
    "    \n",
    "    # Save results\n",
    "    save_metrics(metrics, model_state=model_state)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    plot_type_specific_comparison(metrics['by_type'], FINAL_MODEL_PATH, f'BioMedBLIP {model_state}')\n",
    "    plot_ngram_analysis(metrics['bleu'], FINAL_MODEL_PATH, f'BioMedBLIP {model_state}')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_metrics(metrics, model_state=\"Zero Shot\"):\n",
    "    os.makedirs(FINAL_MODEL_PATH, exist_ok=True)\n",
    "    output_file = os.path.join(FINAL_MODEL_PATH, f'all_metrics_BioMedBLIP_{model_state}.json')\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nAll Metrics saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.764453Z",
     "iopub.status.busy": "2026-01-11T14:34:28.764200Z",
     "iopub.status.idle": "2026-01-11T14:34:28.782432Z",
     "shell.execute_reply": "2026-01-11T14:34:28.781876Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.764432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_all_metrics(model, processor, test_loader, model_state=\"Zero Shot\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_pred_texts = []\n",
    "    all_target_texts = []\n",
    "    all_question_types = []\n",
    "\n",
    "    progress_bar = tqdm(test_loader, desc=f\"{model_state} Evaluation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Generate answers\n",
    "            outputs = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=32,\n",
    "                num_beams=5,  # Beam search for better quality\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            # Decode predictions\n",
    "            pred_texts = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            all_pred_texts.extend(pred_texts)\n",
    "            all_target_texts.extend(batch['answers'])\n",
    "            all_question_types.extend(batch['question_types'])\n",
    "    return all_pred_texts, all_target_texts, all_question_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T17:34:46.958402Z",
     "iopub.status.busy": "2026-01-10T17:34:46.957578Z",
     "iopub.status.idle": "2026-01-10T17:36:35.202106Z",
     "shell.execute_reply": "2026-01-10T17:36:35.201501Z",
     "shell.execute_reply.started": "2026-01-10T17:34:46.958361Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_pred_texts, all_target_texts, all_question_types = calculate_all_metrics(model, processor, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-10T17:38:11.779641Z",
     "iopub.status.busy": "2026-01-10T17:38:11.779029Z",
     "iopub.status.idle": "2026-01-10T17:38:13.836191Z",
     "shell.execute_reply": "2026-01-10T17:38:13.835567Z",
     "shell.execute_reply.started": "2026-01-10T17:38:11.779611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "zero_shot_metrics = evaluate_all_metrics(all_pred_texts, all_target_texts, all_question_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.787107Z",
     "iopub.status.busy": "2026-01-11T14:34:28.786889Z",
     "iopub.status.idle": "2026-01-11T14:34:28.795147Z",
     "shell.execute_reply": "2026-01-11T14:34:28.794611Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.787087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_lora_model(model, lora_config=None):\n",
    "    if lora_config is None:\n",
    "        # Default LoRA configuration for BLIP\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\n",
    "                \"query\",\n",
    "                \"key\",\n",
    "                \"value\",\n",
    "                \"dense\",\n",
    "                \"qkv\",\n",
    "                \"projection\",\n",
    "            ],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "    \n",
    "    print(f\"LoRA Configuration:\")\n",
    "    print(f\"  Rank (r):            {lora_config.r}\")\n",
    "    print(f\"  Alpha:               {lora_config.lora_alpha}\")\n",
    "    print(f\"  Target modules:      {lora_config.target_modules}\")\n",
    "    print(f\"  Dropout:             {lora_config.lora_dropout}\")\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"\\nParameter Statistics:\")\n",
    "    print(f\"  Trainable params:    {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "    print(f\"  Total params:        {total_params:,}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.796356Z",
     "iopub.status.busy": "2026-01-11T14:34:28.796061Z",
     "iopub.status.idle": "2026-01-11T14:34:28.815035Z",
     "shell.execute_reply": "2026-01-11T14:34:28.814329Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.796324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_single_epoch(\n",
    "        model,\n",
    "        train_loader, \n",
    "        gradient_accumulation_steps,\n",
    "        optimizer, \n",
    "        epoch, \n",
    "        num_epochs,\n",
    "        max_grad_norm=1.0 \n",
    "    ):\n",
    "    # model.to(device)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad_token_id = model.config.text_config.pad_token_id\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"Starting Training Loop\")\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        labels_for_shift = labels.clone()\n",
    "        labels_for_shift[labels == -100] = pad_token_id\n",
    "\n",
    "        decoder_input_ids = torch.full_like(labels, pad_token_id)\n",
    "        decoder_input_ids[:, 1:] = labels_for_shift[:, :-1]\n",
    "\n",
    "        decoder_attention_mask = (decoder_input_ids != pad_token_id).long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item() * gradient_accumulation_steps:.4f}'})\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.816209Z",
     "iopub.status.busy": "2026-01-11T14:34:28.815975Z",
     "iopub.status.idle": "2026-01-11T14:34:28.832368Z",
     "shell.execute_reply": "2026-01-11T14:34:28.831846Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.816178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_(model, processor, val_loader):\n",
    "    # model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    exact_matches = 0\n",
    "    total = 0\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "    pad_token_id = model.config.text_config.pad_token_id\n",
    "\n",
    "    print(\"Starting Validating Loop\")\n",
    "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            labels_for_shift = labels.clone()\n",
    "            labels_for_shift[labels == -100] = pad_token_id\n",
    "    \n",
    "            decoder_input_ids = torch.full_like(labels, pad_token_id)\n",
    "            decoder_input_ids[:, 1:] = labels_for_shift[:, :-1]\n",
    "    \n",
    "            decoder_attention_mask = (decoder_input_ids != pad_token_id).long()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            \n",
    "            # Generate for exact match\n",
    "            generated = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=32\n",
    "            )\n",
    "            \n",
    "            pred_texts = processor.batch_decode(generated, skip_special_tokens=True)\n",
    "            predictions.extend(pred_texts)\n",
    "            references.extend(batch['answers'])\n",
    "\n",
    "            for pred, true_ans in zip(pred_texts, batch['answers']):\n",
    "                if pred.lower().strip() == true_ans.lower().strip():\n",
    "                    exact_matches += 1\n",
    "                total += 1\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    exact_match_acc = 100 * exact_matches / total\n",
    "\n",
    "    P, R, F1 = bert_score(\n",
    "        predictions, \n",
    "        references, \n",
    "        lang='en',\n",
    "        model_type='distilbert-base-uncased',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    P = P.mean().item()\n",
    "    R = R.mean().item()\n",
    "    F1 = F1.mean().item()\n",
    "    \n",
    "    return avg_loss, exact_match_acc, P, R, F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioMedBLIP Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:28.833410Z",
     "iopub.status.busy": "2026-01-11T14:34:28.833156Z",
     "iopub.status.idle": "2026-01-11T14:34:28.853401Z",
     "shell.execute_reply": "2026-01-11T14:34:28.852775Z",
     "shell.execute_reply.started": "2026-01-11T14:34:28.833378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BioMedBLIPTrainer:\n",
    "    def __init__(self, model, processor, train_dataset, val_dataset):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.results_dir = Path(FINAL_MODEL_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_exact_match': [],\n",
    "            'learning_rates': [],\n",
    "            'bertscore_precision': [],\n",
    "            'bertscore_recall': [],\n",
    "            'bertscore_f1': []\n",
    "        }\n",
    "\n",
    "    def train(self, num_epochs=20, batch_size=8, learning_rate=5e-5, \n",
    "              patience=5, gradient_accumulation_steps=4):\n",
    "        print(f\"Epochs:              {num_epochs}\")\n",
    "        print(f\"Batch size:          {batch_size}\")\n",
    "        print(f\"Learning rate:       {learning_rate}\")\n",
    "        print(f\"Gradient accum:      {gradient_accumulation_steps}\")\n",
    "        print(f\"Effective batch:     {batch_size * gradient_accumulation_steps}\")\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=num_epochs,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        best_exact_match = 0.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = train_single_epoch(\n",
    "                model=self.model,\n",
    "                train_loader=train_loader, \n",
    "                gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "                optimizer=optimizer,\n",
    "                epoch=epoch, \n",
    "                num_epochs=num_epochs,\n",
    "            )\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_exact_match, P, R, F1 = validate_(self.model, self.processor, val_loader)\n",
    "            \n",
    "            # Learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_exact_match'].append(val_exact_match)\n",
    "            self.history['learning_rates'].append(current_lr)\n",
    "            self.history['bertscore_precision'].append(P)\n",
    "            self.history['bertscore_recall'].append(R)\n",
    "            self.history['bertscore_f1'].append(F1)\n",
    "            \n",
    "            print(f\"Train Loss:      {train_loss:.4f}\")\n",
    "            print(f\"Val Loss:        {val_loss:.4f}\")\n",
    "            print(f\"Val Exact Match: {val_exact_match:.2f}%\")\n",
    "            print(f\"Val BERTScore Precision: {P:.2f}%\")\n",
    "            print(f\"Val BERTScore Recall: {R:.2f}%\")\n",
    "            print(f\"Val BERTScore F1: {F1:.2f}%\")\n",
    "            print(f\"Learning Rate:   {current_lr:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_exact_match > best_exact_match:\n",
    "                best_exact_match = val_exact_match\n",
    "                patience_counter = 0\n",
    "                self.save_checkpoint(epoch, val_exact_match, 'best_lora_model')\n",
    "                print(f\"New best model! Exact Match: {val_exact_match:.2f}%\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Plot training curves\n",
    "        self.plot_training_curves()\n",
    "        \n",
    "        print(\"FINE-TUNING COMPLETE\")\n",
    "        print(f\"Best Exact Match: {best_exact_match:.2f}%\")\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def save_checkpoint(self, epoch, exact_match, prefix):\n",
    "        # Save LoRA weights\n",
    "        self.model.save_pretrained(self.results_dir / f'{prefix}_epoch_{epoch}')\n",
    "        \n",
    "        # Save training history\n",
    "        with open(self.results_dir / f'{prefix}_history.json', 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        self.model = PeftModel.from_pretrained(self.model, checkpoint_path)\n",
    "        print(f\"Loaded model from {checkpoint_path}\")\n",
    "    \n",
    "    def plot_training_curves(self):\n",
    "        _, axes = plt.subplots(2, 3, figsize=(18, 5))\n",
    "        \n",
    "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
    "        \n",
    "        # Loss\n",
    "        axes[0][0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "        axes[0][0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "        axes[0][0].set_xlabel('Epoch')\n",
    "        axes[0][0].set_ylabel('Loss')\n",
    "        axes[0][0].set_title('Training and Validation Loss')\n",
    "        axes[0][0].legend()\n",
    "        axes[0][0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Exact Match\n",
    "        axes[0][1].plot(epochs, self.history['val_exact_match'], 'g-', linewidth=2)\n",
    "        axes[0][1].set_xlabel('Epoch')\n",
    "        axes[0][1].set_ylabel('Exact Match (%)')\n",
    "        axes[0][1].set_title('Validation Exact Match')\n",
    "        axes[0][1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning Rate\n",
    "        axes[0][2].plot(epochs, self.history['learning_rates'], 'purple', linewidth=2)\n",
    "        axes[0][2].set_xlabel('Epoch')\n",
    "        axes[0][2].set_ylabel('Learning Rate')\n",
    "        axes[0][2].set_title('Learning Rate Schedule')\n",
    "        axes[0][2].set_yscale('log')\n",
    "        axes[0][2].grid(True, alpha=0.3)\n",
    "\n",
    "        # BERTScores\n",
    "        axes[1][0].plot(epochs, self.history['bertscore_precision'], 'g-', linewidth=2)\n",
    "        axes[1][0].set_xlabel('Epoch')\n",
    "        axes[1][0].set_ylabel('BERTScore Precision (%)')\n",
    "        axes[1][0].set_title('Validation BERTScore Precision')\n",
    "        axes[1][0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1][1].plot(epochs, self.history['bertscore_recall'], 'g-', linewidth=2)\n",
    "        axes[1][1].set_xlabel('Epoch')\n",
    "        axes[1][1].set_ylabel('BERTScore Recall (%)')\n",
    "        axes[1][1].set_title('Validation BERTScore Recall')\n",
    "        axes[1][1].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1][2].plot(epochs, self.history['bertscore_f1'], 'g-', linewidth=2)\n",
    "        axes[1][2].set_xlabel('Epoch')\n",
    "        axes[1][2].set_ylabel('BERTScore F1 (%)')\n",
    "        axes[1][2].set_title('Validation BERTScore F1')\n",
    "        axes[1][2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.results_dir / 'training_curves.png', dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:53.882513Z",
     "iopub.status.busy": "2026-01-11T14:34:53.881880Z",
     "iopub.status.idle": "2026-01-11T14:34:53.898702Z",
     "shell.execute_reply": "2026-01-11T14:34:53.897897Z",
     "shell.execute_reply.started": "2026-01-11T14:34:53.882482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QuickLoRATuner:\n",
    "    def __init__(self, processor, train_dataset, val_dataset):\n",
    "        self.processor = processor\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.results_dir = Path(TUNING_RESULTS_PATH)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def test_config(self, config):\n",
    "        try:\n",
    "            num_epochs = 10\n",
    "            history = {\n",
    "                'train_loss': [],\n",
    "                'val_loss': [],\n",
    "                'val_exact_match': [],\n",
    "                'learning_rates': [],\n",
    "                'bertscore_precision': [],\n",
    "                'bertscore_recall': [],\n",
    "                'bertscore_f1': []\n",
    "            }\n",
    "            \n",
    "            # 2. Load fresh model for this trial\n",
    "            model, _ = load_model_with_checkpoint()\n",
    "\n",
    "            # 3. Apply LoRA with sampled config\n",
    "            lora_config = LoraConfig(\n",
    "                r=config['r'],\n",
    "                lora_alpha=config['alpha'] * 2.0,\n",
    "                target_modules=[\n",
    "                    \"query\",\n",
    "                    \"key\",\n",
    "                    \"value\",\n",
    "                    \"dense\",\n",
    "                    \"qkv\",\n",
    "                    \"projection\",\n",
    "                ],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\"\n",
    "            )\n",
    "\n",
    "            model = setup_lora_model(model, lora_config).to(device)\n",
    "\n",
    "            # 4. Create Data Loaders\n",
    "            train_loader = DataLoader(\n",
    "                self.train_dataset,\n",
    "                batch_size=config['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=0,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                self.val_dataset,\n",
    "                batch_size=config['batch_size'],\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "\n",
    "            # 5. Setup optimizer\n",
    "            optimizer = AdamW(\n",
    "                model.parameters(),\n",
    "                lr=config['lr'],\n",
    "                weight_decay=0.01\n",
    "            )\n",
    "\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=num_epochs,\n",
    "                eta_min=1e-6\n",
    "            )\n",
    "\n",
    "            best_exact_match = 0.0\n",
    "            patience = 3\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "                \n",
    "                # Train\n",
    "                train_loss = train_single_epoch(\n",
    "                    model=model,\n",
    "                    train_loader=train_loader, \n",
    "                    gradient_accumulation_steps=config['grad_accum'],\n",
    "                    optimizer=optimizer, \n",
    "                    epoch=epoch, \n",
    "                    num_epochs=num_epochs,\n",
    "                )\n",
    "                \n",
    "                # Validate\n",
    "                val_loss, val_exact_match, P, R, F1 = validate_(model, self.processor, val_loader)\n",
    "                \n",
    "                # Learning rate\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "                # Update history\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['val_exact_match'].append(val_exact_match)\n",
    "                history['learning_rates'].append(current_lr)\n",
    "                history['bertscore_precision'].append(P)\n",
    "                history['bertscore_recall'].append(R)\n",
    "                history['bertscore_f1'].append(F1)\n",
    "                \n",
    "                print(f\"Train Loss:      {train_loss:.4f}\")\n",
    "                print(f\"Val Loss:        {val_loss:.4f}\")\n",
    "                print(f\"Val Exact Match: {val_exact_match:.2f}%\")\n",
    "                print(f\"Val BERTScore Precision: {P:.2f}%\")\n",
    "                print(f\"Val BERTScore Recall: {R:.2f}%\")\n",
    "                print(f\"Val BERTScore F1: {F1:.2f}%\")\n",
    "                print(f\"Learning Rate:   {current_lr:.6f}\")\n",
    "                \n",
    "                if val_exact_match > best_exact_match:\n",
    "                    best_exact_match = val_exact_match\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "                \n",
    "                scheduler.step()\n",
    "            \n",
    "            print(f\"CONFIG {config['name'].upper()} TESTING COMPLETE\")\n",
    "            print(f\"Best Exact Match: {best_exact_match:.2f}%\")\n",
    "            \n",
    "            # Clean up\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            return best_exact_match\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed with error: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def run(self):\n",
    "        # Predefined configurations based on LoRA practices\n",
    "        configs = [\n",
    "            # Conservative\n",
    "            {'r': 8, 'alpha': 16, 'lr': 5e-5, 'batch_size': 4, 'grad_accum': 8,  'name': 'conservative'},\n",
    "            \n",
    "            # Balanced\n",
    "            {'r': 16, 'alpha': 32, 'lr': 5e-5, 'batch_size': 4, 'grad_accum': 8, 'name': 'balanced'},\n",
    "            \n",
    "            # Higher capacity\n",
    "            {'r': 32, 'alpha': 64, 'lr': 3e-5, 'batch_size': 4, 'grad_accum': 8, 'name': 'high_capacity'},\n",
    "\n",
    "            # Balanced with Bigger Batch\n",
    "            {'r': 16, 'alpha': 32, 'lr': 5e-5, 'batch_size': 8, 'grad_accum': 4, 'name': 'balanced_with_bigger_batch'},\n",
    "            \n",
    "            # Faster learning\n",
    "            {'r': 16, 'alpha': 32, 'lr': 1e-4, 'batch_size': 8, 'grad_accum': 4, 'name': 'fast_learning'},\n",
    "            \n",
    "            # Higher rank with higher learning\n",
    "            {'r': 32, 'alpha': 64, 'lr': 1e-4, 'batch_size': 8, 'grad_accum': 4, 'name': 'high_rank_fast_learning'},\n",
    "        ]\n",
    "        \n",
    "        print(\"QUICK LORA TUNING (5 CONFIGS)\")\n",
    "        \n",
    "        results = []\n",
    "        best_score = 0\n",
    "        best_config = None\n",
    "        \n",
    "        for i, config in enumerate(configs):\n",
    "            print(f\"\\n[{i+1}/{len(configs)}] Testing: {config['name']}\")\n",
    "            print(json.dumps(config, indent=2))\n",
    "            \n",
    "            score = self.test_config(config)\n",
    "            \n",
    "            results.append({\n",
    "                'config': config,\n",
    "                'exact_match': f\"{score:.2f}\"\n",
    "            })\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_config = config\n",
    "            \n",
    "            print(f\"  Result: {score:.2f}% exact match\")\n",
    "        \n",
    "        # Save results\n",
    "        with open(self.results_dir / f'short_history.json', 'w') as f:\n",
    "            json.dump({\n",
    "                'best_config': best_config,\n",
    "                'best_score': best_score,\n",
    "                'all_results': results\n",
    "            }, f, indent=2)\n",
    "        \n",
    "        print(\"QUICK TUNING COMPLETE\")\n",
    "        print(f\"Best: {best_config['name']} with {best_score:.2f}%\")\n",
    "        print(f\"Config: r={best_config['r']}, alpha={best_config['alpha']}, lr={best_config['lr']}\")\n",
    "        \n",
    "        return best_config, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:34:54.493906Z",
     "iopub.status.busy": "2026-01-11T14:34:54.493588Z",
     "iopub.status.idle": "2026-01-11T14:34:58.601604Z",
     "shell.execute_reply": "2026-01-11T14:34:58.600978Z",
     "shell.execute_reply.started": "2026-01-11T14:34:54.493872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset_blip = SLAKEDatasetBioMedBLIP(dataset['train'], processor)\n",
    "val_dataset_blip = SLAKEDatasetBioMedBLIP(dataset['validation'], processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:35:05.194192Z",
     "iopub.status.busy": "2026-01-11T14:35:05.193569Z",
     "iopub.status.idle": "2026-01-11T14:35:05.197916Z",
     "shell.execute_reply": "2026-01-11T14:35:05.197297Z",
     "shell.execute_reply.started": "2026-01-11T14:35:05.194164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tuner = QuickLoRATuner(\n",
    "    processor=processor,\n",
    "    train_dataset=train_dataset_blip,\n",
    "    val_dataset=val_dataset_blip\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T14:35:07.035164Z",
     "iopub.status.busy": "2026-01-11T14:35:07.034864Z",
     "iopub.status.idle": "2026-01-11T19:01:52.712867Z",
     "shell.execute_reply": "2026-01-11T19:01:52.711858Z",
     "shell.execute_reply.started": "2026-01-11T14:35:07.035132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_params, best_score = tuner.run()\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T19:02:56.016193Z",
     "iopub.status.busy": "2026-01-11T19:02:56.015889Z",
     "iopub.status.idle": "2026-01-11T19:02:56.022462Z",
     "shell.execute_reply": "2026-01-11T19:02:56.021908Z",
     "shell.execute_reply.started": "2026-01-11T19:02:56.016163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load best params from short_history.json if needed\n",
    "# with open(os.path.join(TUNING_RESULTS_PATH, 'short_history.json'), 'r') as f:\n",
    "#     history = json.load(f)\n",
    "# best_params = history['best_config']\n",
    "\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T19:05:36.710539Z",
     "iopub.status.busy": "2026-01-11T19:05:36.709794Z",
     "iopub.status.idle": "2026-01-11T19:05:41.562599Z",
     "shell.execute_reply": "2026-01-11T19:05:41.562040Z",
     "shell.execute_reply.started": "2026-01-11T19:05:36.710509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, _ = load_model_with_checkpoint()\n",
    "processor = BlipProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-base\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=best_params['r'],\n",
    "    lora_alpha=best_params['alpha'],\n",
    "    target_modules=[\n",
    "        \"query\",\n",
    "        \"key\",\n",
    "        \"value\",\n",
    "        \"dense\",\n",
    "        \"qkv\",\n",
    "        \"projection\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = setup_lora_model(model, lora_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T19:05:52.520032Z",
     "iopub.status.busy": "2026-01-11T19:05:52.519480Z",
     "iopub.status.idle": "2026-01-11T19:05:56.575153Z",
     "shell.execute_reply": "2026-01-11T19:05:56.574549Z",
     "shell.execute_reply.started": "2026-01-11T19:05:52.520003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset_blip = SLAKEDatasetBioMedBLIP(dataset['train'], processor)\n",
    "val_dataset_blip = SLAKEDatasetBioMedBLIP(dataset['validation'], processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BioMedBLIPTrainer(model, processor, train_dataset_blip, val_dataset_blip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T19:06:00.819882Z",
     "iopub.status.busy": "2026-01-11T19:06:00.819578Z",
     "iopub.status.idle": "2026-01-11T22:33:09.073758Z",
     "shell.execute_reply": "2026-01-11T22:33:09.072223Z",
     "shell.execute_reply.started": "2026-01-11T19:06:00.819857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = trainer.train(\n",
    "    num_epochs=20, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    learning_rate=best_params['lr'], \n",
    "    gradient_accumulation_steps=best_params['grad_accum']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "model, _ = load_model_with_checkpoint()\n",
    "processor = BlipProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-vqa-base\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# Replace with the path to the best LoRA model. In my case, it's epoch 9.\n",
    "model = PeftModel.from_pretrained(model, os.path.join(FINAL_MODEL_PATH, 'best_lora_model_epoch_9')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset_blip = SLAKEDatasetBioMedBLIP(dataset['test'], processor)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_blip,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_pred_texts, all_target_texts, all_question_types = calculate_all_metrics(model, processor, test_loader, model_state=\"Final Finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_finetune_metrics = evaluate_all_metrics(all_pred_texts, all_target_texts, all_question_types, model_state=\"Final Finetuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [item['question'] for item in test_dataset_blip.data if item is not None]\n",
    "images = [item['img_name'] for item in test_dataset_blip.data if item is not None]\n",
    "\n",
    "# Print a few to verify\n",
    "print(\"Sample Questions:\")\n",
    "for i in range(min(5, len(questions))):\n",
    "    print(f\"  Question: {questions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_vqa_case(case):\n",
    "    path_img = os.path.join(IMAGE_PATH, case['image'])\n",
    "    display(Image(filename=path_img, width=400))\n",
    "    \n",
    "    # Print text on subsequent lines\n",
    "    print(f\"Question: {case['question']}\")\n",
    "    print(f\"Predicted Answer: {case['prediction']}\")\n",
    "    print(f\"Actual Answer: {case['ground_truth']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all wrong predictions\n",
    "wrong_predictions = []\n",
    "for i in range(len(all_pred_texts)):\n",
    "    if all_pred_texts[i].lower().strip() != all_target_texts[i].lower().strip():\n",
    "        wrong_predictions.append({\n",
    "            'question': questions[i],\n",
    "            'ground_truth': all_target_texts[i],\n",
    "            'prediction': all_pred_texts[i],\n",
    "            'image': images[i],\n",
    "            'question_type': all_question_types[i]\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directional Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectionalErrorAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.directional_terms = {\n",
    "            'lateral': ['left', 'right'],\n",
    "            'vertical': ['upper', 'lower', 'top', 'bottom'],\n",
    "            'medial': ['central', 'center', 'middle']\n",
    "        }\n",
    "    \n",
    "    def extract_directional_info(self, text):\n",
    "        text_lower = text.lower().strip()\n",
    "        \n",
    "        directions = {\n",
    "            'lateral': [],\n",
    "            'vertical': [],\n",
    "            'medial': []\n",
    "        }\n",
    "        \n",
    "        for direction_type, terms in self.directional_terms.items():\n",
    "            for term in terms:\n",
    "                if re.search(r'\\b' + term + r'\\b', text_lower):\n",
    "                    directions[direction_type].append(term)\n",
    "        \n",
    "        return directions\n",
    "    \n",
    "    def compare_directions(self, gt_directions, pred_directions):\n",
    "        for direction_type in ['lateral', 'vertical', 'medial']:\n",
    "            gt_terms_array = gt_directions[direction_type]\n",
    "            pred_terms_array = pred_directions[direction_type]\n",
    "            gt_terms = set(gt_directions[direction_type])\n",
    "            pred_terms = set(pred_directions[direction_type])\n",
    "\n",
    "            if direction_type == 'medial':\n",
    "                if not gt_terms and pred_terms:\n",
    "                    return 'extra_medial_direction'\n",
    "                elif gt_terms and not pred_terms:\n",
    "                    return 'missing_medial_direction'\n",
    "                \n",
    "            if direction_type == 'lateral':\n",
    "                if len(gt_terms_array) < len(pred_terms_array):\n",
    "                    return 'extra_lateral_direction'\n",
    "                elif len(gt_terms_array) > len(pred_terms_array):\n",
    "                    return 'missing_lateral_direction'\n",
    "                else:\n",
    "                    for i in range(len(gt_terms_array)):\n",
    "                        if gt_terms_array[i] != pred_terms_array[i]:\n",
    "                            return 'left_right_confusion'\n",
    "                        \n",
    "            if direction_type == 'vertical':\n",
    "                if len(gt_terms_array) < len(pred_terms_array):\n",
    "                    return 'extra_vertical_direction'\n",
    "                elif len(gt_terms_array) > len(pred_terms_array):\n",
    "                    return 'missing_vertical_direction'\n",
    "                else:\n",
    "                    for i in range(len(gt_terms_array)):\n",
    "                        if gt_terms_array[i] != pred_terms_array[i]:\n",
    "                            return 'upper_lower_confusion'\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def has_directional_info(self, text):\n",
    "        directions = self.extract_directional_info(text)\n",
    "        return any(len(terms) > 0 for terms in directions.values())\n",
    "    \n",
    "    def analyze_predictions(self, predictions, ground_truths, questions, images):\n",
    "        results = {\n",
    "            'errors': {\n",
    "                'left_right_confusion': [],\n",
    "                'upper_lower_confusion': [],\n",
    "                'partial_match': [],\n",
    "                'extra_medial_direction': [],\n",
    "                'missing_medial_direction': [],\n",
    "                'missing_lateral_direction': [],\n",
    "                'extra_lateral_direction': [],\n",
    "                'missing_vertical_direction': [],\n",
    "                'extra_vertical_direction': [],\n",
    "                'missing_direction': [],\n",
    "                'extra_direction': []\n",
    "            },\n",
    "            'correct': [],\n",
    "            'non_directional': []\n",
    "        }\n",
    "        \n",
    "        for pred, gt, question, img in zip(predictions, ground_truths, questions, images):\n",
    "            pred_lower = pred.lower().strip()\n",
    "            gt_lower = gt.lower().strip()\n",
    "            \n",
    "            has_gt_direction = self.has_directional_info(gt)\n",
    "            has_pred_direction = self.has_directional_info(pred)\n",
    "            \n",
    "            if not has_gt_direction and not has_pred_direction:\n",
    "                results['non_directional'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred,\n",
    "                    'correct': pred_lower == gt_lower\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            if not has_gt_direction and has_pred_direction:\n",
    "                results['errors']['extra_direction'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            if pred_lower == gt_lower:\n",
    "                results['correct'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            gt_directions = self.extract_directional_info(gt)\n",
    "            pred_directions = self.extract_directional_info(pred)\n",
    "            \n",
    "            if not has_pred_direction:\n",
    "                results['errors']['missing_direction'].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred,\n",
    "                    'gt_directions': gt_directions\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            error_type = self.compare_directions(gt_directions, pred_directions)\n",
    "            \n",
    "            if error_type:\n",
    "                results['errors'][error_type].append({\n",
    "                    'image': img,\n",
    "                    'question': question,\n",
    "                    'ground_truth': gt,\n",
    "                    'prediction': pred,\n",
    "                    'gt_directions': gt_directions,\n",
    "                    'pred_directions': pred_directions\n",
    "                })\n",
    "            else:\n",
    "                if gt_directions != pred_directions:\n",
    "                    results['errors']['partial_match'].append({\n",
    "                        'image': img,\n",
    "                        'question': question,\n",
    "                        'ground_truth': gt,\n",
    "                        'prediction': pred,\n",
    "                        'gt_directions': gt_directions,\n",
    "                        'pred_directions': pred_directions\n",
    "                    })\n",
    "                else:\n",
    "                    results['correct'].append({\n",
    "                        'image': img,\n",
    "                        'question': question,\n",
    "                        'ground_truth': gt,\n",
    "                        'prediction': pred,\n",
    "                        'note': 'Directionally correct but overall wrong'\n",
    "                    })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_statistics(self, results):\n",
    "        total_directional = (\n",
    "            len(results['correct']) + \n",
    "            sum(len(errors) for errors in results['errors'].values())\n",
    "        )\n",
    "        \n",
    "        if total_directional == 0:\n",
    "            return None\n",
    "        \n",
    "        stats = {\n",
    "            'total_directional_questions': total_directional,\n",
    "            'correct': len(results['correct']),\n",
    "            'directional_accuracy': 100 * len(results['correct']) / total_directional,\n",
    "            'error_breakdown': {}\n",
    "        }\n",
    "        \n",
    "        for error_type, errors in results['errors'].items():\n",
    "            count = len(errors)\n",
    "            stats['error_breakdown'][error_type] = {\n",
    "                'count': count,\n",
    "                'percentage': 100 * count / total_directional\n",
    "            }\n",
    "        \n",
    "        total_errors = sum(len(errors) for errors in results['errors'].values())\n",
    "        stats['total_errors'] = total_errors\n",
    "        stats['error_rate'] = 100 * total_errors / total_directional\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def print_analysis(self, results, stats):\n",
    "        print(\"DIRECTIONAL ERROR ANALYSIS\")\n",
    "        \n",
    "        if stats is None:\n",
    "            print(\"No directional questions found in dataset.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nTotal questions with directional info: {stats['total_directional_questions']}\")\n",
    "        print(f\"Correct: {stats['correct']} ({stats['directional_accuracy']:.2f}%)\")\n",
    "        print(f\"Errors: {stats['total_errors']} ({stats['error_rate']:.2f}%)\")\n",
    "        \n",
    "        print(\"Error Breakdown:\")\n",
    "        \n",
    "        sorted_errors = sorted(\n",
    "            stats['error_breakdown'].items(),\n",
    "            key=lambda x: x[1]['count'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        for error_type, error_stats in sorted_errors:\n",
    "            if error_stats['count'] > 0:\n",
    "                error_name = error_type.replace('_', ' ').title()\n",
    "                print(f\"  {error_name:30s}: {error_stats['count']:3d} ({error_stats['percentage']:5.2f}%)\")\n",
    "        \n",
    "        for error_type, errors in results['errors'].items():\n",
    "            if len(errors) > 0:\n",
    "                print(f\"\\n {error_type.replace('_', ' ').title()} Examples:\")\n",
    "                for i, example in enumerate(errors[:3]):\n",
    "                    print(f\"\\n  Example {i+1}:\")\n",
    "                    print(f\"    Question:     {example['question']}\")\n",
    "                    print(f\"    Ground Truth: {example['ground_truth']}\")\n",
    "                    print(f\"    Prediction:   {example['prediction']}\")\n",
    "                    \n",
    "                    if 'gt_directions' in example and 'pred_directions' in example:\n",
    "                        print(f\"    GT Directions: {example['gt_directions']}\")\n",
    "                        print(f\"    Pred Directions: {example['pred_directions']}\")\n",
    "    \n",
    "    def save_detailed_report(self, results, stats, model_name, output_dir=FINAL_MODEL_PATH):\n",
    "        from pathlib import Path\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        report = {\n",
    "            'model': model_name,\n",
    "            'statistics': stats,\n",
    "            'examples': {}\n",
    "        }\n",
    "        \n",
    "        # Add examples for each error type\n",
    "        for error_type, errors in results['errors'].items():\n",
    "            report['examples'][error_type] = errors[:10]  # Save first 10 of each type\n",
    "        \n",
    "        # Add correct examples\n",
    "        report['examples']['correct'] = results['correct'][:10]\n",
    "        \n",
    "        output_file = output_dir / f'{model_name.lower()}_directional_analysis.json'\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(f\"Detailed report saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_biomedblip_directional_errors(predictions, ground_truths, questions, images):\n",
    "    analyzer = DirectionalErrorAnalyzer()\n",
    "    \n",
    "    # Run analysis\n",
    "    results = analyzer.analyze_predictions(predictions, ground_truths, questions, images)\n",
    "    stats = analyzer.calculate_statistics(results)\n",
    "    \n",
    "    # Print results\n",
    "    analyzer.print_analysis(results, stats)\n",
    "    \n",
    "    # Save detailed report\n",
    "    analyzer.save_detailed_report(results, stats, 'BioMedBLIP')\n",
    "    \n",
    "    return results, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, stats = analyze_biomedblip_directional_errors(\n",
    "    all_pred_texts,\n",
    "    all_target_texts,\n",
    "    questions,\n",
    "    images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case study visualization\n",
    "case = results['errors']['left_right_confusion'][3]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = results['errors']['left_right_confusion'][7]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = results['errors']['upper_lower_confusion'][3]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = results['errors']['upper_lower_confusion'][4]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar but not exact answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_modality_confusion = 0\n",
    "# Check answers where \"ct scan\", \"x-ray\", or \"mri\" are in ground truth but predicted wrong\n",
    "for i in range(len(wrong_predictions)):\n",
    "    gt_lower = wrong_predictions[i]['ground_truth'].lower()\n",
    "    if any(modality == gt_lower for modality in ['ct', 'x-ray', 'mri', 't2']):\n",
    "        # Remove spaces from prediction for clarity\n",
    "        clean_prediction = wrong_predictions[i]['prediction'].replace(' ', '')\n",
    "        if clean_prediction == gt_lower:\n",
    "            print(f\"\\nModality Confusion Case: {i}\")\n",
    "            print(f\"  Question: {wrong_predictions[i]['question']}\")\n",
    "            print(f\"  Predicted Answer: {wrong_predictions[i]['prediction']}\")\n",
    "            print(f\"  Actual Answer: {wrong_predictions[i]['ground_truth']}\")\n",
    "\n",
    "            total_modality_confusion += 1\n",
    "\n",
    "print(f\"\\nTotal Modality Confusion Cases: {total_modality_confusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = wrong_predictions[16]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = wrong_predictions[127]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, replace all \"x - ray\" with \"x-ray\" in predictions and re-evaluate\n",
    "corrected_predictions = []\n",
    "for pred in all_pred_texts:\n",
    "    corrected_pred = re.sub(r'x\\s*-\\s*ray', 'x-ray', pred, flags=re.IGNORECASE)\n",
    "    corrected_predictions.append(corrected_pred)\n",
    "\n",
    "final_finetune_metrics_corrected = evaluate_all_metrics(corrected_predictions, all_target_texts, all_question_types, model_state=\"Final Finetuning - Corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the wrong predictions where question is open-ended\n",
    "open_ended_wrongs = []\n",
    "for i in range(len(wrong_predictions)):\n",
    "    if wrong_predictions[i]['question_type'] == 'OPEN':\n",
    "        open_ended_wrongs.append(wrong_predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check wrong open-ended cases with more than 3 words in ground truth answer\n",
    "long_open_ended_wrongs = []\n",
    "for i in range(len(open_ended_wrongs)):\n",
    "    gt_word_count = len(open_ended_wrongs[i]['ground_truth'].split())\n",
    "    if gt_word_count > 3:\n",
    "        long_open_ended_wrongs.append(open_ended_wrongs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = long_open_ended_wrongs[4]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = long_open_ended_wrongs[-4]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = long_open_ended_wrongs[-3]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = long_open_ended_wrongs[-6]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = long_open_ended_wrongs[12]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = long_open_ended_wrongs[14]\n",
    "show_vqa_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "aml_aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
