{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport zipfile\nimport nltk\nimport json\nimport optuna\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom datasets import load_dataset\nfrom pprint import pprint\nfrom PIL import Image\nfrom pathlib import Path\n\nimport torchvision.transforms as transforms\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom huggingface_hub import hf_hub_download\n\nnltk.download('punkt_tab', quiet=True)\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:06.211079Z","iopub.execute_input":"2025-12-16T19:15:06.211877Z","iopub.status.idle":"2025-12-16T19:15:18.162234Z","shell.execute_reply.started":"2025-12-16T19:15:06.211838Z","shell.execute_reply":"2025-12-16T19:15:18.161421Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"## Constants","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (224, 224)\nVOCAB_SIZE = 5000\nBATCH_SIZE = 32\nMAX_NODES_PER_QUESTION = 10\n\n# Directory Information\nDATA_DIR = \"/kaggle/working/\"\nDATASET_PATH = os.path.join(DATA_DIR, 'dataset/')\nIMAGE_PATH = os.path.join(DATA_DIR, 'imgs/')\nVOCABS_PATH = os.path.join(DATA_DIR, 'vocabs/')\nHYPERPARAMETERS_RESULT_PATH = os.path.join(DATA_DIR, 'tuning/')\n\n# Huggingface Repository Information\nrepo_id = \"BoKelvin/SLAKE\"\nrepo_type = \"dataset\"\nimg_file = \"imgs.zip\"\n\n# Seeding\nGLOBAL_SEED = 42\n\n# Device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:20.989160Z","iopub.execute_input":"2025-12-16T19:15:20.989665Z","iopub.status.idle":"2025-12-16T19:15:21.057685Z","shell.execute_reply.started":"2025-12-16T19:15:20.989639Z","shell.execute_reply":"2025-12-16T19:15:21.056604Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Dataset Setup\n\n### Dataset Download","metadata":{}},{"cell_type":"code","source":"# Utility function for downloading and extracting ZIP file\ndef download_and_store_ZIP(filename, save_dir):\n    print(f\"Fetching file {filename} from {repo_id} repo\")\n\n    try:\n        # Caches the file locally and returns the path to the cached file\n        cached_zip_path = hf_hub_download(\n          repo_id=repo_id,\n          filename=filename,\n          repo_type=repo_type\n        )\n        print(f\"{filename} download complete. Cached at: {cached_zip_path}\")\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        # Extract the contents\n        print(f\"Extracting to {save_dir}...\")\n        with zipfile.ZipFile(cached_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(save_dir)\n\n        print(\"Extraction complete.\")\n        print(f\"{filename} files are located in: {os.path.abspath(save_dir)}\")\n    except Exception as e:\n        print(f\"Failed to download or extract {filename}: {e}\")\n\n# Scoping to English only\ndef filter_language(original):\n    return original.filter(lambda data: data['q_lang'] == 'en')\n\n# Download and store the dataset\ndef download_and_store_english_dataset():\n    print(f\"Downloading dataset from {repo_id} repo\")\n\n    # Load from Hugging Face\n    original = load_dataset(repo_id)\n\n    # Scope to English Only\n    original = filter_language(original)\n\n    # Show the dataset formatting\n    pprint(original)\n\n    # Save the original dataset\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n\n    if not os.path.exists(DATASET_PATH):\n        os.makedirs(DATASET_PATH)\n\n    original.save_to_disk(DATASET_PATH)\n    return original\n\n# Download and store the image files\ndef download_and_store_image():\n    download_and_store_ZIP(img_file, DATA_DIR)\n\n# Download necessary files\ndef download_and_store_slake():\n    dataset = download_and_store_english_dataset()\n    download_and_store_image()\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:24.579611Z","iopub.execute_input":"2025-12-16T19:15:24.580416Z","iopub.status.idle":"2025-12-16T19:15:24.588436Z","shell.execute_reply.started":"2025-12-16T19:15:24.580383Z","shell.execute_reply":"2025-12-16T19:15:24.587587Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Vocabulary Builder","metadata":{}},{"cell_type":"code","source":"class VocabularyBuilder:\n    def __init__(self, min_freq=1):\n        self.min_freq = min_freq\n        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n        self.stoi = {v: k for k, v in self.itos.items()}\n\n    def tokenize(self, text):\n        return nltk.word_tokenize(text.lower())\n    \n    def __len__(self):\n        return len(self.stoi)\n    \n    def build_word_vocabs(self, sentences):\n        counter = Counter()\n        start_index = len(self.stoi)\n\n        # 1. Count frequencies of all tokens in the tokenized sentences\n        for sentence in sentences:\n            tokens = self.tokenize(sentence)\n            counter.update(tokens)\n\n        # 2. Add words that meet the frequency threshold\n        for word, count in counter.items():\n            if count >= self.min_freq and word not in self.stoi:\n                self.stoi[word] = start_index\n                self.itos[start_index] = word\n                start_index += 1\n\n        print(f\"Vocabulary Built. Vocabulary Size: {len(self.stoi)}\")\n\n    def numericalize(self, text):\n        tokens = self.tokenize(text)\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n            for token in tokens\n        ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:28.218662Z","iopub.execute_input":"2025-12-16T19:15:28.219523Z","iopub.status.idle":"2025-12-16T19:15:28.226539Z","shell.execute_reply.started":"2025-12-16T19:15:28.219492Z","shell.execute_reply":"2025-12-16T19:15:28.225680Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Build vocabularies for questions and answers\ndef build_vocabs(dataset):\n    questions = [item['question'] for item in dataset]\n    answers = [item['answer'] for item in dataset]\n\n    # Question Vocabulary\n    questvocab_builder = VocabularyBuilder(min_freq=1)\n    questvocab_builder.build_word_vocabs(questions)\n    \n    # Answer Vocabulary\n    ansvocab_builder = VocabularyBuilder(min_freq=1)\n\n    # Use a dummy tokenizer that just returns the whole lowercased string as one token\n    identity_tokenizer = lambda x: [x.lower().strip()]\n    ansvocab_builder.tokenize = identity_tokenizer\n\n    ansvocab_builder.build_word_vocabs(answers)\n\n    return questvocab_builder, ansvocab_builder\n\n# Save vocabularies to JSON files\ndef save_vocabs(quest_vocab, ans_vocab):\n    if not os.path.exists(VOCABS_PATH):\n        os.makedirs(VOCABS_PATH)\n\n    # Save Question Vocabulary\n    with open(os.path.join(VOCABS_PATH, 'question_vocab.json'), 'w') as f:\n        json.dump({'stoi': quest_vocab.stoi, 'itos': quest_vocab.itos}, f)\n\n    # Save Answer Vocabulary\n    with open(os.path.join(VOCABS_PATH, 'answer_vocab.json'), 'w') as f:\n        json.dump({'stoi': ans_vocab.stoi, 'itos': ans_vocab.itos}, f)\n\n    print(\"Vocabularies saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:30.958298Z","iopub.execute_input":"2025-12-16T19:15:30.958637Z","iopub.status.idle":"2025-12-16T19:15:30.965870Z","shell.execute_reply.started":"2025-12-16T19:15:30.958607Z","shell.execute_reply":"2025-12-16T19:15:30.964956Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Dataset Class","metadata":{}},{"cell_type":"code","source":"class SlakeDataset(Dataset):\n    def __init__(self, dataset, question_vocab, answer_vocab, transform=None, cache_images=True):\n        self.data = dataset\n        self.question_vocab = question_vocab\n        self.answer_vocab = answer_vocab\n        self.transform = transform\n        self.cache_images = cache_images\n\n        # Caching\n        self.image_cache = {}\n        if self.cache_images:\n            print(f\"Caching images for into RAM...\")\n            # Get unique image names to avoid duplicate loading\n            unique_imgs = set(item['img_name'] for item in self.data)\n            \n            for img_name in unique_imgs:\n                path = os.path.join(IMAGE_PATH, img_name)\n                # Load and convert to RGB\n                img = Image.open(path).convert('RGB')\n                \n                # Resize immediately to save RAM and CPU later\n                img = img.resize((224, 224)) \n                \n                self.image_cache[img_name] = img\n            print(f\"Cached {len(self.image_cache)} images.\")\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        # 1. Image Processing\n        image_path = item['img_name']\n\n        if self.cache_images:\n            # Get from RAM\n            image = self.image_cache[image_path]\n        else:\n            # Load from Disk and Resize\n            img_path = os.path.join(IMAGE_PATH, image_path)\n            image = Image.open(img_path).convert('RGB')\n            image = image.resize((224, 224))\n\n        if self.transform:\n            image = self.transform(image)\n\n        # 2. Question Processing\n        question = item['question']\n        question_indices = self.question_vocab.numericalize(question)\n\n        # 3. Answer Processing\n        answer = str(item.get('answer', '')) # Answer may be missing in test set\n        answer_index = self.answer_vocab.numericalize(answer)\n\n        return {\n            'image': image,\n            'question' : torch.tensor(question_indices),\n            'answer' : torch.tensor(answer_index, dtype=torch.long),\n            # Add original items for reference\n            'original_question': question,\n            'original_answer': answer,\n            # Add ID for tracking\n            'id': item['qid']\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:36.575550Z","iopub.execute_input":"2025-12-16T19:15:36.575866Z","iopub.status.idle":"2025-12-16T19:15:36.585288Z","shell.execute_reply.started":"2025-12-16T19:15:36.575842Z","shell.execute_reply":"2025-12-16T19:15:36.584507Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Collate Function","metadata":{}},{"cell_type":"code","source":"def slake_collate_fn(batch, pad_index=0):\n    # Separate different components\n    images = []\n    questions = []\n    answers = []\n    original_questions = []\n    original_answers = []\n    ids = []\n    \n    for item in batch:\n        images.append(item['image'])\n        questions.append(item['question'])\n        answers.append(item['answer'])\n        original_questions.append(item['original_question'])\n        original_answers.append(item['original_answer'])\n        ids.append(item['id'])\n    \n    # Stack images\n    images = torch.stack(images)  # [batch_size, 3, H, W]\n    \n    # Get question lengths BEFORE padding\n    question_lengths = torch.tensor([len(q) for q in questions])\n    \n    # Pad questions to the longest sequence in THIS batch\n    # pad_sequence expects list of tensors, pads with 0 by default\n    questions_padded = pad_sequence(questions, batch_first=True, padding_value=pad_index)\n    # questions_padded: [batch_size, max_len_in_batch]\n    \n    # Handling answers\n    # Handling each answer as a single class\n    # answers = torch.stack(answers)\n    answers = torch.tensor([item['answer'] for item in batch])\n    \n    return {\n        'image': images,\n        'question': questions_padded,\n        'question_lengths': question_lengths,\n        'answer': answers,\n        'original_question': original_questions,\n        'original_answer': original_answers,\n        'id': ids\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:38.775384Z","iopub.execute_input":"2025-12-16T19:15:38.775716Z","iopub.status.idle":"2025-12-16T19:15:38.782507Z","shell.execute_reply.started":"2025-12-16T19:15:38.775681Z","shell.execute_reply":"2025-12-16T19:15:38.781453Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Preparation","metadata":{}},{"cell_type":"code","source":"# Comment out if dataset is already downloaded\ndataset = download_and_store_slake()\n\n# Uncomment if dataset is already downloaded\n# dataset = load_from_disk(DATASET_PATH)\n\n# Build vocabularies for training\ntrain_data = dataset['train']\nvalidation_data = dataset['validation']\nquestion_vocab, answer_vocab = build_vocabs(train_data)\n\n# Define image transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Create train dataset and dataloader\ntrain_dataset = SlakeDataset(train_data, question_vocab, answer_vocab, transform=transform)\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    collate_fn=slake_collate_fn\n)\n\nvalidation_dataset = SlakeDataset(validation_data, question_vocab, answer_vocab, transform=transform)\nvalidation_loader = DataLoader(\n    validation_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False, \n    collate_fn=slake_collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:15:45.962691Z","iopub.execute_input":"2025-12-16T19:15:45.963699Z","iopub.status.idle":"2025-12-16T19:16:00.034724Z","shell.execute_reply.started":"2025-12-16T19:15:45.963655Z","shell.execute_reply":"2025-12-16T19:16:00.033937Z"}},"outputs":[{"name":"stdout","text":"Downloading dataset from BoKelvin/SLAKE repo\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/568 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5cdb246758487cae8b38692deaeab7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12a93e9514549b39bc0d23b92aba2a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dceca35a9c64e218baf7fa56560b112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd428b9ea9494744be32040660c7d6fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9835 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b45ab466f29448c0b26d867fa5704980"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2099 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f4fde63f744a7289e24d8822c353a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82a6a9a24cce40739e13aacd2ee78639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/9835 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275a184897a54e77ac2197c646278889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2099 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b855b56d412a4046a5d852e6fedb842e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f471fdc10cae4b5fa92b047623511a54"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['img_name', 'location', 'answer', 'modality', 'base_type', 'answer_type', 'question', 'qid', 'content_type', 'triple', 'img_id', 'q_lang'],\n        num_rows: 4919\n    })\n    validation: Dataset({\n        features: ['img_name', 'location', 'answer', 'modality', 'base_type', 'answer_type', 'question', 'qid', 'content_type', 'triple', 'img_id', 'q_lang'],\n        num_rows: 1053\n    })\n    test: Dataset({\n        features: ['img_name', 'location', 'answer', 'modality', 'base_type', 'answer_type', 'question', 'qid', 'content_type', 'triple', 'img_id', 'q_lang'],\n        num_rows: 1061\n    })\n})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4919 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad7b4d69db95425396c70f2eb246bf57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1053 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a73fc1a663d4cdcbe9f0c2b46978571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1061 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43517ec925244260b4cf670510e5aae2"}},"metadata":{}},{"name":"stdout","text":"Fetching file imgs.zip from BoKelvin/SLAKE repo\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"imgs.zip:   0%|          | 0.00/212M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6f365ad66f4172ae22259dfcb56770"}},"metadata":{}},{"name":"stdout","text":"imgs.zip download complete. Cached at: /root/.cache/huggingface/hub/datasets--BoKelvin--SLAKE/snapshots/a9083ce6c34ac3ffb17671a605962924d8a8f9e9/imgs.zip\nExtracting to /kaggle/working/...\nExtraction complete.\nimgs.zip files are located in: /kaggle/working\nVocabulary Built. Vocabulary Size: 281\nVocabulary Built. Vocabulary Size: 225\nCaching images for into RAM...\nCached 450 images.\nCaching images for into RAM...\nCached 96 images.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Modeling Baseline\n\n1. Basic CNN-LSTM\n2. CNN with Bidirectional LSTM with Self-Attention","metadata":{}},{"cell_type":"code","source":"class MedicalVQABaseline(nn.Module):\n    def __init__(self, vocab_size, num_classes, embed_dim=256, hidden_dim=512, fusion_dropout=0.5, fusion_dim=1024):\n        super(MedicalVQABaseline, self).__init__()\n\n        # 1. CNN - ResNet\n        resnet = models.resnet34(pretrained=True)\n        num_features = 512\n        # Remove the last classification layer\n        self.resnet_features = nn.Sequential(*list(resnet.children())[:-1])\n        self.img_projector = nn.Linear(num_features, hidden_dim)\n        self.bn_img = nn.BatchNorm1d(hidden_dim)\n\n        # 2. LSTM\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embed_dim, \n            hidden_dim, \n            batch_first=True\n        )\n\n        # 4. Classifier\n        # hidden_dim * 2 => One for the CNN, one for the LSTM\n        total_dim = hidden_dim * 2\n\n        self.attention = nn.Linear(embed_dim, 1)\n        self.kg_gate = nn.Parameter(torch.tensor(0.0))\n\n        self.classifier = nn.Sequential(\n            nn.Linear(total_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(fusion_dropout),\n            nn.Linear(fusion_dim, num_classes)\n        )\n\n    def forward(self, images, questions, question_lengths=None):\n        # CNN\n        # Extract features\n        img_feats = self.resnet_features(images).view(images.size(0), -1)\n        img_feats = self.img_projector(img_feats)\n        img_feats = self.bn_img(img_feats) # Normalize\n        img_feats = torch.relu(img_feats)\n\n        # LSTM\n        embeds = self.embedding(questions) # (Batch, Seq, Embed_Dim)\n        # LSTM output: (Batch, Seq, Hidden), (h_n, c_n)\n        # Take and modify the final hidden state h_n: (1, Batch, Hidden)\n        _, (h_n, _) = self.lstm(embeds)\n        text_feats = h_n.squeeze(0) # (Batch, Hidden)\n        \n        # Fusion\n        combined = torch.cat((img_feats, text_feats), dim=1)\n\n        # Classification\n        logits = self.classifier(combined)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:16:17.232137Z","iopub.execute_input":"2025-12-16T19:16:17.232441Z","iopub.status.idle":"2025-12-16T19:16:17.241632Z","shell.execute_reply.started":"2025-12-16T19:16:17.232420Z","shell.execute_reply":"2025-12-16T19:16:17.240745Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Bidirectional LSTM with Self-Attention for question encoding\nclass BiLSTMWithSelfAttention(nn.Module):\n    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=1, \n                 dropout=0.5, pooling_strategy='mean', attention_heads=8):\n        super(BiLSTMWithSelfAttention, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.pooling_strategy = pooling_strategy\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        \n        # Bidirectional LSTM\n        self.bilstm = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            bidirectional=True,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Self-attention mechanism\n        # BiLSTM outputs hidden_dim * 2 (forward + backward)\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim * 2,\n            num_heads=attention_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n        \n    def forward(self, questions, question_lengths=None):\n        # Embed questions\n        embeds = self.embedding(questions)  # [B, seq_len, embed_dim]\n        embeds = self.dropout(embeds)\n        \n        # Pack sequence if lengths provided (for efficiency)\n        if question_lengths is not None:\n            embeds = nn.utils.rnn.pack_padded_sequence(\n                embeds, question_lengths.cpu(), \n                batch_first=True, enforce_sorted=False\n            )\n        \n        # BiLSTM encoding\n        lstm_out, (hidden, cell) = self.bilstm(embeds)\n        \n        # Unpack if needed\n        if question_lengths is not None:\n            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(\n                lstm_out, batch_first=True\n            )\n        \n        # lstm_out: [B, seq_len, hidden_dim * 2]\n        \n        # Self-attention: query = key = value = lstm_out\n        attn_out, attn_weights = self.attention(\n            query=lstm_out,\n            key=lstm_out,\n            value=lstm_out,\n            need_weights=True\n        )\n        \n        # Residual connection + Layer Norm\n        attn_out = self.layer_norm(lstm_out + attn_out)\n        attn_out = self.dropout(attn_out)\n        \n        # Pooling strategy - you can experiment with these:\n        if self.pooling_strategy == 'mean':\n            question_feature = attn_out.mean(dim=1)  # [B, hidden_dim * 2]\n        elif self.pooling_strategy == 'max':\n            question_feature = attn_out.max(dim=1)[0]\n        else:\n            # Last hidden state (concatenate forward and backward)\n            question_feature = torch.cat([hidden[-2], hidden[-1]], dim=1)\n        \n        return question_feature, attn_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:16:19.290507Z","iopub.execute_input":"2025-12-16T19:16:19.290816Z","iopub.status.idle":"2025-12-16T19:16:19.300932Z","shell.execute_reply.started":"2025-12-16T19:16:19.290793Z","shell.execute_reply":"2025-12-16T19:16:19.300088Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Complete VQA model: ResNet34 + BiLSTM with Self-Attention\nclass VQA_ResNet_BiLSTM_Attention(nn.Module):\n    def __init__(self, vocab_size, num_classes, embed_dim=300, \n                 lstm_hidden=512, fusion_dim=1024, lstm_dropout=0.5, \n                 lstm_num_layers=1, attention_heads=8, fusion_dropout=0.5,\n                 pooling_strategy='mean'):\n        super(VQA_ResNet_BiLSTM_Attention, self).__init__()\n        \n        # Image encoder: ResNet34\n        resnet = models.resnet34(pretrained=True)\n        # Remove the final FC layer\n        self.image_encoder = nn.Sequential(*list(resnet.children())[:-1])\n        self.image_feature_dim = 512  # ResNet34 final layer\n        \n        # Question encoder: BiLSTM + Self-Attention\n        self.question_encoder = BiLSTMWithSelfAttention(\n            vocab_size=vocab_size,\n            embed_dim=embed_dim,\n            hidden_dim=lstm_hidden,\n            num_layers=lstm_num_layers,\n            dropout=lstm_dropout,\n            attention_heads=attention_heads,\n            pooling_strategy=pooling_strategy\n        )\n        self.question_feature_dim = lstm_hidden * 2  # Bidirectional\n        \n        # Multimodal fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(self.image_feature_dim + self.question_feature_dim, fusion_dim),\n            nn.BatchNorm1d(fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(fusion_dropout),\n            nn.Linear(fusion_dim, fusion_dim // 2),\n            nn.BatchNorm1d(fusion_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(fusion_dropout)\n        )\n        \n        # Classifier\n        self.classifier = nn.Linear(fusion_dim // 2, num_classes)\n        \n    def forward(self, images, questions, question_lengths=None):\n        # Extract image features\n        img_features = self.image_encoder(images)  # [B, 512, 1, 1]\n        img_features = img_features.squeeze(-1).squeeze(-1)  # [B, 512]\n        \n        # Extract question features with attention\n        q_features, attn_weights = self.question_encoder(questions, question_lengths) # [B, lstm_hidden * 2]\n        \n        # Concatenate image and question features\n        combined = torch.cat([img_features, q_features], dim=1)\n        # combined: [B, 512 + lstm_hidden*2]\n        \n        # Fusion\n        fused = self.fusion(combined)  # [B, fusion_dim // 2]\n        \n        # Classification\n        logits = self.classifier(fused)  # [B, num_classes]\n        \n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:16:21.528576Z","iopub.execute_input":"2025-12-16T19:16:21.528892Z","iopub.status.idle":"2025-12-16T19:16:21.537918Z","shell.execute_reply.started":"2025-12-16T19:16:21.528868Z","shell.execute_reply":"2025-12-16T19:16:21.537176Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"class HyperparameterTuner:\n    def __init__(self, train_dataset, validation_dataset, vocab_size, num_classes, \n                 n_trials=50, basic_model=True):\n        self.train_dataset = train_dataset\n        self.validation_dataset = validation_dataset\n        self.vocab_size = vocab_size\n        self.num_classes = num_classes\n        self.n_trials = n_trials\n        self.basic_model = basic_model\n        \n        # Track all trial results\n        self.trial_results = []\n\n    def train_single_epoch(self, model, dataloader, criterion, optimizer):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for batch in dataloader:\n            images = batch['image'].to(device)\n            questions = batch['question'].to(device)\n            question_lengths = batch['question_lengths'].to(device)\n            answers = batch['answer'].to(device)\n            \n            # Forward\n            logits = model(images, questions, question_lengths)\n            loss = criterion(logits, answers)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n            optimizer.step()\n            \n            # Metrics\n            total_loss += loss.item()\n            predictions = torch.argmax(logits, dim=1)\n            correct += (predictions == answers).sum().item()\n            total += answers.size(0)\n        \n        return total_loss / len(dataloader), 100 * correct / total\n\n    def validate(self, model, dataloader, criterion):\n        model.eval()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for batch in dataloader:\n                images = batch['image'].to(device)\n                questions = batch['question'].to(device)\n                question_lengths = batch['question_lengths'].to(device)\n                answers = batch['answer'].to(device)\n                \n                logits = model(images, questions, question_lengths)\n                loss = criterion(logits, answers)\n                \n                total_loss += loss.item()\n                predictions = torch.argmax(logits, dim=1)\n                correct += (predictions == answers).sum().item()\n                total += answers.size(0)\n        \n        return total_loss / len(dataloader), 100 * correct / total\n\n    def config_BLSTM(self, trial):\n        return {\n            # Embedding parameters\n            'embed_dim': trial.suggest_categorical('embed_dim', [200, 300, 512]),\n\n            # LSTM parameters\n            'lstm_hidden': trial.suggest_categorical('lstm_hidden', [256, 512, 768, 1024]),\n            'lstm_num_layers': trial.suggest_int('lstm_num_layers', 1, 3),\n            'lstm_dropout': trial.suggest_float('lstm_dropout', 0.1, 0.6),\n            'pooling_strategy': trial.suggest_categorical('pooling_strategy', ['mean', 'max', 'last']),\n\n            # Attention parameters\n            'attention_heads': trial.suggest_categorical('attention_heads', [4, 8, 16]),\n\n            # Fusion parameters\n            'fusion_dim': trial.suggest_categorical('fusion_dim', [512, 1024, 2048]),\n            'fusion_dropout': trial.suggest_float('fusion_dropout', 0.2, 0.6),\n\n            # Training parameters\n            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n            'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n            'scheduler_step_size': trial.suggest_int('scheduler_step_size', 5, 15),\n            'scheduler_gamma': trial.suggest_float('scheduler_gamma', 0.3, 0.7),\n        }\n\n    def config_basic(self, trial):\n        return {\n            # Embedding parameters\n            'embed_dim': trial.suggest_categorical('embed_dim', [256, 300, 512]),\n\n            # Fusion parameters\n            'fusion_dim': trial.suggest_categorical('fusion_dim', [512, 1024, 2048]),\n            'fusion_dropout': trial.suggest_float('fusion_dropout', 0.2, 0.6),\n\n            # LSTM and CNN parameters\n            'hidden_dim': trial.suggest_categorical('lstm_hidden', [256, 512, 768, 1024]),\n\n            # Training parameters\n            'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n            'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n            'scheduler_step_size': trial.suggest_int('scheduler_step_size', 5, 15),\n            'scheduler_gamma': trial.suggest_float('scheduler_gamma', 0.3, 0.7),\n        }\n\n    def objective(self, trial):\n        print(f\"Trial {trial.number + 1}/{self.n_trials}\")\n\n        if not self.basic_model:\n            config = self.config_BLSTM(trial)\n            model = VQA_ResNet_BiLSTM_Attention(\n                vocab_size=self.vocab_size,\n                num_classes=self.num_classes,\n                embed_dim=config['embed_dim'],\n                lstm_hidden=config['lstm_hidden'],\n                lstm_num_layers=config['lstm_num_layers'],\n                attention_heads=config['attention_heads'],\n                fusion_dim=config['fusion_dim'],\n                lstm_dropout=config['lstm_dropout'],\n                fusion_dropout=config['fusion_dropout'],\n                pooling_strategy=config['pooling_strategy']\n            ).to(device)\n            \n            for param in model.image_encoder.parameters():\n                param.requires_grad = False\n        else:\n            config = self.config_basic(trial)\n            model = MedicalVQABaseline(\n                vocab_size=self.vocab_size,\n                num_classes=self.num_classes,\n                embed_dim=config['embed_dim'],\n                fusion_dim=config['fusion_dim'],\n                fusion_dropout=config['fusion_dropout'],\n                hidden_dim=config['hidden_dim'],\n            ).to(device)\n            \n            for param in model.resnet_features.parameters():\n                param.requires_grad = False\n                \n        print(f\"Config: {json.dumps(config, indent=2)}\")\n\n        train_loader = DataLoader(\n            self.train_dataset,\n            batch_size=config['batch_size'],\n            shuffle=True,\n            collate_fn=slake_collate_fn,\n            # pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            self.validation_dataset,\n            batch_size=config['batch_size'],\n            shuffle=False,\n            collate_fn=slake_collate_fn,\n            # pin_memory=True\n        )\n\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config['learning_rate'],\n            weight_decay=config['weight_decay']\n        )\n        \n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer,\n            step_size=config['scheduler_step_size'],\n            gamma=config['scheduler_gamma']\n        )\n\n        best_val_acc = 0.0\n        threshold = 5\n        threshold_count = 0\n        max_epochs = 30\n\n        for epoch in range(max_epochs):\n            train_loss, train_acc = self.train_single_epoch(\n                model, train_loader, criterion, optimizer\n            )\n\n            val_loss, val_acc = self.validate(\n                model, val_loader, criterion\n            )\n\n            scheduler.step()\n            print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%\")\n\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                threshold_count = 0\n            else:\n                threshold_count += 1\n            \n            if threshold_count >= threshold:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n            trial.report(val_acc, epoch)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n\n        trial_result = {\n            'trial_number': trial.number,\n            'config': config,\n            'best_val_acc': best_val_acc,\n            'final_epoch': epoch + 1\n        }\n        self.trial_results.append(trial_result)\n        \n        return best_val_acc\n\n    def run(self):\n        if self.basic_model:\n            print(\"STARTING HYPERPARAMETER TUNING FOR BASIC MODEL\\n\")\n        else:\n            print(\"STARTING HYPERPARAMETER TUNING FOR BLSTM MODEL\\n\")\n        \n        study = optuna.create_study(\n            direction='maximize',\n            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n            sampler=optuna.samplers.TPESampler(seed=GLOBAL_SEED)\n        )\n        study.optimize(self.objective, n_trials=self.n_trials)\n\n        # Print best results\n        print(\"HYPERPARAMETER TUNING COMPLETE\")\n        print(f\"Best Trial: {study.best_trial.number}\")\n        print(f\"Best Validation Accuracy: {study.best_value:.2f}%\\n\")\n        print(f\"Best Hyperparameters:\")\n        for key, value in study.best_params.items():\n            print(f\"  {key}: {value}\")\n        \n        return study","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:16:24.232653Z","iopub.execute_input":"2025-12-16T19:16:24.232950Z","iopub.status.idle":"2025-12-16T19:16:24.258414Z","shell.execute_reply.started":"2025-12-16T19:16:24.232928Z","shell.execute_reply":"2025-12-16T19:16:24.257565Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Test run\ntuner = HyperparameterTuner(\n    vocab_size=len(question_vocab),\n    num_classes=len(answer_vocab),\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    n_trials=10,\n    basic_model=True\n)\n\n# Run tuning\nstudy = tuner.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:16:28.966789Z","iopub.execute_input":"2025-12-16T19:16:28.967108Z","iopub.status.idle":"2025-12-16T20:00:24.849824Z","shell.execute_reply.started":"2025-12-16T19:16:28.967079Z","shell.execute_reply":"2025-12-16T20:00:24.849065Z"}},"outputs":[{"name":"stderr","text":"[I 2025-12-16 19:16:28,969] A new study created in memory with name: no-name-dc0ce072-4c60-4900-bdd5-85adba9991df\n","output_type":"stream"},{"name":"stdout","text":"STARTING HYPERPARAMETER TUNING FOR BASIC MODEL\n\nTrial 1/10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1776512635.py:107: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n/tmp/ipykernel_47/1776512635.py:108: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 191MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Config: {\n  \"embed_dim\": 300,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.2232334448672798,\n  \"hidden_dim\": 256,\n  \"batch_size\": 16,\n  \"learning_rate\": 2.3102018878452926e-05,\n  \"weight_decay\": 3.549878832196506e-06,\n  \"scheduler_step_size\": 8,\n  \"scheduler_gamma\": 0.5099025726528951\n}\nEpoch 1: Train Acc=14.52%, Val Acc=18.52%\nEpoch 2: Train Acc=17.26%, Val Acc=20.80%\nEpoch 3: Train Acc=21.39%, Val Acc=26.88%\nEpoch 4: Train Acc=28.26%, Val Acc=31.72%\nEpoch 5: Train Acc=31.43%, Val Acc=34.57%\nEpoch 6: Train Acc=34.82%, Val Acc=34.28%\nEpoch 7: Train Acc=37.98%, Val Acc=39.13%\nEpoch 8: Train Acc=41.84%, Val Acc=41.60%\nEpoch 9: Train Acc=43.50%, Val Acc=43.30%\nEpoch 10: Train Acc=45.31%, Val Acc=44.35%\nEpoch 11: Train Acc=47.35%, Val Acc=45.77%\nEpoch 12: Train Acc=47.81%, Val Acc=47.39%\nEpoch 13: Train Acc=49.44%, Val Acc=48.81%\nEpoch 14: Train Acc=50.29%, Val Acc=48.81%\nEpoch 15: Train Acc=51.41%, Val Acc=50.05%\nEpoch 16: Train Acc=51.72%, Val Acc=51.09%\nEpoch 17: Train Acc=53.71%, Val Acc=52.14%\nEpoch 18: Train Acc=53.53%, Val Acc=52.33%\nEpoch 19: Train Acc=53.71%, Val Acc=53.28%\nEpoch 20: Train Acc=54.89%, Val Acc=53.75%\nEpoch 21: Train Acc=55.36%, Val Acc=54.61%\nEpoch 22: Train Acc=55.17%, Val Acc=55.37%\nEpoch 23: Train Acc=55.97%, Val Acc=55.18%\nEpoch 24: Train Acc=55.52%, Val Acc=54.89%\nEpoch 25: Train Acc=56.68%, Val Acc=55.08%\nEpoch 26: Train Acc=56.54%, Val Acc=54.99%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:22:45,731] Trial 0 finished with value: 55.3656220322887 and parameters: {'embed_dim': 300, 'fusion_dim': 512, 'fusion_dropout': 0.2232334448672798, 'lstm_hidden': 256, 'batch_size': 16, 'learning_rate': 2.3102018878452926e-05, 'weight_decay': 3.549878832196506e-06, 'scheduler_step_size': 8, 'scheduler_gamma': 0.5099025726528951}. Best is trial 0 with value: 55.3656220322887.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Acc=56.68%, Val Acc=55.18%\nEarly stopping at epoch 27\nTrial 2/10\nConfig: {\n  \"embed_dim\": 512,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.38242799368681435,\n  \"hidden_dim\": 256,\n  \"batch_size\": 32,\n  \"learning_rate\": 1.3492834268013232e-05,\n  \"weight_decay\": 0.0007025166339242157,\n  \"scheduler_step_size\": 15,\n  \"scheduler_gamma\": 0.6233589392465844\n}\nEpoch 1: Train Acc=14.17%, Val Acc=17.57%\nEpoch 2: Train Acc=18.11%, Val Acc=17.57%\nEpoch 3: Train Acc=17.52%, Val Acc=18.14%\nEpoch 4: Train Acc=19.29%, Val Acc=21.84%\nEpoch 5: Train Acc=23.26%, Val Acc=25.45%\nEpoch 6: Train Acc=27.10%, Val Acc=26.59%\nEpoch 7: Train Acc=29.01%, Val Acc=29.25%\nEpoch 8: Train Acc=29.17%, Val Acc=29.06%\nEpoch 9: Train Acc=30.39%, Val Acc=28.58%\nEpoch 10: Train Acc=30.68%, Val Acc=32.29%\nEpoch 11: Train Acc=32.65%, Val Acc=30.96%\nEpoch 12: Train Acc=32.26%, Val Acc=32.95%\nEpoch 13: Train Acc=35.33%, Val Acc=33.71%\nEpoch 14: Train Acc=36.47%, Val Acc=35.52%\nEpoch 15: Train Acc=36.84%, Val Acc=36.94%\nEpoch 16: Train Acc=37.93%, Val Acc=37.04%\nEpoch 17: Train Acc=38.97%, Val Acc=37.42%\nEpoch 18: Train Acc=41.00%, Val Acc=38.27%\nEpoch 19: Train Acc=41.13%, Val Acc=39.13%\nEpoch 20: Train Acc=42.35%, Val Acc=38.37%\nEpoch 21: Train Acc=43.20%, Val Acc=41.12%\nEpoch 22: Train Acc=44.38%, Val Acc=41.98%\nEpoch 23: Train Acc=45.84%, Val Acc=43.68%\nEpoch 24: Train Acc=46.17%, Val Acc=44.16%\nEpoch 25: Train Acc=46.74%, Val Acc=45.49%\nEpoch 26: Train Acc=47.65%, Val Acc=45.58%\nEpoch 27: Train Acc=49.36%, Val Acc=47.39%\nEpoch 28: Train Acc=49.60%, Val Acc=48.15%\nEpoch 29: Train Acc=51.35%, Val Acc=48.81%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:29:15,920] Trial 1 finished with value: 48.812915479582145 and parameters: {'embed_dim': 512, 'fusion_dim': 2048, 'fusion_dropout': 0.38242799368681435, 'lstm_hidden': 256, 'batch_size': 32, 'learning_rate': 1.3492834268013232e-05, 'weight_decay': 0.0007025166339242157, 'scheduler_step_size': 15, 'scheduler_gamma': 0.6233589392465844}. Best is trial 0 with value: 55.3656220322887.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Acc=51.07%, Val Acc=48.05%\nTrial 3/10\nConfig: {\n  \"embed_dim\": 512,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.21375540844608737,\n  \"hidden_dim\": 256,\n  \"batch_size\": 32,\n  \"learning_rate\": 0.0008692991511139548,\n  \"weight_decay\": 0.00021154290797261214,\n  \"scheduler_step_size\": 15,\n  \"scheduler_gamma\": 0.6579309401710595\n}\nEpoch 1: Train Acc=31.35%, Val Acc=44.92%\nEpoch 2: Train Acc=55.52%, Val Acc=59.92%\nEpoch 3: Train Acc=64.65%, Val Acc=66.48%\nEpoch 4: Train Acc=69.81%, Val Acc=68.57%\nEpoch 5: Train Acc=74.85%, Val Acc=67.90%\nEpoch 6: Train Acc=77.84%, Val Acc=70.75%\nEpoch 7: Train Acc=81.38%, Val Acc=72.08%\nEpoch 8: Train Acc=84.33%, Val Acc=74.74%\nEpoch 9: Train Acc=86.81%, Val Acc=73.50%\nEpoch 10: Train Acc=89.06%, Val Acc=75.50%\nEpoch 11: Train Acc=90.65%, Val Acc=76.83%\nEpoch 12: Train Acc=91.85%, Val Acc=77.59%\nEpoch 13: Train Acc=94.04%, Val Acc=74.83%\nEpoch 14: Train Acc=93.31%, Val Acc=77.30%\nEpoch 15: Train Acc=94.84%, Val Acc=78.16%\nEpoch 16: Train Acc=97.03%, Val Acc=79.20%\nEpoch 17: Train Acc=97.56%, Val Acc=79.87%\nEpoch 18: Train Acc=98.15%, Val Acc=79.20%\nEpoch 19: Train Acc=98.31%, Val Acc=79.77%\nEpoch 20: Train Acc=99.11%, Val Acc=79.87%\nEpoch 21: Train Acc=98.76%, Val Acc=79.39%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:34:04,658] Trial 2 finished with value: 79.86704653371321 and parameters: {'embed_dim': 512, 'fusion_dim': 2048, 'fusion_dropout': 0.21375540844608737, 'lstm_hidden': 256, 'batch_size': 32, 'learning_rate': 0.0008692991511139548, 'weight_decay': 0.00021154290797261214, 'scheduler_step_size': 15, 'scheduler_gamma': 0.6579309401710595}. Best is trial 2 with value: 79.86704653371321.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Acc=98.82%, Val Acc=79.11%\nEarly stopping at epoch 22\nTrial 4/10\nConfig: {\n  \"embed_dim\": 300,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.3554709158757928,\n  \"hidden_dim\": 512,\n  \"batch_size\": 64,\n  \"learning_rate\": 1.4096175149815848e-05,\n  \"weight_decay\": 0.0009133995846860973,\n  \"scheduler_step_size\": 13,\n  \"scheduler_gamma\": 0.37948627261366896\n}\nEpoch 1: Train Acc=12.20%, Val Acc=18.23%\nEpoch 2: Train Acc=17.83%, Val Acc=19.37%\nEpoch 3: Train Acc=18.60%, Val Acc=18.99%\nEpoch 4: Train Acc=19.50%, Val Acc=21.46%\nEpoch 5: Train Acc=23.26%, Val Acc=26.40%\nEpoch 6: Train Acc=27.77%, Val Acc=26.97%\nEpoch 7: Train Acc=29.19%, Val Acc=29.15%\nEpoch 8: Train Acc=29.58%, Val Acc=29.44%\nEpoch 9: Train Acc=30.74%, Val Acc=30.67%\nEpoch 10: Train Acc=30.80%, Val Acc=31.05%\nEpoch 11: Train Acc=31.88%, Val Acc=32.67%\nEpoch 12: Train Acc=33.83%, Val Acc=31.62%\nEpoch 13: Train Acc=34.17%, Val Acc=31.81%\nEpoch 14: Train Acc=35.62%, Val Acc=33.24%\nEpoch 15: Train Acc=35.39%, Val Acc=33.52%\nEpoch 16: Train Acc=35.86%, Val Acc=34.28%\nEpoch 17: Train Acc=35.88%, Val Acc=34.38%\nEpoch 18: Train Acc=36.55%, Val Acc=34.57%\nEpoch 19: Train Acc=36.78%, Val Acc=33.81%\nEpoch 20: Train Acc=37.18%, Val Acc=34.28%\nEpoch 21: Train Acc=36.98%, Val Acc=34.57%\nEpoch 22: Train Acc=37.08%, Val Acc=34.28%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:38:56,092] Trial 3 finished with value: 34.5679012345679 and parameters: {'embed_dim': 300, 'fusion_dim': 2048, 'fusion_dropout': 0.3554709158757928, 'lstm_hidden': 512, 'batch_size': 64, 'learning_rate': 1.4096175149815848e-05, 'weight_decay': 0.0009133995846860973, 'scheduler_step_size': 13, 'scheduler_gamma': 0.37948627261366896}. Best is trial 2 with value: 79.86704653371321.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Acc=37.69%, Val Acc=34.28%\nEarly stopping at epoch 23\nTrial 5/10\nConfig: {\n  \"embed_dim\": 300,\n  \"fusion_dim\": 1024,\n  \"fusion_dropout\": 0.34338629141770904,\n  \"hidden_dim\": 512,\n  \"batch_size\": 64,\n  \"learning_rate\": 0.0002878805718308924,\n  \"weight_decay\": 8.178476574339548e-05,\n  \"scheduler_step_size\": 14,\n  \"scheduler_gamma\": 0.4888859700647797\n}\nEpoch 1: Train Acc=19.76%, Val Acc=28.11%\nEpoch 2: Train Acc=33.54%, Val Acc=36.94%\nEpoch 3: Train Acc=41.92%, Val Acc=48.81%\nEpoch 4: Train Acc=53.97%, Val Acc=55.18%\nEpoch 5: Train Acc=62.17%, Val Acc=58.88%\nEpoch 6: Train Acc=66.94%, Val Acc=63.44%\nEpoch 7: Train Acc=71.48%, Val Acc=61.73%\nEpoch 8: Train Acc=73.73%, Val Acc=66.10%\nEpoch 9: Train Acc=76.54%, Val Acc=66.48%\nEpoch 10: Train Acc=78.06%, Val Acc=67.33%\nEpoch 11: Train Acc=80.12%, Val Acc=68.85%\nEpoch 12: Train Acc=81.72%, Val Acc=67.14%\nEpoch 13: Train Acc=82.17%, Val Acc=68.95%\nEpoch 14: Train Acc=83.68%, Val Acc=68.76%\nEpoch 15: Train Acc=86.07%, Val Acc=68.09%\nEpoch 16: Train Acc=86.56%, Val Acc=68.38%\nEpoch 17: Train Acc=86.72%, Val Acc=68.76%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:42:44,540] Trial 4 finished with value: 68.94586894586895 and parameters: {'embed_dim': 300, 'fusion_dim': 1024, 'fusion_dropout': 0.34338629141770904, 'lstm_hidden': 512, 'batch_size': 64, 'learning_rate': 0.0002878805718308924, 'weight_decay': 8.178476574339548e-05, 'scheduler_step_size': 14, 'scheduler_gamma': 0.4888859700647797}. Best is trial 2 with value: 79.86704653371321.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Acc=87.07%, Val Acc=68.38%\nEarly stopping at epoch 18\nTrial 6/10\nConfig: {\n  \"embed_dim\": 512,\n  \"fusion_dim\": 1024,\n  \"fusion_dropout\": 0.4090931317527976,\n  \"hidden_dim\": 256,\n  \"batch_size\": 16,\n  \"learning_rate\": 0.0006533305220227731,\n  \"weight_decay\": 5.595986878006084e-06,\n  \"scheduler_step_size\": 9,\n  \"scheduler_gamma\": 0.6022204554172195\n}\nEpoch 1: Train Acc=32.16%, Val Acc=47.67%\nEpoch 2: Train Acc=54.71%, Val Acc=61.54%\nEpoch 3: Train Acc=64.28%, Val Acc=66.29%\nEpoch 4: Train Acc=69.77%, Val Acc=71.98%\nEpoch 5: Train Acc=72.98%, Val Acc=72.65%\nEpoch 6: Train Acc=75.79%, Val Acc=71.51%\nEpoch 7: Train Acc=78.59%, Val Acc=75.88%\nEpoch 8: Train Acc=80.44%, Val Acc=75.88%\nEpoch 9: Train Acc=82.29%, Val Acc=75.02%\nEpoch 10: Train Acc=85.79%, Val Acc=76.73%\nEpoch 11: Train Acc=87.88%, Val Acc=76.64%\nEpoch 12: Train Acc=88.47%, Val Acc=78.63%\nEpoch 13: Train Acc=90.24%, Val Acc=78.44%\nEpoch 14: Train Acc=91.20%, Val Acc=78.82%\nEpoch 15: Train Acc=91.81%, Val Acc=79.39%\nEpoch 16: Train Acc=92.88%, Val Acc=80.25%\nEpoch 17: Train Acc=93.27%, Val Acc=79.58%\nEpoch 18: Train Acc=94.21%, Val Acc=80.06%\nEpoch 19: Train Acc=95.55%, Val Acc=80.25%\nEpoch 20: Train Acc=96.42%, Val Acc=81.20%\nEpoch 21: Train Acc=96.67%, Val Acc=80.53%\nEpoch 22: Train Acc=96.91%, Val Acc=81.10%\nEpoch 23: Train Acc=97.58%, Val Acc=80.34%\nEpoch 24: Train Acc=96.87%, Val Acc=80.63%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:48:34,036] Trial 5 finished with value: 81.19658119658119 and parameters: {'embed_dim': 512, 'fusion_dim': 1024, 'fusion_dropout': 0.4090931317527976, 'lstm_hidden': 256, 'batch_size': 16, 'learning_rate': 0.0006533305220227731, 'weight_decay': 5.595986878006084e-06, 'scheduler_step_size': 9, 'scheduler_gamma': 0.6022204554172195}. Best is trial 5 with value: 81.19658119658119.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Acc=97.34%, Val Acc=81.10%\nEarly stopping at epoch 25\nTrial 7/10\nConfig: {\n  \"embed_dim\": 512,\n  \"fusion_dim\": 1024,\n  \"fusion_dropout\": 0.45336150260416935,\n  \"hidden_dim\": 1024,\n  \"batch_size\": 64,\n  \"learning_rate\": 4.325207525386527e-05,\n  \"weight_decay\": 2.138729075414893e-06,\n  \"scheduler_step_size\": 7,\n  \"scheduler_gamma\": 0.4708431154505025\n}\nEpoch 1: Train Acc=16.89%, Val Acc=19.18%\nEpoch 2: Train Acc=24.80%, Val Acc=29.72%\nEpoch 3: Train Acc=31.41%, Val Acc=32.10%\nEpoch 4: Train Acc=35.17%, Val Acc=34.95%\nEpoch 5: Train Acc=38.56%, Val Acc=38.27%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:50:00,424] Trial 6 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=41.55%, Val Acc=39.60%\nTrial 8/10\nConfig: {\n  \"embed_dim\": 300,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.24794614693347314,\n  \"hidden_dim\": 512,\n  \"batch_size\": 64,\n  \"learning_rate\": 0.0008411909465645721,\n  \"weight_decay\": 5.6930747676446135e-06,\n  \"scheduler_step_size\": 10,\n  \"scheduler_gamma\": 0.4203513239267078\n}\nEpoch 1: Train Acc=22.67%, Val Acc=33.14%\nEpoch 2: Train Acc=44.16%, Val Acc=47.96%\nEpoch 3: Train Acc=55.84%, Val Acc=56.51%\nEpoch 4: Train Acc=63.53%, Val Acc=61.73%\nEpoch 5: Train Acc=67.47%, Val Acc=64.10%\nEpoch 6: Train Acc=71.23%, Val Acc=65.72%\nEpoch 7: Train Acc=73.47%, Val Acc=65.24%\nEpoch 8: Train Acc=76.48%, Val Acc=66.29%\nEpoch 9: Train Acc=77.84%, Val Acc=68.66%\nEpoch 10: Train Acc=80.24%, Val Acc=67.24%\nEpoch 11: Train Acc=84.29%, Val Acc=68.66%\nEpoch 12: Train Acc=85.34%, Val Acc=68.47%\nEpoch 13: Train Acc=85.93%, Val Acc=68.95%\nEpoch 14: Train Acc=86.72%, Val Acc=69.80%\nEpoch 15: Train Acc=88.49%, Val Acc=71.98%\nEpoch 16: Train Acc=88.92%, Val Acc=71.60%\nEpoch 17: Train Acc=89.77%, Val Acc=72.36%\nEpoch 18: Train Acc=89.39%, Val Acc=72.27%\nEpoch 19: Train Acc=90.53%, Val Acc=73.03%\nEpoch 20: Train Acc=90.49%, Val Acc=72.93%\nEpoch 21: Train Acc=91.99%, Val Acc=72.93%\nEpoch 22: Train Acc=92.95%, Val Acc=73.12%\nEpoch 23: Train Acc=93.11%, Val Acc=73.79%\nEpoch 24: Train Acc=93.41%, Val Acc=73.60%\nEpoch 25: Train Acc=93.92%, Val Acc=74.64%\nEpoch 26: Train Acc=93.56%, Val Acc=74.74%\nEpoch 27: Train Acc=93.86%, Val Acc=75.40%\nEpoch 28: Train Acc=94.31%, Val Acc=75.50%\nEpoch 29: Train Acc=94.49%, Val Acc=76.16%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:56:51,790] Trial 7 finished with value: 76.25830959164293 and parameters: {'embed_dim': 300, 'fusion_dim': 512, 'fusion_dropout': 0.24794614693347314, 'lstm_hidden': 512, 'batch_size': 64, 'learning_rate': 0.0008411909465645721, 'weight_decay': 5.6930747676446135e-06, 'scheduler_step_size': 10, 'scheduler_gamma': 0.4203513239267078}. Best is trial 5 with value: 81.19658119658119.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Acc=95.06%, Val Acc=76.26%\nTrial 9/10\nConfig: {\n  \"embed_dim\": 512,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.5633063543866614,\n  \"hidden_dim\": 1024,\n  \"batch_size\": 64,\n  \"learning_rate\": 2.9872741995638415e-05,\n  \"weight_decay\": 0.00015298506868937454,\n  \"scheduler_step_size\": 9,\n  \"scheduler_gamma\": 0.5529223322374317\n}\nEpoch 1: Train Acc=13.50%, Val Acc=16.52%\nEpoch 2: Train Acc=17.32%, Val Acc=19.75%\nEpoch 3: Train Acc=22.42%, Val Acc=26.31%\nEpoch 4: Train Acc=26.81%, Val Acc=27.54%\nEpoch 5: Train Acc=27.40%, Val Acc=29.63%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 19:58:16,270] Trial 8 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=30.05%, Val Acc=32.76%\nTrial 10/10\nConfig: {\n  \"embed_dim\": 256,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.21631005662190558,\n  \"hidden_dim\": 512,\n  \"batch_size\": 32,\n  \"learning_rate\": 0.00024092145436664767,\n  \"weight_decay\": 1.4461256820152148e-05,\n  \"scheduler_step_size\": 15,\n  \"scheduler_gamma\": 0.35500837765839727\n}\nEpoch 1: Train Acc=25.01%, Val Acc=32.95%\nEpoch 2: Train Acc=38.75%, Val Acc=41.79%\nEpoch 3: Train Acc=49.79%, Val Acc=49.67%\nEpoch 4: Train Acc=59.28%, Val Acc=59.92%\nEpoch 5: Train Acc=62.63%, Val Acc=61.25%\nEpoch 6: Train Acc=68.04%, Val Acc=65.53%\nEpoch 7: Train Acc=70.34%, Val Acc=65.34%\nEpoch 8: Train Acc=72.78%, Val Acc=66.86%\nEpoch 9: Train Acc=75.02%, Val Acc=65.34%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:00:24,845] Trial 9 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Acc=76.70%, Val Acc=65.81%\nHYPERPARAMETER TUNING COMPLETE\nBest Trial: 5\nBest Validation Accuracy: 81.20%\n\nBest Hyperparameters:\n  embed_dim: 512\n  fusion_dim: 1024\n  fusion_dropout: 0.4090931317527976\n  lstm_hidden: 256\n  batch_size: 16\n  learning_rate: 0.0006533305220227731\n  weight_decay: 5.595986878006084e-06\n  scheduler_step_size: 9\n  scheduler_gamma: 0.6022204554172195\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Test run for BLSTM\ntuner = HyperparameterTuner(\n    vocab_size=len(question_vocab),\n    num_classes=len(answer_vocab),\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    n_trials=10,\n    basic_model=False\n)\n\n# Run tuning\nstudy = tuner.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T20:03:01.102725Z","iopub.execute_input":"2025-12-16T20:03:01.103696Z","iopub.status.idle":"2025-12-16T20:44:29.604348Z","shell.execute_reply.started":"2025-12-16T20:03:01.103659Z","shell.execute_reply":"2025-12-16T20:44:29.603740Z"}},"outputs":[{"name":"stderr","text":"[I 2025-12-16 20:03:01,106] A new study created in memory with name: no-name-c04660fd-4d82-483f-8d2d-903cc7a7fefa\n","output_type":"stream"},{"name":"stdout","text":"STARTING HYPERPARAMETER TUNING FOR BLSTM MODEL\n\nTrial 1/10\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1776512635.py:87: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-3),\n/tmp/ipykernel_47/1776512635.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'weight_decay': trial.suggest_loguniform('weight_decay', 1e-6, 1e-3),\n","output_type":"stream"},{"name":"stdout","text":"Config: {\n  \"embed_dim\": 300,\n  \"lstm_hidden\": 256,\n  \"lstm_num_layers\": 3,\n  \"lstm_dropout\": 0.40055750587160444,\n  \"pooling_strategy\": \"last\",\n  \"attention_heads\": 4,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.3727780074568463,\n  \"batch_size\": 32,\n  \"learning_rate\": 3.8396292998041685e-05,\n  \"weight_decay\": 1.2562773503807034e-05,\n  \"scheduler_step_size\": 10,\n  \"scheduler_gamma\": 0.6140703845572054\n}\nEpoch 1: Train Acc=12.60%, Val Acc=19.85%\nEpoch 2: Train Acc=26.57%, Val Acc=30.86%\nEpoch 3: Train Acc=36.61%, Val Acc=38.37%\nEpoch 4: Train Acc=46.59%, Val Acc=47.48%\nEpoch 5: Train Acc=52.39%, Val Acc=50.43%\nEpoch 6: Train Acc=55.05%, Val Acc=51.28%\nEpoch 7: Train Acc=58.22%, Val Acc=55.27%\nEpoch 8: Train Acc=62.25%, Val Acc=57.83%\nEpoch 9: Train Acc=63.12%, Val Acc=58.02%\nEpoch 10: Train Acc=64.81%, Val Acc=61.06%\nEpoch 11: Train Acc=67.47%, Val Acc=62.11%\nEpoch 12: Train Acc=69.18%, Val Acc=63.44%\nEpoch 13: Train Acc=69.83%, Val Acc=64.77%\nEpoch 14: Train Acc=70.89%, Val Acc=64.01%\nEpoch 15: Train Acc=71.90%, Val Acc=65.15%\nEpoch 16: Train Acc=73.55%, Val Acc=67.05%\nEpoch 17: Train Acc=74.47%, Val Acc=66.86%\nEpoch 18: Train Acc=74.89%, Val Acc=68.85%\nEpoch 19: Train Acc=75.20%, Val Acc=68.95%\nEpoch 20: Train Acc=76.99%, Val Acc=68.76%\nEpoch 21: Train Acc=77.27%, Val Acc=68.66%\nEpoch 22: Train Acc=78.27%, Val Acc=69.52%\nEpoch 23: Train Acc=77.98%, Val Acc=68.85%\nEpoch 24: Train Acc=78.39%, Val Acc=69.14%\nEpoch 25: Train Acc=78.88%, Val Acc=69.42%\nEpoch 26: Train Acc=79.83%, Val Acc=69.71%\nEpoch 27: Train Acc=79.65%, Val Acc=69.61%\nEpoch 28: Train Acc=80.04%, Val Acc=69.80%\nEpoch 29: Train Acc=80.59%, Val Acc=69.90%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:10:18,767] Trial 0 finished with value: 70.27540360873694 and parameters: {'embed_dim': 300, 'lstm_hidden': 256, 'lstm_num_layers': 3, 'lstm_dropout': 0.40055750587160444, 'pooling_strategy': 'last', 'attention_heads': 4, 'fusion_dim': 2048, 'fusion_dropout': 0.3727780074568463, 'batch_size': 32, 'learning_rate': 3.8396292998041685e-05, 'weight_decay': 1.2562773503807034e-05, 'scheduler_step_size': 10, 'scheduler_gamma': 0.6140703845572054}. Best is trial 0 with value: 70.27540360873694.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Acc=81.64%, Val Acc=70.28%\nTrial 2/10\nConfig: {\n  \"embed_dim\": 512,\n  \"lstm_hidden\": 512,\n  \"lstm_num_layers\": 3,\n  \"lstm_dropout\": 0.5828160165372797,\n  \"pooling_strategy\": \"mean\",\n  \"attention_heads\": 4,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.3035119926400068,\n  \"batch_size\": 16,\n  \"learning_rate\": 0.00012399967836846095,\n  \"weight_decay\": 3.5856126103453987e-06,\n  \"scheduler_step_size\": 15,\n  \"scheduler_gamma\": 0.6100531293444458\n}\nEpoch 1: Train Acc=39.58%, Val Acc=52.80%\nEpoch 2: Train Acc=57.92%, Val Acc=59.83%\nEpoch 3: Train Acc=63.12%, Val Acc=65.72%\nEpoch 4: Train Acc=67.49%, Val Acc=66.38%\nEpoch 5: Train Acc=70.30%, Val Acc=70.09%\nEpoch 6: Train Acc=74.34%, Val Acc=71.04%\nEpoch 7: Train Acc=76.50%, Val Acc=72.74%\nEpoch 8: Train Acc=77.56%, Val Acc=73.50%\nEpoch 9: Train Acc=80.00%, Val Acc=76.07%\nEpoch 10: Train Acc=81.15%, Val Acc=75.50%\nEpoch 11: Train Acc=82.62%, Val Acc=77.59%\nEpoch 12: Train Acc=83.80%, Val Acc=77.40%\nEpoch 13: Train Acc=85.04%, Val Acc=77.21%\nEpoch 14: Train Acc=86.26%, Val Acc=79.58%\nEpoch 15: Train Acc=87.50%, Val Acc=77.68%\nEpoch 16: Train Acc=90.55%, Val Acc=78.92%\nEpoch 17: Train Acc=91.08%, Val Acc=79.11%\nEpoch 18: Train Acc=92.34%, Val Acc=78.54%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:16:32,522] Trial 1 finished with value: 79.58214624881292 and parameters: {'embed_dim': 512, 'lstm_hidden': 512, 'lstm_num_layers': 3, 'lstm_dropout': 0.5828160165372797, 'pooling_strategy': 'mean', 'attention_heads': 4, 'fusion_dim': 2048, 'fusion_dropout': 0.3035119926400068, 'batch_size': 16, 'learning_rate': 0.00012399967836846095, 'weight_decay': 3.5856126103453987e-06, 'scheduler_step_size': 15, 'scheduler_gamma': 0.6100531293444458}. Best is trial 1 with value: 79.58214624881292.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Acc=92.78%, Val Acc=78.63%\nEarly stopping at epoch 19\nTrial 3/10\nConfig: {\n  \"embed_dim\": 200,\n  \"lstm_hidden\": 256,\n  \"lstm_num_layers\": 1,\n  \"lstm_dropout\": 0.29433864484474104,\n  \"pooling_strategy\": \"max\",\n  \"attention_heads\": 8,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.508897907718663,\n  \"batch_size\": 64,\n  \"learning_rate\": 0.0002592475660475158,\n  \"weight_decay\": 0.00015382308040278996,\n  \"scheduler_step_size\": 13,\n  \"scheduler_gamma\": 0.32961786069363613\n}\nEpoch 1: Train Acc=38.04%, Val Acc=53.28%\nEpoch 2: Train Acc=57.92%, Val Acc=62.30%\nEpoch 3: Train Acc=66.17%, Val Acc=67.33%\nEpoch 4: Train Acc=69.91%, Val Acc=69.04%\nEpoch 5: Train Acc=72.35%, Val Acc=73.31%\nEpoch 6: Train Acc=76.05%, Val Acc=73.60%\nEpoch 7: Train Acc=77.98%, Val Acc=74.93%\nEpoch 8: Train Acc=79.79%, Val Acc=76.16%\nEpoch 9: Train Acc=81.48%, Val Acc=76.07%\nEpoch 10: Train Acc=83.39%, Val Acc=76.45%\nEpoch 11: Train Acc=84.12%, Val Acc=75.88%\nEpoch 12: Train Acc=85.93%, Val Acc=76.83%\nEpoch 13: Train Acc=86.75%, Val Acc=76.35%\nEpoch 14: Train Acc=90.06%, Val Acc=77.02%\nEpoch 15: Train Acc=90.81%, Val Acc=77.68%\nEpoch 16: Train Acc=91.66%, Val Acc=77.59%\nEpoch 17: Train Acc=91.24%, Val Acc=77.21%\nEpoch 18: Train Acc=92.66%, Val Acc=77.68%\nEpoch 19: Train Acc=92.99%, Val Acc=77.11%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:21:14,849] Trial 2 finished with value: 77.68281101614436 and parameters: {'embed_dim': 200, 'lstm_hidden': 256, 'lstm_num_layers': 1, 'lstm_dropout': 0.29433864484474104, 'pooling_strategy': 'max', 'attention_heads': 8, 'fusion_dim': 2048, 'fusion_dropout': 0.508897907718663, 'batch_size': 64, 'learning_rate': 0.0002592475660475158, 'weight_decay': 0.00015382308040278996, 'scheduler_step_size': 13, 'scheduler_gamma': 0.32961786069363613}. Best is trial 1 with value: 79.58214624881292.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Acc=93.09%, Val Acc=77.68%\nEarly stopping at epoch 20\nTrial 4/10\nConfig: {\n  \"embed_dim\": 512,\n  \"lstm_hidden\": 256,\n  \"lstm_num_layers\": 1,\n  \"lstm_dropout\": 0.464803089169032,\n  \"pooling_strategy\": \"max\",\n  \"attention_heads\": 16,\n  \"fusion_dim\": 1024,\n  \"fusion_dropout\": 0.4090931317527976,\n  \"batch_size\": 16,\n  \"learning_rate\": 1.1557352816269867e-05,\n  \"weight_decay\": 8.11392957263784e-05,\n  \"scheduler_step_size\": 8,\n  \"scheduler_gamma\": 0.5034282764658811\n}\nEpoch 1: Train Acc=3.39%, Val Acc=17.66%\nEpoch 2: Train Acc=15.84%, Val Acc=27.35%\nEpoch 3: Train Acc=24.31%, Val Acc=35.23%\nEpoch 4: Train Acc=29.64%, Val Acc=37.61%\nEpoch 5: Train Acc=33.34%, Val Acc=40.08%\nEpoch 6: Train Acc=35.60%, Val Acc=41.12%\nEpoch 7: Train Acc=37.51%, Val Acc=42.83%\nEpoch 8: Train Acc=40.62%, Val Acc=45.39%\nEpoch 9: Train Acc=41.37%, Val Acc=45.77%\nEpoch 10: Train Acc=42.63%, Val Acc=45.96%\nEpoch 11: Train Acc=43.40%, Val Acc=46.72%\nEpoch 12: Train Acc=44.97%, Val Acc=48.34%\nEpoch 13: Train Acc=45.35%, Val Acc=48.24%\nEpoch 14: Train Acc=45.11%, Val Acc=48.81%\nEpoch 15: Train Acc=47.27%, Val Acc=49.67%\nEpoch 16: Train Acc=47.39%, Val Acc=49.48%\nEpoch 17: Train Acc=47.71%, Val Acc=50.52%\nEpoch 18: Train Acc=47.71%, Val Acc=49.76%\nEpoch 19: Train Acc=49.16%, Val Acc=50.62%\nEpoch 20: Train Acc=48.95%, Val Acc=50.62%\nEpoch 21: Train Acc=49.01%, Val Acc=50.90%\nEpoch 22: Train Acc=48.81%, Val Acc=51.28%\nEpoch 23: Train Acc=49.28%, Val Acc=51.09%\nEpoch 24: Train Acc=49.44%, Val Acc=52.42%\nEpoch 25: Train Acc=50.58%, Val Acc=50.90%\nEpoch 26: Train Acc=49.52%, Val Acc=51.19%\nEpoch 27: Train Acc=50.64%, Val Acc=51.28%\nEpoch 28: Train Acc=49.62%, Val Acc=51.38%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:29:00,452] Trial 3 finished with value: 52.421652421652425 and parameters: {'embed_dim': 512, 'lstm_hidden': 256, 'lstm_num_layers': 1, 'lstm_dropout': 0.464803089169032, 'pooling_strategy': 'max', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.4090931317527976, 'batch_size': 16, 'learning_rate': 1.1557352816269867e-05, 'weight_decay': 8.11392957263784e-05, 'scheduler_step_size': 8, 'scheduler_gamma': 0.5034282764658811}. Best is trial 1 with value: 79.58214624881292.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Acc=49.89%, Val Acc=51.76%\nEarly stopping at epoch 29\nTrial 5/10\nConfig: {\n  \"embed_dim\": 200,\n  \"lstm_hidden\": 256,\n  \"lstm_num_layers\": 1,\n  \"lstm_dropout\": 0.5648488261712865,\n  \"pooling_strategy\": \"last\",\n  \"attention_heads\": 16,\n  \"fusion_dim\": 2048,\n  \"fusion_dropout\": 0.32720138998874554,\n  \"batch_size\": 64,\n  \"learning_rate\": 0.0004325432427964555,\n  \"weight_decay\": 0.00038211294416912254,\n  \"scheduler_step_size\": 5,\n  \"scheduler_gamma\": 0.5042989210310262\n}\nEpoch 1: Train Acc=31.55%, Val Acc=43.21%\nEpoch 2: Train Acc=55.76%, Val Acc=60.68%\nEpoch 3: Train Acc=66.44%, Val Acc=62.49%\nEpoch 4: Train Acc=72.70%, Val Acc=68.57%\nEpoch 5: Train Acc=77.60%, Val Acc=70.85%\nEpoch 6: Train Acc=83.98%, Val Acc=72.46%\nEpoch 7: Train Acc=85.87%, Val Acc=71.70%\nEpoch 8: Train Acc=87.86%, Val Acc=72.46%\nEpoch 9: Train Acc=89.10%, Val Acc=73.22%\nEpoch 10: Train Acc=91.03%, Val Acc=74.55%\nEpoch 11: Train Acc=93.68%, Val Acc=73.98%\nEpoch 12: Train Acc=94.23%, Val Acc=75.40%\nEpoch 13: Train Acc=94.98%, Val Acc=74.93%\nEpoch 14: Train Acc=95.69%, Val Acc=74.74%\nEpoch 15: Train Acc=95.55%, Val Acc=75.88%\nEpoch 16: Train Acc=96.73%, Val Acc=76.26%\nEpoch 17: Train Acc=97.09%, Val Acc=76.45%\nEpoch 18: Train Acc=97.19%, Val Acc=76.07%\nEpoch 19: Train Acc=97.40%, Val Acc=75.59%\nEpoch 20: Train Acc=97.52%, Val Acc=76.26%\nEpoch 21: Train Acc=97.91%, Val Acc=76.54%\nEpoch 22: Train Acc=97.62%, Val Acc=76.16%\nEpoch 23: Train Acc=97.89%, Val Acc=76.16%\nEpoch 24: Train Acc=97.89%, Val Acc=75.40%\nEpoch 25: Train Acc=98.37%, Val Acc=76.45%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:34:57,102] Trial 4 finished with value: 76.54320987654322 and parameters: {'embed_dim': 200, 'lstm_hidden': 256, 'lstm_num_layers': 1, 'lstm_dropout': 0.5648488261712865, 'pooling_strategy': 'last', 'attention_heads': 16, 'fusion_dim': 2048, 'fusion_dropout': 0.32720138998874554, 'batch_size': 64, 'learning_rate': 0.0004325432427964555, 'weight_decay': 0.00038211294416912254, 'scheduler_step_size': 5, 'scheduler_gamma': 0.5042989210310262}. Best is trial 1 with value: 79.58214624881292.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Acc=98.70%, Val Acc=75.97%\nEarly stopping at epoch 26\nTrial 6/10\nConfig: {\n  \"embed_dim\": 200,\n  \"lstm_hidden\": 512,\n  \"lstm_num_layers\": 3,\n  \"lstm_dropout\": 0.28181480118964697,\n  \"pooling_strategy\": \"mean\",\n  \"attention_heads\": 4,\n  \"fusion_dim\": 1024,\n  \"fusion_dropout\": 0.22059150049999576,\n  \"batch_size\": 32,\n  \"learning_rate\": 1.9489008462344228e-05,\n  \"weight_decay\": 2.9400741309033083e-05,\n  \"scheduler_step_size\": 15,\n  \"scheduler_gamma\": 0.39682210860460015\n}\nEpoch 1: Train Acc=19.27%, Val Acc=42.07%\nEpoch 2: Train Acc=44.20%, Val Acc=48.34%\nEpoch 3: Train Acc=50.72%, Val Acc=52.04%\nEpoch 4: Train Acc=54.95%, Val Acc=55.18%\nEpoch 5: Train Acc=58.53%, Val Acc=57.45%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:36:30,499] Trial 5 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=60.74%, Val Acc=59.54%\nTrial 7/10\nConfig: {\n  \"embed_dim\": 300,\n  \"lstm_hidden\": 256,\n  \"lstm_num_layers\": 2,\n  \"lstm_dropout\": 0.14514488502720416,\n  \"pooling_strategy\": \"mean\",\n  \"attention_heads\": 16,\n  \"fusion_dim\": 1024,\n  \"fusion_dropout\": 0.45806911616377993,\n  \"batch_size\": 32,\n  \"learning_rate\": 0.0007472397689332936,\n  \"weight_decay\": 2.585608890731339e-06,\n  \"scheduler_step_size\": 8,\n  \"scheduler_gamma\": 0.3453894084962356\n}\nEpoch 1: Train Acc=45.58%, Val Acc=58.50%\nEpoch 2: Train Acc=61.17%, Val Acc=67.71%\nEpoch 3: Train Acc=68.16%, Val Acc=68.28%\nEpoch 4: Train Acc=71.25%, Val Acc=73.22%\nEpoch 5: Train Acc=74.14%, Val Acc=75.21%\nEpoch 6: Train Acc=76.62%, Val Acc=76.35%\nEpoch 7: Train Acc=78.43%, Val Acc=76.92%\nEpoch 8: Train Acc=80.57%, Val Acc=75.50%\nEpoch 9: Train Acc=83.80%, Val Acc=78.73%\nEpoch 10: Train Acc=85.12%, Val Acc=79.77%\nEpoch 11: Train Acc=86.07%, Val Acc=78.92%\nEpoch 12: Train Acc=87.42%, Val Acc=79.77%\nEpoch 13: Train Acc=87.76%, Val Acc=78.73%\nEpoch 14: Train Acc=89.10%, Val Acc=79.20%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:39:53,767] Trial 6 finished with value: 79.77207977207978 and parameters: {'embed_dim': 300, 'lstm_hidden': 256, 'lstm_num_layers': 2, 'lstm_dropout': 0.14514488502720416, 'pooling_strategy': 'mean', 'attention_heads': 16, 'fusion_dim': 1024, 'fusion_dropout': 0.45806911616377993, 'batch_size': 32, 'learning_rate': 0.0007472397689332936, 'weight_decay': 2.585608890731339e-06, 'scheduler_step_size': 8, 'scheduler_gamma': 0.3453894084962356}. Best is trial 6 with value: 79.77207977207978.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Acc=90.06%, Val Acc=79.77%\nEarly stopping at epoch 15\nTrial 8/10\nConfig: {\n  \"embed_dim\": 200,\n  \"lstm_hidden\": 512,\n  \"lstm_num_layers\": 1,\n  \"lstm_dropout\": 0.1465513839029496,\n  \"pooling_strategy\": \"max\",\n  \"attention_heads\": 16,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.4568126584617151,\n  \"batch_size\": 64,\n  \"learning_rate\": 0.00016325185294676912,\n  \"weight_decay\": 1.0655924993232579e-06,\n  \"scheduler_step_size\": 6,\n  \"scheduler_gamma\": 0.5654007076432224\n}\nEpoch 1: Train Acc=30.41%, Val Acc=46.34%\nEpoch 2: Train Acc=49.05%, Val Acc=53.47%\nEpoch 3: Train Acc=55.01%, Val Acc=56.89%\nEpoch 4: Train Acc=57.88%, Val Acc=58.40%\nEpoch 5: Train Acc=61.21%, Val Acc=61.82%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:41:15,804] Trial 7 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=63.49%, Val Acc=63.91%\nTrial 9/10\nConfig: {\n  \"embed_dim\": 512,\n  \"lstm_hidden\": 1024,\n  \"lstm_num_layers\": 1,\n  \"lstm_dropout\": 0.26269984907963384,\n  \"pooling_strategy\": \"last\",\n  \"attention_heads\": 4,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.5892042219009782,\n  \"batch_size\": 32,\n  \"learning_rate\": 0.0003887072196612053,\n  \"weight_decay\": 3.2204108362516767e-05,\n  \"scheduler_step_size\": 11,\n  \"scheduler_gamma\": 0.4970070775275455\n}\nEpoch 1: Train Acc=33.52%, Val Acc=49.76%\nEpoch 2: Train Acc=52.51%, Val Acc=57.74%\nEpoch 3: Train Acc=57.49%, Val Acc=59.92%\nEpoch 4: Train Acc=61.52%, Val Acc=63.15%\nEpoch 5: Train Acc=64.89%, Val Acc=68.09%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:42:40,310] Trial 8 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=68.53%, Val Acc=69.99%\nTrial 10/10\nConfig: {\n  \"embed_dim\": 300,\n  \"lstm_hidden\": 1024,\n  \"lstm_num_layers\": 3,\n  \"lstm_dropout\": 0.5574321951102242,\n  \"pooling_strategy\": \"last\",\n  \"attention_heads\": 8,\n  \"fusion_dim\": 512,\n  \"fusion_dropout\": 0.5404546686067427,\n  \"batch_size\": 64,\n  \"learning_rate\": 0.000745262979291264,\n  \"weight_decay\": 0.00012248682856804866,\n  \"scheduler_step_size\": 11,\n  \"scheduler_gamma\": 0.3388705975083074\n}\nEpoch 1: Train Acc=28.22%, Val Acc=42.64%\nEpoch 2: Train Acc=46.11%, Val Acc=51.57%\nEpoch 3: Train Acc=52.63%, Val Acc=52.80%\nEpoch 4: Train Acc=57.76%, Val Acc=59.26%\nEpoch 5: Train Acc=61.01%, Val Acc=63.91%\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-12-16 20:44:29,600] Trial 9 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc=62.98%, Val Acc=63.82%\nHYPERPARAMETER TUNING COMPLETE\nBest Trial: 6\nBest Validation Accuracy: 79.77%\n\nBest Hyperparameters:\n  embed_dim: 300\n  lstm_hidden: 256\n  lstm_num_layers: 2\n  lstm_dropout: 0.14514488502720416\n  pooling_strategy: mean\n  attention_heads: 16\n  fusion_dim: 1024\n  fusion_dropout: 0.45806911616377993\n  batch_size: 32\n  learning_rate: 0.0007472397689332936\n  weight_decay: 2.585608890731339e-06\n  scheduler_step_size: 8\n  scheduler_gamma: 0.3453894084962356\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}